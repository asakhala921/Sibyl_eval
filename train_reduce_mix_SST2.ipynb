{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReduceMix Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix, SibylCollator\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "        \n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "    if len(labels.shape) > 1: \n",
    "        acc = acc_at_k(labels, preds, k=2)\n",
    "        return { 'accuracy': acc }        \n",
    "    else:\n",
    "        acc = accuracy_score(labels, preds.argmax(-1))\n",
    "        return { 'accuracy': acc }        \n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        if len(labels.shape) > 1: \n",
    "            loss = CEwST_loss(logits, labels)\n",
    "        else:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, \n",
    "                 tokenize_fn, \n",
    "                 transform, \n",
    "                 transform_prob=1.0, \n",
    "                 target_pairs=[], \n",
    "                 target_prob=1.0, \n",
    "                 num_classes=2):\n",
    "        \n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.transform_prob = transform_prob\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        if torch.rand(1) < self.transform_prob:\n",
    "            batch = self.transform(\n",
    "                batch, \n",
    "                self.target_pairs,   \n",
    "                self.target_prob,\n",
    "                self.num_classes\n",
    "            )\n",
    "        text, labels = batch\n",
    "        labels = torch.tensor(labels)\n",
    "        if len(labels.shape) == 1:\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=self.num_classes)\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = labels\n",
    "        batch.pop('idx', None)\n",
    "        batch.pop('label', None)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['xlnet-base-cased']\n",
    "# ts = ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']\n",
    "ts = ['SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "<ipython-input-5-2d91b84c39ec>:34: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  dataset.rename_column_('sentence', 'text')\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4062deb3f2e1483795c925a309bc6fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=0 and num_sampled_SIB=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='36000' max='191943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36000/191943 1:08:53 < 4:58:27, 8.71 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>0.470491</td>\n",
       "      <td>0.835038</td>\n",
       "      <td>14.658400</td>\n",
       "      <td>206.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.397779</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>14.649800</td>\n",
       "      <td>206.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.504900</td>\n",
       "      <td>0.345930</td>\n",
       "      <td>0.881557</td>\n",
       "      <td>14.438400</td>\n",
       "      <td>209.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.474579</td>\n",
       "      <td>0.879248</td>\n",
       "      <td>13.795100</td>\n",
       "      <td>219.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.531787</td>\n",
       "      <td>0.877598</td>\n",
       "      <td>15.467600</td>\n",
       "      <td>195.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.513983</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>13.452000</td>\n",
       "      <td>225.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.547270</td>\n",
       "      <td>0.855823</td>\n",
       "      <td>13.832000</td>\n",
       "      <td>219.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.433912</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>13.511900</td>\n",
       "      <td>224.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.424506</td>\n",
       "      <td>0.871330</td>\n",
       "      <td>13.541800</td>\n",
       "      <td>223.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.429425</td>\n",
       "      <td>0.864071</td>\n",
       "      <td>13.714300</td>\n",
       "      <td>221.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.419036</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>14.156100</td>\n",
       "      <td>214.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.513080</td>\n",
       "      <td>0.865391</td>\n",
       "      <td>14.184100</td>\n",
       "      <td>213.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.562956</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>14.141200</td>\n",
       "      <td>214.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.660603</td>\n",
       "      <td>0.838997</td>\n",
       "      <td>13.475000</td>\n",
       "      <td>224.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.609100</td>\n",
       "      <td>0.684026</td>\n",
       "      <td>0.840647</td>\n",
       "      <td>14.815400</td>\n",
       "      <td>204.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>0.644988</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>12.673600</td>\n",
       "      <td>239.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>0.881212</td>\n",
       "      <td>0.690531</td>\n",
       "      <td>14.127200</td>\n",
       "      <td>214.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.547757</td>\n",
       "      <td>0.792148</td>\n",
       "      <td>13.790100</td>\n",
       "      <td>219.795000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SST2-SibylCollator-SIB\n",
      "{'eval_loss': 0.30294692516326904, 'eval_accuracy': 0.9257609502598366, 'eval_runtime': 26.3794, 'eval_samples_per_second': 255.313, 'epoch': 3.75, 'run': './results/xlnet-base-cased-SST2-SibylCollator-SIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c007ab8233d4b3aa5deeab95d0b8db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with num_sampled_INV=1 and num_sampled_SIB=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='36000' max='191943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36000/191943 1:18:17 < 5:39:07, 7.66 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.589471</td>\n",
       "      <td>0.774002</td>\n",
       "      <td>22.093900</td>\n",
       "      <td>137.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>0.503504</td>\n",
       "      <td>0.838997</td>\n",
       "      <td>21.533000</td>\n",
       "      <td>140.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.465756</td>\n",
       "      <td>0.848565</td>\n",
       "      <td>22.092500</td>\n",
       "      <td>137.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.479523</td>\n",
       "      <td>0.838667</td>\n",
       "      <td>20.743500</td>\n",
       "      <td>146.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.412564</td>\n",
       "      <td>0.846585</td>\n",
       "      <td>22.663800</td>\n",
       "      <td>133.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.469427</td>\n",
       "      <td>0.861102</td>\n",
       "      <td>18.857600</td>\n",
       "      <td>160.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.501554</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>21.321000</td>\n",
       "      <td>142.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.491795</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>20.632300</td>\n",
       "      <td>146.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.461174</td>\n",
       "      <td>0.854503</td>\n",
       "      <td>19.736700</td>\n",
       "      <td>153.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.507519</td>\n",
       "      <td>0.864401</td>\n",
       "      <td>17.598500</td>\n",
       "      <td>172.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.577169</td>\n",
       "      <td>0.831739</td>\n",
       "      <td>20.044000</td>\n",
       "      <td>151.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.489678</td>\n",
       "      <td>0.842956</td>\n",
       "      <td>21.836700</td>\n",
       "      <td>138.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.829737</td>\n",
       "      <td>0.816232</td>\n",
       "      <td>22.657400</td>\n",
       "      <td>133.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.864212</td>\n",
       "      <td>0.751567</td>\n",
       "      <td>22.548700</td>\n",
       "      <td>134.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>1.020359</td>\n",
       "      <td>0.487958</td>\n",
       "      <td>20.602300</td>\n",
       "      <td>147.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.966800</td>\n",
       "      <td>0.942815</td>\n",
       "      <td>0.504124</td>\n",
       "      <td>17.873500</td>\n",
       "      <td>169.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.951600</td>\n",
       "      <td>1.146985</td>\n",
       "      <td>0.491257</td>\n",
       "      <td>23.610700</td>\n",
       "      <td>128.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>1.081039</td>\n",
       "      <td>0.479050</td>\n",
       "      <td>21.141900</td>\n",
       "      <td>143.365000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SST2-SibylCollator-INVSIB\n",
      "{'eval_loss': 0.2668299973011017, 'eval_accuracy': 0.9253155159613957, 'eval_runtime': 26.3495, 'eval_samples_per_second': 255.603, 'epoch': 3.75, 'run': './results/xlnet-base-cased-SST2-SibylCollator-INVSIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8a26b0bd984e4990630dab710049e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='48000' max='191943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48000/191943 1:24:09 < 4:12:24, 9.50 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>0.517927</td>\n",
       "      <td>0.771363</td>\n",
       "      <td>10.907800</td>\n",
       "      <td>277.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.520226</td>\n",
       "      <td>0.835698</td>\n",
       "      <td>10.658500</td>\n",
       "      <td>284.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.538134</td>\n",
       "      <td>0.843616</td>\n",
       "      <td>10.452000</td>\n",
       "      <td>289.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.450566</td>\n",
       "      <td>0.860442</td>\n",
       "      <td>10.458000</td>\n",
       "      <td>289.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.499400</td>\n",
       "      <td>0.455773</td>\n",
       "      <td>0.866711</td>\n",
       "      <td>10.778200</td>\n",
       "      <td>281.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.553503</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>300.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.496163</td>\n",
       "      <td>0.859782</td>\n",
       "      <td>10.976600</td>\n",
       "      <td>276.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.470007</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>10.288500</td>\n",
       "      <td>294.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.564205</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>10.392200</td>\n",
       "      <td>291.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.639038</td>\n",
       "      <td>0.844936</td>\n",
       "      <td>10.362300</td>\n",
       "      <td>292.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.619906</td>\n",
       "      <td>0.860442</td>\n",
       "      <td>10.639500</td>\n",
       "      <td>284.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.489653</td>\n",
       "      <td>0.866711</td>\n",
       "      <td>10.500900</td>\n",
       "      <td>288.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.742572</td>\n",
       "      <td>0.866711</td>\n",
       "      <td>10.526800</td>\n",
       "      <td>287.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.413812</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>10.625600</td>\n",
       "      <td>285.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.506557</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>10.985600</td>\n",
       "      <td>275.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.530502</td>\n",
       "      <td>0.870340</td>\n",
       "      <td>9.805800</td>\n",
       "      <td>309.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.499933</td>\n",
       "      <td>0.868030</td>\n",
       "      <td>10.485000</td>\n",
       "      <td>289.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.549799</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>10.433100</td>\n",
       "      <td>290.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>0.450094</td>\n",
       "      <td>0.870010</td>\n",
       "      <td>10.629600</td>\n",
       "      <td>285.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.539270</td>\n",
       "      <td>0.872319</td>\n",
       "      <td>11.059400</td>\n",
       "      <td>274.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.564174</td>\n",
       "      <td>0.870670</td>\n",
       "      <td>10.270500</td>\n",
       "      <td>295.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.458608</td>\n",
       "      <td>0.859452</td>\n",
       "      <td>10.328400</td>\n",
       "      <td>293.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.570279</td>\n",
       "      <td>0.819202</td>\n",
       "      <td>10.853000</td>\n",
       "      <td>279.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.574145</td>\n",
       "      <td>0.867041</td>\n",
       "      <td>10.521800</td>\n",
       "      <td>288.067000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SST2-SibylCollator-TextMix\n",
      "{'eval_loss': 0.267045259475708, 'eval_accuracy': 0.9328878990348923, 'eval_runtime': 26.3675, 'eval_samples_per_second': 255.428, 'epoch': 5.0, 'run': './results/xlnet-base-cased-SST2-SibylCollator-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd44e0e78a2488bae23d53df1184253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='52000' max='191943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 52000/191943 1:32:43 < 4:09:32, 9.35 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.553951</td>\n",
       "      <td>0.751567</td>\n",
       "      <td>10.511900</td>\n",
       "      <td>288.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.534504</td>\n",
       "      <td>0.832399</td>\n",
       "      <td>10.730300</td>\n",
       "      <td>282.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.577473</td>\n",
       "      <td>0.828110</td>\n",
       "      <td>10.471000</td>\n",
       "      <td>289.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.503339</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>10.206700</td>\n",
       "      <td>296.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.759560</td>\n",
       "      <td>0.852854</td>\n",
       "      <td>10.982600</td>\n",
       "      <td>275.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.651991</td>\n",
       "      <td>0.843946</td>\n",
       "      <td>10.121900</td>\n",
       "      <td>299.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.494900</td>\n",
       "      <td>0.611563</td>\n",
       "      <td>0.851534</td>\n",
       "      <td>10.481000</td>\n",
       "      <td>289.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.531208</td>\n",
       "      <td>0.861762</td>\n",
       "      <td>10.000200</td>\n",
       "      <td>303.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>0.650011</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>10.009200</td>\n",
       "      <td>302.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.710924</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>9.986300</td>\n",
       "      <td>303.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.480900</td>\n",
       "      <td>0.604889</td>\n",
       "      <td>0.833388</td>\n",
       "      <td>10.376200</td>\n",
       "      <td>292.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.564480</td>\n",
       "      <td>0.841966</td>\n",
       "      <td>10.359300</td>\n",
       "      <td>292.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.861051</td>\n",
       "      <td>0.807654</td>\n",
       "      <td>10.734300</td>\n",
       "      <td>282.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.665437</td>\n",
       "      <td>0.843946</td>\n",
       "      <td>10.113900</td>\n",
       "      <td>299.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.677652</td>\n",
       "      <td>0.847905</td>\n",
       "      <td>10.826000</td>\n",
       "      <td>279.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.435685</td>\n",
       "      <td>0.878258</td>\n",
       "      <td>9.825700</td>\n",
       "      <td>308.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.628673</td>\n",
       "      <td>0.858463</td>\n",
       "      <td>10.248600</td>\n",
       "      <td>295.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.768113</td>\n",
       "      <td>0.807984</td>\n",
       "      <td>10.224600</td>\n",
       "      <td>296.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.624591</td>\n",
       "      <td>0.859122</td>\n",
       "      <td>10.325400</td>\n",
       "      <td>293.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.471300</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.822171</td>\n",
       "      <td>10.636500</td>\n",
       "      <td>284.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.625733</td>\n",
       "      <td>0.840977</td>\n",
       "      <td>10.292500</td>\n",
       "      <td>294.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.716525</td>\n",
       "      <td>0.845925</td>\n",
       "      <td>10.330400</td>\n",
       "      <td>293.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.543200</td>\n",
       "      <td>0.734864</td>\n",
       "      <td>0.833058</td>\n",
       "      <td>11.052400</td>\n",
       "      <td>274.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.586948</td>\n",
       "      <td>0.842626</td>\n",
       "      <td>10.659500</td>\n",
       "      <td>284.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.703410</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>10.394200</td>\n",
       "      <td>291.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.815179</td>\n",
       "      <td>0.828769</td>\n",
       "      <td>10.151800</td>\n",
       "      <td>298.567000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SST2-SibylCollator-SentMix\n",
      "{'eval_loss': 0.29202547669410706, 'eval_accuracy': 0.932293986636971, 'eval_runtime': 26.2897, 'eval_samples_per_second': 256.184, 'epoch': 5.42, 'run': './results/xlnet-base-cased-SST2-SibylCollator-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718f172a5cfb4be0b34da9b8bac2977b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='40000' max='191943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 40000/191943 1:11:46 < 4:32:41, 9.29 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.591090</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>10.528800</td>\n",
       "      <td>287.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.614500</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>0.794457</td>\n",
       "      <td>10.525800</td>\n",
       "      <td>287.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.540938</td>\n",
       "      <td>0.792148</td>\n",
       "      <td>10.428100</td>\n",
       "      <td>290.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.538365</td>\n",
       "      <td>0.808644</td>\n",
       "      <td>10.414100</td>\n",
       "      <td>291.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.585113</td>\n",
       "      <td>0.779611</td>\n",
       "      <td>11.316800</td>\n",
       "      <td>267.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.544400</td>\n",
       "      <td>0.498109</td>\n",
       "      <td>0.782910</td>\n",
       "      <td>10.149800</td>\n",
       "      <td>298.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.577490</td>\n",
       "      <td>0.816232</td>\n",
       "      <td>10.484900</td>\n",
       "      <td>289.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>0.630510</td>\n",
       "      <td>0.800396</td>\n",
       "      <td>10.256600</td>\n",
       "      <td>295.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.595756</td>\n",
       "      <td>0.819861</td>\n",
       "      <td>10.230600</td>\n",
       "      <td>296.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.535857</td>\n",
       "      <td>0.830419</td>\n",
       "      <td>9.902500</td>\n",
       "      <td>306.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.850405</td>\n",
       "      <td>0.776971</td>\n",
       "      <td>10.760200</td>\n",
       "      <td>281.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.811264</td>\n",
       "      <td>0.788189</td>\n",
       "      <td>10.327400</td>\n",
       "      <td>293.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.656200</td>\n",
       "      <td>0.609415</td>\n",
       "      <td>0.709997</td>\n",
       "      <td>10.360300</td>\n",
       "      <td>292.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.601200</td>\n",
       "      <td>0.628022</td>\n",
       "      <td>0.762785</td>\n",
       "      <td>10.265500</td>\n",
       "      <td>295.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.700143</td>\n",
       "      <td>0.802375</td>\n",
       "      <td>10.540800</td>\n",
       "      <td>287.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.561500</td>\n",
       "      <td>0.809271</td>\n",
       "      <td>0.822831</td>\n",
       "      <td>9.817700</td>\n",
       "      <td>308.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.627628</td>\n",
       "      <td>0.785219</td>\n",
       "      <td>10.283500</td>\n",
       "      <td>294.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>0.755114</td>\n",
       "      <td>0.754207</td>\n",
       "      <td>10.592700</td>\n",
       "      <td>286.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.604239</td>\n",
       "      <td>0.817882</td>\n",
       "      <td>10.555800</td>\n",
       "      <td>287.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.553600</td>\n",
       "      <td>0.766941</td>\n",
       "      <td>0.799736</td>\n",
       "      <td>10.831000</td>\n",
       "      <td>279.844000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SST2-SibylCollator-WordMix\n",
      "{'eval_loss': 0.33149608969688416, 'eval_accuracy': 0.9058648849294729, 'eval_runtime': 26.3794, 'eval_samples_per_second': 255.313, 'epoch': 4.17, 'run': './results/xlnet-base-cased-SST2-SibylCollator-WordMix', 'test': 'ORIG'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "        \n",
    "    for t in ts: \n",
    "        \n",
    "        transform = None\n",
    "        num_sampled_INV = 0\n",
    "        num_sampled_SIB = 0\n",
    "        label_type = \"soft\"\n",
    "        \n",
    "        if t == \"INV\":\n",
    "            num_sampled_INV = 2\n",
    "            label_type = \"hard\"\n",
    "        elif t == \"SIB\":\n",
    "            num_sampled_SIB = 2\n",
    "        elif t == 'INVSIB':\n",
    "            num_sampled_INV = 1\n",
    "            num_sampled_SIB = 1\n",
    "            label_type = None\n",
    "        elif t == \"TextMix\":\n",
    "            transform = TextMix()\n",
    "        elif t == \"SentMix\":\n",
    "            transform = SentMix()\n",
    "        elif t == \"WordMix\":\n",
    "            transform = WordMix()\n",
    "        \n",
    "        checkpoint = './results/' + MODEL_NAME + '-SST2-SibylCollator-' + t\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "        \n",
    "        dataset = load_dataset('glue', 'sst2', split='train[:90%]') \n",
    "        dataset.rename_column_('sentence', 'text')\n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[90%:]')\n",
    "        test_dataset = test_dataset.rename_column('sentence', 'text') \n",
    "        test_dataset = test_dataset.rename_column('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 6\n",
    "        eval_batch_size  = 32\n",
    "        num_epoch = 20\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "#         tmcb = TargetedMixturesCallback(\n",
    "#             dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "#             device=device\n",
    "#         )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "#         tmc = TargetedMixturesCollator(\n",
    "#             tokenize_fn=tokenize_fn, \n",
    "#             transform=t, \n",
    "#             transform_prob=0.5,\n",
    "#             target_pairs=[],\n",
    "#             target_prob=0.5,\n",
    "#             num_classes=4\n",
    "#         )\n",
    "        sibyl_collator = SibylCollator( \n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=transform, \n",
    "            num_sampled_INV=num_sampled_INV, \n",
    "            num_sampled_SIB=num_sampled_SIB, \n",
    "            task_type=\"topic\", \n",
    "            tran_type=None, \n",
    "            label_type=label_type,\n",
    "            one_hot=label_type == \"soft\",\n",
    "            transform_prob=0.5,\n",
    "            target_pairs=[],\n",
    "            target_prob=0.0,\n",
    "            reduce_mixed=True,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\\\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=sibyl_collator if t != \"ORIG\" else DefaultCollator(),\n",
    "            callbacks=[escb] # [tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        # trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_SibylCollator_XLNET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>run</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.302947</td>\n",
       "      <td>0.925761</td>\n",
       "      <td>26.3794</td>\n",
       "      <td>255.313</td>\n",
       "      <td>3.75</td>\n",
       "      <td>./results/xlnet-base-cased-SST2-SibylCollator-SIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266830</td>\n",
       "      <td>0.925316</td>\n",
       "      <td>26.3495</td>\n",
       "      <td>255.603</td>\n",
       "      <td>3.75</td>\n",
       "      <td>./results/xlnet-base-cased-SST2-SibylCollator-...</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.267045</td>\n",
       "      <td>0.932888</td>\n",
       "      <td>26.3675</td>\n",
       "      <td>255.428</td>\n",
       "      <td>5.00</td>\n",
       "      <td>./results/xlnet-base-cased-SST2-SibylCollator-...</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.292025</td>\n",
       "      <td>0.932294</td>\n",
       "      <td>26.2897</td>\n",
       "      <td>256.184</td>\n",
       "      <td>5.42</td>\n",
       "      <td>./results/xlnet-base-cased-SST2-SibylCollator-...</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.331496</td>\n",
       "      <td>0.905865</td>\n",
       "      <td>26.3794</td>\n",
       "      <td>255.313</td>\n",
       "      <td>4.17</td>\n",
       "      <td>./results/xlnet-base-cased-SST2-SibylCollator-...</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  epoch  \\\n",
       "0   0.302947       0.925761       26.3794                  255.313   3.75   \n",
       "1   0.266830       0.925316       26.3495                  255.603   3.75   \n",
       "2   0.267045       0.932888       26.3675                  255.428   5.00   \n",
       "3   0.292025       0.932294       26.2897                  256.184   5.42   \n",
       "4   0.331496       0.905865       26.3794                  255.313   4.17   \n",
       "\n",
       "                                                 run  test  \n",
       "0  ./results/xlnet-base-cased-SST2-SibylCollator-SIB  ORIG  \n",
       "1  ./results/xlnet-base-cased-SST2-SibylCollator-...  ORIG  \n",
       "2  ./results/xlnet-base-cased-SST2-SibylCollator-...  ORIG  \n",
       "3  ./results/xlnet-base-cased-SST2-SibylCollator-...  ORIG  \n",
       "4  ./results/xlnet-base-cased-SST2-SibylCollator-...  ORIG  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
