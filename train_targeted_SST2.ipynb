{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }        \n",
    "        \n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, tokenize_fn, transform, target_pairs=[], target_prob=1.0, num_classes=2):\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        batch = self.transform(\n",
    "            batch, \n",
    "            self.target_pairs,   \n",
    "            self.target_prob,\n",
    "            self.num_classes\n",
    "        )\n",
    "        text, labels = batch\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = torch.tensor(labels)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "ts = [TextMix(), SentMix(), WordMix()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "<ipython-input-5-de585e8999e9>:13: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  dataset.rename_column_('sentence', 'text')\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0823f6b833406f8946bb36a32cbc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "C:\\Users\\sleev\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='108000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108000/143957 3:42:49 < 1:14:11, 8.08 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.565204</td>\n",
       "      <td>0.760145</td>\n",
       "      <td>7.500300</td>\n",
       "      <td>404.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.419039</td>\n",
       "      <td>0.868030</td>\n",
       "      <td>7.517300</td>\n",
       "      <td>403.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.420110</td>\n",
       "      <td>0.883537</td>\n",
       "      <td>7.703100</td>\n",
       "      <td>393.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.422574</td>\n",
       "      <td>0.900693</td>\n",
       "      <td>7.710000</td>\n",
       "      <td>393.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.896404</td>\n",
       "      <td>7.564400</td>\n",
       "      <td>400.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.401242</td>\n",
       "      <td>0.896734</td>\n",
       "      <td>7.590500</td>\n",
       "      <td>399.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.408346</td>\n",
       "      <td>0.886506</td>\n",
       "      <td>7.466300</td>\n",
       "      <td>405.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.441292</td>\n",
       "      <td>0.892775</td>\n",
       "      <td>7.753500</td>\n",
       "      <td>390.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.429068</td>\n",
       "      <td>0.898713</td>\n",
       "      <td>7.495000</td>\n",
       "      <td>404.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.406545</td>\n",
       "      <td>0.912240</td>\n",
       "      <td>7.565200</td>\n",
       "      <td>400.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.375615</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>7.553400</td>\n",
       "      <td>401.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.425476</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>7.624500</td>\n",
       "      <td>397.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.419692</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>7.760000</td>\n",
       "      <td>390.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.412609</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>7.616100</td>\n",
       "      <td>397.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.446263</td>\n",
       "      <td>0.930386</td>\n",
       "      <td>7.704900</td>\n",
       "      <td>393.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.395135</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>7.501600</td>\n",
       "      <td>404.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.418008</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>7.805500</td>\n",
       "      <td>388.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.429422</td>\n",
       "      <td>0.922138</td>\n",
       "      <td>7.715800</td>\n",
       "      <td>392.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.429506</td>\n",
       "      <td>0.923788</td>\n",
       "      <td>7.437600</td>\n",
       "      <td>407.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.418047</td>\n",
       "      <td>0.933355</td>\n",
       "      <td>7.793800</td>\n",
       "      <td>388.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.420623</td>\n",
       "      <td>0.928406</td>\n",
       "      <td>7.584700</td>\n",
       "      <td>399.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.398603</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.603600</td>\n",
       "      <td>398.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.401416</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>7.557700</td>\n",
       "      <td>401.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.451872</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>7.543000</td>\n",
       "      <td>401.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.396868</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>7.538300</td>\n",
       "      <td>402.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.433657</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.422100</td>\n",
       "      <td>408.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.422253</td>\n",
       "      <td>0.932366</td>\n",
       "      <td>7.621200</td>\n",
       "      <td>397.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.412008</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>7.356900</td>\n",
       "      <td>411.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.409945</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>7.742900</td>\n",
       "      <td>391.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.429779</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.525500</td>\n",
       "      <td>402.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.399414</td>\n",
       "      <td>0.946552</td>\n",
       "      <td>7.629100</td>\n",
       "      <td>397.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.423792</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.617200</td>\n",
       "      <td>397.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.460609</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.510100</td>\n",
       "      <td>403.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.418606</td>\n",
       "      <td>0.948202</td>\n",
       "      <td>7.619400</td>\n",
       "      <td>397.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.424607</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>7.508900</td>\n",
       "      <td>403.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.430030</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>7.408800</td>\n",
       "      <td>409.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.421742</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>7.574400</td>\n",
       "      <td>400.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.422705</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.514600</td>\n",
       "      <td>403.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.419197</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>7.573300</td>\n",
       "      <td>400.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.286200</td>\n",
       "      <td>0.444477</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>7.508000</td>\n",
       "      <td>403.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.427877</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>7.667400</td>\n",
       "      <td>395.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.422157</td>\n",
       "      <td>0.948532</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>408.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.427727</td>\n",
       "      <td>0.950511</td>\n",
       "      <td>7.702400</td>\n",
       "      <td>393.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>7.707100</td>\n",
       "      <td>393.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.428690</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>7.628000</td>\n",
       "      <td>397.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.454198</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>7.570100</td>\n",
       "      <td>400.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.426293</td>\n",
       "      <td>0.953151</td>\n",
       "      <td>7.577300</td>\n",
       "      <td>400.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.463222</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>7.734800</td>\n",
       "      <td>391.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.435580</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>7.583500</td>\n",
       "      <td>399.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.410310</td>\n",
       "      <td>0.954141</td>\n",
       "      <td>7.631400</td>\n",
       "      <td>397.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>7.646600</td>\n",
       "      <td>396.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.442343</td>\n",
       "      <td>0.951501</td>\n",
       "      <td>7.635600</td>\n",
       "      <td>396.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.466341</td>\n",
       "      <td>0.946882</td>\n",
       "      <td>7.569900</td>\n",
       "      <td>400.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.275200</td>\n",
       "      <td>0.457629</td>\n",
       "      <td>0.953811</td>\n",
       "      <td>7.644300</td>\n",
       "      <td>396.503000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n",
      "ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
      "{'eval_loss': 4.352267742156982, 'eval_accuracy': 0.9365998515219005, 'eval_f1': 0.9354884608859432, 'eval_precision': 0.9367433162413096, 'eval_recall': 0.9344281809017018, 'eval_runtime': 16.9715, 'eval_samples_per_second': 396.842, 'epoch': 15.0, 'run': './results/bert-base-uncased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea2d1a7a5b4467bd20b8b45a1e9c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='130000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130000/143957 4:31:20 < 29:07, 7.99 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.520877</td>\n",
       "      <td>0.798416</td>\n",
       "      <td>7.745100</td>\n",
       "      <td>391.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.432630</td>\n",
       "      <td>0.851204</td>\n",
       "      <td>7.554400</td>\n",
       "      <td>401.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.867370</td>\n",
       "      <td>7.775400</td>\n",
       "      <td>389.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.440370</td>\n",
       "      <td>0.888815</td>\n",
       "      <td>7.825000</td>\n",
       "      <td>387.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.387140</td>\n",
       "      <td>0.897723</td>\n",
       "      <td>7.804500</td>\n",
       "      <td>388.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.400763</td>\n",
       "      <td>0.904322</td>\n",
       "      <td>7.656400</td>\n",
       "      <td>395.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.397200</td>\n",
       "      <td>0.420630</td>\n",
       "      <td>0.883867</td>\n",
       "      <td>7.831100</td>\n",
       "      <td>387.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.447835</td>\n",
       "      <td>0.871330</td>\n",
       "      <td>7.768600</td>\n",
       "      <td>390.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.503292</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>7.781900</td>\n",
       "      <td>389.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.439690</td>\n",
       "      <td>0.893105</td>\n",
       "      <td>7.883000</td>\n",
       "      <td>384.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.426978</td>\n",
       "      <td>0.904322</td>\n",
       "      <td>7.803100</td>\n",
       "      <td>388.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>0.450024</td>\n",
       "      <td>0.887166</td>\n",
       "      <td>7.583600</td>\n",
       "      <td>399.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.416926</td>\n",
       "      <td>0.920158</td>\n",
       "      <td>7.728300</td>\n",
       "      <td>392.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.449967</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>8.082000</td>\n",
       "      <td>375.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.416317</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>7.640400</td>\n",
       "      <td>396.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.440151</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>7.865500</td>\n",
       "      <td>385.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>0.429624</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>7.750600</td>\n",
       "      <td>391.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.423505</td>\n",
       "      <td>0.917189</td>\n",
       "      <td>7.599700</td>\n",
       "      <td>398.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.451980</td>\n",
       "      <td>0.919499</td>\n",
       "      <td>7.700100</td>\n",
       "      <td>393.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>0.928077</td>\n",
       "      <td>7.750600</td>\n",
       "      <td>391.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.446532</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>7.584800</td>\n",
       "      <td>399.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.419220</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>7.597000</td>\n",
       "      <td>398.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.402596</td>\n",
       "      <td>0.932036</td>\n",
       "      <td>7.567300</td>\n",
       "      <td>400.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.435122</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>7.597400</td>\n",
       "      <td>398.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.453732</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>7.587500</td>\n",
       "      <td>399.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.439923</td>\n",
       "      <td>0.932696</td>\n",
       "      <td>7.513300</td>\n",
       "      <td>403.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.474982</td>\n",
       "      <td>0.917849</td>\n",
       "      <td>7.848100</td>\n",
       "      <td>386.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.452939</td>\n",
       "      <td>0.920818</td>\n",
       "      <td>7.648300</td>\n",
       "      <td>396.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.454281</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>7.872800</td>\n",
       "      <td>384.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.447107</td>\n",
       "      <td>0.923458</td>\n",
       "      <td>7.755800</td>\n",
       "      <td>390.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.457024</td>\n",
       "      <td>0.913230</td>\n",
       "      <td>7.811800</td>\n",
       "      <td>388.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.447578</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>7.767100</td>\n",
       "      <td>390.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.497170</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>7.799100</td>\n",
       "      <td>388.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.441793</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>7.510700</td>\n",
       "      <td>403.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.459374</td>\n",
       "      <td>0.929396</td>\n",
       "      <td>7.648400</td>\n",
       "      <td>396.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.430490</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>7.731900</td>\n",
       "      <td>392.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.472194</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>7.663200</td>\n",
       "      <td>395.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.929726</td>\n",
       "      <td>7.728300</td>\n",
       "      <td>392.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.451578</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>7.756700</td>\n",
       "      <td>390.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>0.448868</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>7.792500</td>\n",
       "      <td>388.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.459970</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>7.667400</td>\n",
       "      <td>395.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>7.760700</td>\n",
       "      <td>390.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.479486</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>7.895100</td>\n",
       "      <td>383.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.442543</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>7.657500</td>\n",
       "      <td>395.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.491407</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>7.510700</td>\n",
       "      <td>403.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.472647</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>7.662000</td>\n",
       "      <td>395.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.454676</td>\n",
       "      <td>0.940284</td>\n",
       "      <td>7.688900</td>\n",
       "      <td>394.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.460073</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>7.731900</td>\n",
       "      <td>392.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.473125</td>\n",
       "      <td>0.940284</td>\n",
       "      <td>7.594700</td>\n",
       "      <td>399.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.451282</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>7.777600</td>\n",
       "      <td>389.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.477805</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>7.909000</td>\n",
       "      <td>383.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.460665</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>7.773600</td>\n",
       "      <td>389.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.491278</td>\n",
       "      <td>0.928077</td>\n",
       "      <td>7.588500</td>\n",
       "      <td>399.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.459196</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.561200</td>\n",
       "      <td>400.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.449660</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>7.546700</td>\n",
       "      <td>401.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.474645</td>\n",
       "      <td>0.932696</td>\n",
       "      <td>7.888300</td>\n",
       "      <td>384.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.469289</td>\n",
       "      <td>0.936985</td>\n",
       "      <td>7.554200</td>\n",
       "      <td>401.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.478252</td>\n",
       "      <td>0.938634</td>\n",
       "      <td>7.762200</td>\n",
       "      <td>390.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.485709</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>7.782700</td>\n",
       "      <td>389.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.461907</td>\n",
       "      <td>0.936984</td>\n",
       "      <td>7.605100</td>\n",
       "      <td>398.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.511902</td>\n",
       "      <td>0.935665</td>\n",
       "      <td>7.701400</td>\n",
       "      <td>393.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.481781</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>7.559100</td>\n",
       "      <td>400.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.459647</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>7.591700</td>\n",
       "      <td>399.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.475843</td>\n",
       "      <td>0.945892</td>\n",
       "      <td>7.627000</td>\n",
       "      <td>397.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.480205</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>7.642600</td>\n",
       "      <td>396.594000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n",
      "ORIG for ./results/bert-base-uncased-targeted-SentMix\n",
      "{'eval_loss': 4.018394470214844, 'eval_accuracy': 0.9373422420193022, 'eval_f1': 0.9363830430347224, 'eval_precision': 0.936546757099965, 'eval_recall': 0.9362236404591564, 'eval_runtime': 16.9581, 'eval_samples_per_second': 397.155, 'epoch': 18.06, 'run': './results/bert-base-uncased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7617110d390d4dfa82d045258527fc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='102000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102000/143957 3:33:43 < 1:27:55, 7.95 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.592250</td>\n",
       "      <td>0.697130</td>\n",
       "      <td>7.538600</td>\n",
       "      <td>402.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.511400</td>\n",
       "      <td>0.495407</td>\n",
       "      <td>0.770043</td>\n",
       "      <td>7.643900</td>\n",
       "      <td>396.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.482835</td>\n",
       "      <td>0.791818</td>\n",
       "      <td>7.717600</td>\n",
       "      <td>392.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>0.778291</td>\n",
       "      <td>7.771700</td>\n",
       "      <td>390.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.483504</td>\n",
       "      <td>0.810624</td>\n",
       "      <td>7.765100</td>\n",
       "      <td>390.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.462857</td>\n",
       "      <td>0.819532</td>\n",
       "      <td>7.625100</td>\n",
       "      <td>397.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.462076</td>\n",
       "      <td>0.823491</td>\n",
       "      <td>7.685300</td>\n",
       "      <td>394.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.462339</td>\n",
       "      <td>0.826460</td>\n",
       "      <td>7.554900</td>\n",
       "      <td>401.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.467763</td>\n",
       "      <td>0.796437</td>\n",
       "      <td>7.775900</td>\n",
       "      <td>389.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.428521</td>\n",
       "      <td>0.849555</td>\n",
       "      <td>7.694000</td>\n",
       "      <td>393.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.447721</td>\n",
       "      <td>0.853844</td>\n",
       "      <td>7.630500</td>\n",
       "      <td>397.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.465957</td>\n",
       "      <td>0.843286</td>\n",
       "      <td>7.687200</td>\n",
       "      <td>394.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>7.890500</td>\n",
       "      <td>384.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.447328</td>\n",
       "      <td>0.850544</td>\n",
       "      <td>7.733300</td>\n",
       "      <td>391.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.443530</td>\n",
       "      <td>0.845266</td>\n",
       "      <td>7.726400</td>\n",
       "      <td>392.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.449033</td>\n",
       "      <td>0.846255</td>\n",
       "      <td>7.729500</td>\n",
       "      <td>392.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.441917</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>7.670300</td>\n",
       "      <td>395.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.450979</td>\n",
       "      <td>0.845925</td>\n",
       "      <td>7.619800</td>\n",
       "      <td>397.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.464084</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>7.900700</td>\n",
       "      <td>383.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.448788</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>7.793300</td>\n",
       "      <td>388.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.446795</td>\n",
       "      <td>0.863411</td>\n",
       "      <td>7.731900</td>\n",
       "      <td>392.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.451046</td>\n",
       "      <td>0.855163</td>\n",
       "      <td>7.866800</td>\n",
       "      <td>385.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.460255</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>7.720500</td>\n",
       "      <td>392.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.453780</td>\n",
       "      <td>0.853514</td>\n",
       "      <td>7.874700</td>\n",
       "      <td>384.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.431614</td>\n",
       "      <td>0.864731</td>\n",
       "      <td>7.778700</td>\n",
       "      <td>389.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.438767</td>\n",
       "      <td>0.871659</td>\n",
       "      <td>7.653600</td>\n",
       "      <td>396.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.405398</td>\n",
       "      <td>0.873309</td>\n",
       "      <td>7.728500</td>\n",
       "      <td>392.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.435014</td>\n",
       "      <td>0.875619</td>\n",
       "      <td>7.908400</td>\n",
       "      <td>383.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.413393</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>7.825500</td>\n",
       "      <td>387.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.434090</td>\n",
       "      <td>0.865061</td>\n",
       "      <td>7.809200</td>\n",
       "      <td>388.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.421180</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>7.770500</td>\n",
       "      <td>390.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.416738</td>\n",
       "      <td>0.885846</td>\n",
       "      <td>7.590300</td>\n",
       "      <td>399.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.429068</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>7.747200</td>\n",
       "      <td>391.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.416205</td>\n",
       "      <td>0.888486</td>\n",
       "      <td>7.722100</td>\n",
       "      <td>392.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.427115</td>\n",
       "      <td>0.864731</td>\n",
       "      <td>7.633600</td>\n",
       "      <td>397.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.422064</td>\n",
       "      <td>0.875289</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>383.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.444099</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>7.834500</td>\n",
       "      <td>386.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.442417</td>\n",
       "      <td>0.874299</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>388.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.440705</td>\n",
       "      <td>0.884856</td>\n",
       "      <td>7.806800</td>\n",
       "      <td>388.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.439514</td>\n",
       "      <td>0.876608</td>\n",
       "      <td>8.083100</td>\n",
       "      <td>374.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.436960</td>\n",
       "      <td>0.899043</td>\n",
       "      <td>7.787800</td>\n",
       "      <td>389.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.421723</td>\n",
       "      <td>0.892775</td>\n",
       "      <td>7.684700</td>\n",
       "      <td>394.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.424030</td>\n",
       "      <td>0.877268</td>\n",
       "      <td>7.621100</td>\n",
       "      <td>397.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.433399</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>7.750700</td>\n",
       "      <td>391.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.444350</td>\n",
       "      <td>0.883537</td>\n",
       "      <td>7.701700</td>\n",
       "      <td>393.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.424646</td>\n",
       "      <td>0.893764</td>\n",
       "      <td>7.793100</td>\n",
       "      <td>388.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.444453</td>\n",
       "      <td>0.879908</td>\n",
       "      <td>7.683900</td>\n",
       "      <td>394.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.436031</td>\n",
       "      <td>0.876938</td>\n",
       "      <td>7.835900</td>\n",
       "      <td>386.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.439536</td>\n",
       "      <td>0.890795</td>\n",
       "      <td>7.793300</td>\n",
       "      <td>388.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.432141</td>\n",
       "      <td>0.886836</td>\n",
       "      <td>8.010600</td>\n",
       "      <td>378.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.425105</td>\n",
       "      <td>0.889145</td>\n",
       "      <td>7.653300</td>\n",
       "      <td>396.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-WordMix\n",
      "{'eval_loss': 3.465677261352539, 'eval_accuracy': 0.9407572383073497, 'eval_f1': 0.9397937519874, 'eval_precision': 0.940408031183402, 'eval_recall': 0.9392326305554441, 'eval_runtime': 16.9784, 'eval_samples_per_second': 396.68, 'epoch': 14.17, 'run': './results/bert-base-uncased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349c78c41f6549f69ae9b6a40ede33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='86000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 86000/143957 2:58:44 < 2:00:27, 8.02 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>0.492169</td>\n",
       "      <td>0.855493</td>\n",
       "      <td>7.749700</td>\n",
       "      <td>391.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.397046</td>\n",
       "      <td>0.893764</td>\n",
       "      <td>7.883000</td>\n",
       "      <td>384.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.434505</td>\n",
       "      <td>0.897064</td>\n",
       "      <td>7.800800</td>\n",
       "      <td>388.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.440823</td>\n",
       "      <td>0.883207</td>\n",
       "      <td>7.866800</td>\n",
       "      <td>385.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.419668</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>7.846200</td>\n",
       "      <td>386.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.448021</td>\n",
       "      <td>0.908281</td>\n",
       "      <td>7.800500</td>\n",
       "      <td>388.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.451364</td>\n",
       "      <td>0.902342</td>\n",
       "      <td>7.773100</td>\n",
       "      <td>389.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.425691</td>\n",
       "      <td>0.923788</td>\n",
       "      <td>7.833000</td>\n",
       "      <td>386.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.495330</td>\n",
       "      <td>0.911910</td>\n",
       "      <td>7.705600</td>\n",
       "      <td>393.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.410587</td>\n",
       "      <td>0.917849</td>\n",
       "      <td>7.655700</td>\n",
       "      <td>395.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.416096</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>7.625500</td>\n",
       "      <td>397.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.364900</td>\n",
       "      <td>0.426238</td>\n",
       "      <td>0.926757</td>\n",
       "      <td>7.830900</td>\n",
       "      <td>387.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.454790</td>\n",
       "      <td>0.921808</td>\n",
       "      <td>7.723600</td>\n",
       "      <td>392.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.409113</td>\n",
       "      <td>0.932695</td>\n",
       "      <td>7.922700</td>\n",
       "      <td>382.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.460398</td>\n",
       "      <td>0.923788</td>\n",
       "      <td>7.849300</td>\n",
       "      <td>386.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.429549</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>7.667400</td>\n",
       "      <td>395.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.418209</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>7.877600</td>\n",
       "      <td>384.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.420434</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>7.744500</td>\n",
       "      <td>391.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.444177</td>\n",
       "      <td>0.912240</td>\n",
       "      <td>7.713400</td>\n",
       "      <td>392.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.424988</td>\n",
       "      <td>0.932696</td>\n",
       "      <td>7.672800</td>\n",
       "      <td>395.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.446073</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>7.727600</td>\n",
       "      <td>392.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.444443</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>7.912900</td>\n",
       "      <td>383.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.447001</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.907300</td>\n",
       "      <td>383.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.459388</td>\n",
       "      <td>0.930056</td>\n",
       "      <td>7.753600</td>\n",
       "      <td>390.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.408529</td>\n",
       "      <td>0.940284</td>\n",
       "      <td>7.756300</td>\n",
       "      <td>390.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.432573</td>\n",
       "      <td>0.943583</td>\n",
       "      <td>7.885100</td>\n",
       "      <td>384.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.448286</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>7.859600</td>\n",
       "      <td>385.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.462969</td>\n",
       "      <td>0.938634</td>\n",
       "      <td>7.757800</td>\n",
       "      <td>390.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.420104</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.641700</td>\n",
       "      <td>396.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.443727</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>7.979000</td>\n",
       "      <td>379.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.419194</td>\n",
       "      <td>0.943913</td>\n",
       "      <td>7.772600</td>\n",
       "      <td>389.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.442548</td>\n",
       "      <td>0.932696</td>\n",
       "      <td>7.820500</td>\n",
       "      <td>387.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.418452</td>\n",
       "      <td>0.953811</td>\n",
       "      <td>7.789900</td>\n",
       "      <td>389.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.432360</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>7.689400</td>\n",
       "      <td>394.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.409450</td>\n",
       "      <td>0.943583</td>\n",
       "      <td>7.921900</td>\n",
       "      <td>382.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.410045</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>7.724100</td>\n",
       "      <td>392.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>7.649700</td>\n",
       "      <td>396.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.460127</td>\n",
       "      <td>0.953151</td>\n",
       "      <td>7.780500</td>\n",
       "      <td>389.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.437252</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>7.737700</td>\n",
       "      <td>391.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.436505</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>7.670600</td>\n",
       "      <td>395.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>7.847200</td>\n",
       "      <td>386.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>0.447847</td>\n",
       "      <td>0.950181</td>\n",
       "      <td>7.742300</td>\n",
       "      <td>391.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.457804</td>\n",
       "      <td>0.947872</td>\n",
       "      <td>7.909500</td>\n",
       "      <td>383.210000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n",
      "ORIG for ./results/roberta-base-targeted-TextMix\n",
      "{'eval_loss': 4.3649187088012695, 'eval_accuracy': 0.9450631031922792, 'eval_f1': 0.9441546593787185, 'eval_precision': 0.9449114518619111, 'eval_recall': 0.9434748411911948, 'eval_runtime': 16.6865, 'eval_samples_per_second': 403.619, 'epoch': 11.95, 'run': './results/roberta-base-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf51cb851f241b5a93881bc12270bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='74000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 74000/143957 2:34:20 < 2:25:54, 7.99 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.507570</td>\n",
       "      <td>0.839657</td>\n",
       "      <td>7.512600</td>\n",
       "      <td>403.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.429127</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>7.710300</td>\n",
       "      <td>393.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.439100</td>\n",
       "      <td>0.384880</td>\n",
       "      <td>0.901023</td>\n",
       "      <td>7.630900</td>\n",
       "      <td>397.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.439793</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>7.533200</td>\n",
       "      <td>402.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.901353</td>\n",
       "      <td>7.406900</td>\n",
       "      <td>409.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.483961</td>\n",
       "      <td>0.897723</td>\n",
       "      <td>7.566900</td>\n",
       "      <td>400.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.402530</td>\n",
       "      <td>0.910921</td>\n",
       "      <td>7.812900</td>\n",
       "      <td>387.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.437117</td>\n",
       "      <td>0.904322</td>\n",
       "      <td>7.385600</td>\n",
       "      <td>410.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.431443</td>\n",
       "      <td>0.921808</td>\n",
       "      <td>7.664100</td>\n",
       "      <td>395.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.400643</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>7.482800</td>\n",
       "      <td>405.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.423524</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>7.449400</td>\n",
       "      <td>406.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.364800</td>\n",
       "      <td>0.402381</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>7.622300</td>\n",
       "      <td>397.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.414495</td>\n",
       "      <td>0.927747</td>\n",
       "      <td>7.531000</td>\n",
       "      <td>402.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.452796</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>7.498000</td>\n",
       "      <td>404.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.416567</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>7.607800</td>\n",
       "      <td>398.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.420356</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>7.508200</td>\n",
       "      <td>403.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.430218</td>\n",
       "      <td>0.931706</td>\n",
       "      <td>7.494000</td>\n",
       "      <td>404.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.422285</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>7.600700</td>\n",
       "      <td>398.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.425122</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>7.717500</td>\n",
       "      <td>392.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.339800</td>\n",
       "      <td>0.420203</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.442900</td>\n",
       "      <td>407.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.415736</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>7.660700</td>\n",
       "      <td>395.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.432673</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>7.719800</td>\n",
       "      <td>392.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.415753</td>\n",
       "      <td>0.944573</td>\n",
       "      <td>7.426300</td>\n",
       "      <td>408.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.416744</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>7.468500</td>\n",
       "      <td>405.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.397695</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>7.507300</td>\n",
       "      <td>403.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.406097</td>\n",
       "      <td>0.940944</td>\n",
       "      <td>7.783500</td>\n",
       "      <td>389.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.423533</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>7.687900</td>\n",
       "      <td>394.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>7.690800</td>\n",
       "      <td>394.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.434094</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>7.550300</td>\n",
       "      <td>401.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.453160</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>7.578700</td>\n",
       "      <td>399.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.436024</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>7.512800</td>\n",
       "      <td>403.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.439272</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>7.607800</td>\n",
       "      <td>398.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.421859</td>\n",
       "      <td>0.944573</td>\n",
       "      <td>7.713400</td>\n",
       "      <td>392.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.440088</td>\n",
       "      <td>0.945562</td>\n",
       "      <td>7.552300</td>\n",
       "      <td>401.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.417963</td>\n",
       "      <td>0.945893</td>\n",
       "      <td>7.613700</td>\n",
       "      <td>398.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.441104</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>7.428700</td>\n",
       "      <td>408.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.443710</td>\n",
       "      <td>0.940284</td>\n",
       "      <td>7.655800</td>\n",
       "      <td>395.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_patience_counter\n",
      "ORIG for ./results/roberta-base-targeted-SentMix\n",
      "{'eval_loss': 3.934272527694702, 'eval_accuracy': 0.9449146250927988, 'eval_f1': 0.9441540154201284, 'eval_precision': 0.9436910306272355, 'eval_recall': 0.9446586129790756, 'eval_runtime': 16.5719, 'eval_samples_per_second': 406.411, 'epoch': 10.28, 'run': './results/roberta-base-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b20ec8993941028b77e28fa2db62b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='143957' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143957/143957 5:03:37, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.597805</td>\n",
       "      <td>0.743319</td>\n",
       "      <td>7.481900</td>\n",
       "      <td>405.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.522207</td>\n",
       "      <td>0.751897</td>\n",
       "      <td>7.803200</td>\n",
       "      <td>388.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.478816</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>7.609400</td>\n",
       "      <td>398.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.466443</td>\n",
       "      <td>0.812603</td>\n",
       "      <td>7.806900</td>\n",
       "      <td>388.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.494900</td>\n",
       "      <td>0.503308</td>\n",
       "      <td>0.785879</td>\n",
       "      <td>7.713700</td>\n",
       "      <td>392.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.485863</td>\n",
       "      <td>0.799406</td>\n",
       "      <td>7.589500</td>\n",
       "      <td>399.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.485866</td>\n",
       "      <td>0.798086</td>\n",
       "      <td>7.516100</td>\n",
       "      <td>403.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.510256</td>\n",
       "      <td>0.818542</td>\n",
       "      <td>7.606200</td>\n",
       "      <td>398.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.510878</td>\n",
       "      <td>0.801716</td>\n",
       "      <td>7.579500</td>\n",
       "      <td>399.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.451405</td>\n",
       "      <td>0.826790</td>\n",
       "      <td>7.671200</td>\n",
       "      <td>395.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.459500</td>\n",
       "      <td>0.475667</td>\n",
       "      <td>0.831409</td>\n",
       "      <td>7.876700</td>\n",
       "      <td>384.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.490453</td>\n",
       "      <td>0.835698</td>\n",
       "      <td>7.607700</td>\n",
       "      <td>398.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.452652</td>\n",
       "      <td>0.832728</td>\n",
       "      <td>7.770900</td>\n",
       "      <td>390.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.711464</td>\n",
       "      <td>0.626526</td>\n",
       "      <td>7.497200</td>\n",
       "      <td>404.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.451105</td>\n",
       "      <td>0.824150</td>\n",
       "      <td>7.649700</td>\n",
       "      <td>396.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.476388</td>\n",
       "      <td>0.830749</td>\n",
       "      <td>7.652900</td>\n",
       "      <td>396.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.417400</td>\n",
       "      <td>0.459267</td>\n",
       "      <td>0.840647</td>\n",
       "      <td>7.830500</td>\n",
       "      <td>387.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.434751</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>7.858600</td>\n",
       "      <td>385.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.447522</td>\n",
       "      <td>0.847905</td>\n",
       "      <td>8.069500</td>\n",
       "      <td>375.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.453959</td>\n",
       "      <td>0.838667</td>\n",
       "      <td>7.707700</td>\n",
       "      <td>393.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.429692</td>\n",
       "      <td>0.858133</td>\n",
       "      <td>7.805200</td>\n",
       "      <td>388.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.760755</td>\n",
       "      <td>0.597822</td>\n",
       "      <td>7.912300</td>\n",
       "      <td>383.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>0.445493</td>\n",
       "      <td>0.853514</td>\n",
       "      <td>7.922100</td>\n",
       "      <td>382.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>0.459549</td>\n",
       "      <td>0.838667</td>\n",
       "      <td>7.631800</td>\n",
       "      <td>397.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.392100</td>\n",
       "      <td>0.427584</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>7.567900</td>\n",
       "      <td>400.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.439035</td>\n",
       "      <td>0.847575</td>\n",
       "      <td>7.665300</td>\n",
       "      <td>395.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>0.855493</td>\n",
       "      <td>7.635200</td>\n",
       "      <td>396.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.469310</td>\n",
       "      <td>0.846585</td>\n",
       "      <td>7.715300</td>\n",
       "      <td>392.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.435593</td>\n",
       "      <td>0.858792</td>\n",
       "      <td>7.624300</td>\n",
       "      <td>397.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.419856</td>\n",
       "      <td>0.863741</td>\n",
       "      <td>7.756100</td>\n",
       "      <td>390.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.415966</td>\n",
       "      <td>0.844276</td>\n",
       "      <td>7.720300</td>\n",
       "      <td>392.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.449435</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>7.777000</td>\n",
       "      <td>389.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.436246</td>\n",
       "      <td>0.864731</td>\n",
       "      <td>7.757600</td>\n",
       "      <td>390.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.862092</td>\n",
       "      <td>7.780700</td>\n",
       "      <td>389.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.422137</td>\n",
       "      <td>0.865061</td>\n",
       "      <td>7.939500</td>\n",
       "      <td>381.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.432818</td>\n",
       "      <td>0.863411</td>\n",
       "      <td>7.645300</td>\n",
       "      <td>396.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.426181</td>\n",
       "      <td>0.880567</td>\n",
       "      <td>7.811900</td>\n",
       "      <td>387.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.434805</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>7.693600</td>\n",
       "      <td>393.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0.426376</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>7.777300</td>\n",
       "      <td>389.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.447172</td>\n",
       "      <td>0.853844</td>\n",
       "      <td>7.639400</td>\n",
       "      <td>396.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.416952</td>\n",
       "      <td>0.873309</td>\n",
       "      <td>7.663000</td>\n",
       "      <td>395.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.426660</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>7.599300</td>\n",
       "      <td>398.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.870340</td>\n",
       "      <td>7.644300</td>\n",
       "      <td>396.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.443986</td>\n",
       "      <td>0.871989</td>\n",
       "      <td>7.847200</td>\n",
       "      <td>386.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.428818</td>\n",
       "      <td>0.876278</td>\n",
       "      <td>7.621800</td>\n",
       "      <td>397.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.430395</td>\n",
       "      <td>0.877268</td>\n",
       "      <td>7.743100</td>\n",
       "      <td>391.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.423787</td>\n",
       "      <td>0.881557</td>\n",
       "      <td>7.739700</td>\n",
       "      <td>391.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.419176</td>\n",
       "      <td>0.867370</td>\n",
       "      <td>7.672100</td>\n",
       "      <td>395.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.433795</td>\n",
       "      <td>0.877928</td>\n",
       "      <td>7.749900</td>\n",
       "      <td>391.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.425828</td>\n",
       "      <td>0.883207</td>\n",
       "      <td>7.699300</td>\n",
       "      <td>393.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.419798</td>\n",
       "      <td>0.880238</td>\n",
       "      <td>7.711700</td>\n",
       "      <td>393.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.430456</td>\n",
       "      <td>0.866381</td>\n",
       "      <td>7.686600</td>\n",
       "      <td>394.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.402178</td>\n",
       "      <td>0.879248</td>\n",
       "      <td>7.712600</td>\n",
       "      <td>392.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.405030</td>\n",
       "      <td>0.887826</td>\n",
       "      <td>7.797600</td>\n",
       "      <td>388.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.437250</td>\n",
       "      <td>0.874629</td>\n",
       "      <td>7.548400</td>\n",
       "      <td>401.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.428317</td>\n",
       "      <td>0.883537</td>\n",
       "      <td>7.729900</td>\n",
       "      <td>392.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.400766</td>\n",
       "      <td>0.875619</td>\n",
       "      <td>7.767400</td>\n",
       "      <td>390.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.434743</td>\n",
       "      <td>0.892445</td>\n",
       "      <td>7.797000</td>\n",
       "      <td>388.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.420898</td>\n",
       "      <td>0.876278</td>\n",
       "      <td>7.716800</td>\n",
       "      <td>392.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.422264</td>\n",
       "      <td>0.887826</td>\n",
       "      <td>7.802300</td>\n",
       "      <td>388.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.424304</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>7.715400</td>\n",
       "      <td>392.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.322900</td>\n",
       "      <td>0.392461</td>\n",
       "      <td>0.893764</td>\n",
       "      <td>7.814700</td>\n",
       "      <td>387.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.444613</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>7.704400</td>\n",
       "      <td>393.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.419663</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>7.840000</td>\n",
       "      <td>386.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.415851</td>\n",
       "      <td>0.884197</td>\n",
       "      <td>7.730500</td>\n",
       "      <td>392.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.420467</td>\n",
       "      <td>0.890795</td>\n",
       "      <td>7.804500</td>\n",
       "      <td>388.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.429265</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>7.773700</td>\n",
       "      <td>389.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.428512</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>7.962700</td>\n",
       "      <td>380.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.433665</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>7.799500</td>\n",
       "      <td>388.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.448364</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>7.654100</td>\n",
       "      <td>395.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.410916</td>\n",
       "      <td>0.885846</td>\n",
       "      <td>7.725100</td>\n",
       "      <td>392.358000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-WordMix\n",
      "{'eval_loss': 3.0727932453155518, 'eval_accuracy': 0.9343726800296956, 'eval_f1': 0.933082992402104, 'eval_precision': 0.9355361645376501, 'eval_recall': 0.9312354472995343, 'eval_runtime': 16.617, 'eval_samples_per_second': 405.308, 'epoch': 20.0, 'run': './results/roberta-base-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e966a23e4b1e4d53b9136b21b440b9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensors must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-de585e8999e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m         )\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# test with ORIG data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    919\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                     \u001b[1;31m# Approximation of exponential moving average of square of gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m                     \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_approx_sq_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg_sq_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m                     \u001b[0mupdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36m_approx_sq_grad\u001b[1;34m(exp_avg_sq_row, exp_avg_sq_col)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mr_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq_row\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mexp_avg_sq_row\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mc_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_factor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_factor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tensors must be 2-D"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    for t in ts:  \n",
    "    \n",
    "        t_str = t.__class__.__name__\n",
    "        checkpoint = './results/' + MODEL_NAME + '-targeted-' + t_str\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "        dataset = load_dataset('glue', 'sst2', split='train[:90%]') \n",
    "        dataset.rename_column_('sentence', 'text')\n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[90%:]')\n",
    "        test_dataset.rename_column_('sentence', 'text') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 20\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "#         tmcb = TargetedMixturesCallback(\n",
    "#             dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "#             device=device\n",
    "#         )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "        tmc = TargetedMixturesCollator(\n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=t,\n",
    "            target_pairs=[(0,1),(1,0)],\n",
    "            target_prob=0.25,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            adafactor=True,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics_w_soft_target,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=tmc,\n",
    "            callbacks=[escb] # [tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.compute_metrics = compute_metrics\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        # trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_targeted_r2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
