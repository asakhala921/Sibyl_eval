{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_core_web_sm' (2.3.1) requires spaCy >=2.3.0,<2.4.0 and is incompatible with the current version (3.0.3). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }        \n",
    "        \n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, tokenize_fn, transform, target_pairs=[], target_prob=1.0, num_classes=2):\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        batch = self.transform(\n",
    "            batch, \n",
    "            self.target_pairs,   \n",
    "            self.target_prob,\n",
    "            self.num_classes\n",
    "        )\n",
    "        text, labels = batch\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = torch.tensor(labels)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "ts = [TextMix(), SentMix(), WordMix()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f689ecc73ef94a188de65a584fb4b7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7777.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d9fbdee24c4f2f8542fa141b3aa338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4473.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-de585e8999e9>:13: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  dataset.rename_column_('sentence', 'text')\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a48ded9aea430183132d82becc0c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='90000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90000/143957 4:25:51 < 2:39:23, 5.64 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.576811</td>\n",
       "      <td>0.759155</td>\n",
       "      <td>12.177200</td>\n",
       "      <td>248.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.863741</td>\n",
       "      <td>12.147400</td>\n",
       "      <td>249.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.415167</td>\n",
       "      <td>0.881557</td>\n",
       "      <td>12.431200</td>\n",
       "      <td>243.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.404400</td>\n",
       "      <td>0.425116</td>\n",
       "      <td>0.883537</td>\n",
       "      <td>12.490900</td>\n",
       "      <td>242.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.397313</td>\n",
       "      <td>0.899043</td>\n",
       "      <td>12.250700</td>\n",
       "      <td>247.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.405267</td>\n",
       "      <td>0.903332</td>\n",
       "      <td>12.282600</td>\n",
       "      <td>246.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.385078</td>\n",
       "      <td>0.906632</td>\n",
       "      <td>12.109200</td>\n",
       "      <td>250.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.398671</td>\n",
       "      <td>0.911910</td>\n",
       "      <td>12.667300</td>\n",
       "      <td>239.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.431189</td>\n",
       "      <td>0.911580</td>\n",
       "      <td>12.168900</td>\n",
       "      <td>249.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.428978</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>12.265700</td>\n",
       "      <td>247.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.398960</td>\n",
       "      <td>0.914880</td>\n",
       "      <td>12.228600</td>\n",
       "      <td>247.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.403152</td>\n",
       "      <td>0.931706</td>\n",
       "      <td>12.323100</td>\n",
       "      <td>245.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.413874</td>\n",
       "      <td>0.925107</td>\n",
       "      <td>12.581800</td>\n",
       "      <td>240.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.416658</td>\n",
       "      <td>0.914550</td>\n",
       "      <td>12.309800</td>\n",
       "      <td>246.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.419339</td>\n",
       "      <td>0.923458</td>\n",
       "      <td>12.503800</td>\n",
       "      <td>242.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.392038</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>12.184600</td>\n",
       "      <td>248.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.405198</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>12.657600</td>\n",
       "      <td>239.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.443170</td>\n",
       "      <td>0.923128</td>\n",
       "      <td>12.422800</td>\n",
       "      <td>243.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.452522</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>12.104700</td>\n",
       "      <td>250.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.423819</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>12.610100</td>\n",
       "      <td>240.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.425940</td>\n",
       "      <td>0.920818</td>\n",
       "      <td>12.296000</td>\n",
       "      <td>246.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.412305</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.635900</td>\n",
       "      <td>239.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.400032</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>12.202700</td>\n",
       "      <td>248.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.425208</td>\n",
       "      <td>0.930386</td>\n",
       "      <td>12.246300</td>\n",
       "      <td>247.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.425692</td>\n",
       "      <td>0.930056</td>\n",
       "      <td>12.255900</td>\n",
       "      <td>247.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>12.054300</td>\n",
       "      <td>251.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.425799</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>12.295000</td>\n",
       "      <td>246.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.453055</td>\n",
       "      <td>0.932366</td>\n",
       "      <td>11.895200</td>\n",
       "      <td>254.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.422903</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>12.501500</td>\n",
       "      <td>242.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.446606</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>12.345800</td>\n",
       "      <td>245.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.402660</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>12.413600</td>\n",
       "      <td>244.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.428038</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.372500</td>\n",
       "      <td>244.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.450219</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>12.174100</td>\n",
       "      <td>248.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.426977</td>\n",
       "      <td>0.947872</td>\n",
       "      <td>12.302000</td>\n",
       "      <td>246.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.421305</td>\n",
       "      <td>0.953151</td>\n",
       "      <td>12.190400</td>\n",
       "      <td>248.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.427528</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>11.973700</td>\n",
       "      <td>253.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.407381</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>12.267000</td>\n",
       "      <td>247.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.431978</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>12.163300</td>\n",
       "      <td>249.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.416718</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>12.293800</td>\n",
       "      <td>246.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.458430</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>12.143800</td>\n",
       "      <td>249.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.450967</td>\n",
       "      <td>0.948532</td>\n",
       "      <td>12.380300</td>\n",
       "      <td>244.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.433102</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>12.010100</td>\n",
       "      <td>252.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.436060</td>\n",
       "      <td>0.949852</td>\n",
       "      <td>12.491900</td>\n",
       "      <td>242.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.457263</td>\n",
       "      <td>0.934015</td>\n",
       "      <td>12.499100</td>\n",
       "      <td>242.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.421025</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>12.343100</td>\n",
       "      <td>245.561000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
      "{'eval_loss': 4.211819648742676, 'eval_accuracy': 0.9315515961395694, 'eval_f1': 0.930008583139513, 'eval_precision': 0.9342971418858865, 'eval_recall': 0.9271840059198881, 'eval_runtime': 27.3246, 'eval_samples_per_second': 246.481, 'epoch': 12.5, 'run': './results/bert-base-uncased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfadbaa7dd04fd48c58ecef572fa856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='96000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 96000/143957 4:45:30 < 2:22:37, 5.60 it/s, Epoch 13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.526736</td>\n",
       "      <td>0.789178</td>\n",
       "      <td>12.437400</td>\n",
       "      <td>243.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.440850</td>\n",
       "      <td>0.857473</td>\n",
       "      <td>12.783700</td>\n",
       "      <td>237.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.424021</td>\n",
       "      <td>0.875619</td>\n",
       "      <td>12.322700</td>\n",
       "      <td>245.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.421625</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>12.710200</td>\n",
       "      <td>238.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>0.870010</td>\n",
       "      <td>12.329200</td>\n",
       "      <td>245.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.406909</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>12.104800</td>\n",
       "      <td>250.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.395187</td>\n",
       "      <td>0.897394</td>\n",
       "      <td>12.359200</td>\n",
       "      <td>245.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.415229</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>12.388000</td>\n",
       "      <td>244.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.384400</td>\n",
       "      <td>0.448871</td>\n",
       "      <td>0.885186</td>\n",
       "      <td>12.638800</td>\n",
       "      <td>239.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.452748</td>\n",
       "      <td>0.890135</td>\n",
       "      <td>12.207700</td>\n",
       "      <td>248.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.420301</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>12.417300</td>\n",
       "      <td>244.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.408084</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>12.605200</td>\n",
       "      <td>240.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.423196</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>12.767300</td>\n",
       "      <td>237.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.411098</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>12.416100</td>\n",
       "      <td>244.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.411114</td>\n",
       "      <td>0.926757</td>\n",
       "      <td>12.607200</td>\n",
       "      <td>240.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.411685</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>12.523100</td>\n",
       "      <td>242.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.387410</td>\n",
       "      <td>0.923458</td>\n",
       "      <td>12.423200</td>\n",
       "      <td>243.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.430399</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>12.555800</td>\n",
       "      <td>241.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.416929</td>\n",
       "      <td>0.925107</td>\n",
       "      <td>12.539400</td>\n",
       "      <td>241.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.404066</td>\n",
       "      <td>0.919169</td>\n",
       "      <td>12.773300</td>\n",
       "      <td>237.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.435812</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>12.509000</td>\n",
       "      <td>242.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.421106</td>\n",
       "      <td>0.929396</td>\n",
       "      <td>12.522200</td>\n",
       "      <td>242.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.464903</td>\n",
       "      <td>0.926757</td>\n",
       "      <td>12.233600</td>\n",
       "      <td>247.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.433564</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>12.587300</td>\n",
       "      <td>240.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.446236</td>\n",
       "      <td>0.932036</td>\n",
       "      <td>12.616000</td>\n",
       "      <td>240.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.429597</td>\n",
       "      <td>0.932036</td>\n",
       "      <td>12.605200</td>\n",
       "      <td>240.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.428943</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>12.339800</td>\n",
       "      <td>245.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.456038</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>12.666200</td>\n",
       "      <td>239.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.932365</td>\n",
       "      <td>12.607500</td>\n",
       "      <td>240.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.424289</td>\n",
       "      <td>0.934015</td>\n",
       "      <td>12.611600</td>\n",
       "      <td>240.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.425533</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>12.399200</td>\n",
       "      <td>244.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.462210</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.315100</td>\n",
       "      <td>246.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>12.692800</td>\n",
       "      <td>238.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.445802</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>12.490200</td>\n",
       "      <td>242.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.465759</td>\n",
       "      <td>0.940284</td>\n",
       "      <td>12.789000</td>\n",
       "      <td>237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.472226</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>12.403500</td>\n",
       "      <td>244.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.447024</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>12.542300</td>\n",
       "      <td>241.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.483625</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>12.556200</td>\n",
       "      <td>241.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.470577</td>\n",
       "      <td>0.930056</td>\n",
       "      <td>12.446700</td>\n",
       "      <td>243.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.432562</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.461300</td>\n",
       "      <td>243.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.474410</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>12.580200</td>\n",
       "      <td>240.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.470713</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>12.433900</td>\n",
       "      <td>243.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.476750</td>\n",
       "      <td>0.937974</td>\n",
       "      <td>12.752700</td>\n",
       "      <td>237.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.466098</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>12.426800</td>\n",
       "      <td>243.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.541200</td>\n",
       "      <td>241.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.464668</td>\n",
       "      <td>0.925107</td>\n",
       "      <td>12.674600</td>\n",
       "      <td>239.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.483478</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>12.371200</td>\n",
       "      <td>245.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.457196</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>12.513100</td>\n",
       "      <td>242.226000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-SentMix\n",
      "{'eval_loss': 4.2843403816223145, 'eval_accuracy': 0.9374907201187824, 'eval_f1': 0.9366051873058385, 'eval_precision': 0.9362883046318555, 'eval_recall': 0.9369410277321129, 'eval_runtime': 27.3015, 'eval_samples_per_second': 246.69, 'epoch': 13.34, 'run': './results/bert-base-uncased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d617733ba04c8ab2a24dda02a7ea26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='74000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 74000/143957 3:42:38 < 3:30:28, 5.54 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>0.593078</td>\n",
       "      <td>0.696470</td>\n",
       "      <td>12.565700</td>\n",
       "      <td>241.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>0.497381</td>\n",
       "      <td>0.774662</td>\n",
       "      <td>12.448100</td>\n",
       "      <td>243.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>0.493788</td>\n",
       "      <td>0.787529</td>\n",
       "      <td>12.582000</td>\n",
       "      <td>240.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.465200</td>\n",
       "      <td>0.478132</td>\n",
       "      <td>0.792808</td>\n",
       "      <td>12.613500</td>\n",
       "      <td>240.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.457370</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>12.624800</td>\n",
       "      <td>240.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.460667</td>\n",
       "      <td>0.825140</td>\n",
       "      <td>12.593500</td>\n",
       "      <td>240.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.479925</td>\n",
       "      <td>0.815242</td>\n",
       "      <td>12.716000</td>\n",
       "      <td>238.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.506149</td>\n",
       "      <td>0.800066</td>\n",
       "      <td>12.749400</td>\n",
       "      <td>237.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.475042</td>\n",
       "      <td>0.829429</td>\n",
       "      <td>12.595500</td>\n",
       "      <td>240.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.444619</td>\n",
       "      <td>0.840977</td>\n",
       "      <td>12.778600</td>\n",
       "      <td>237.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.442723</td>\n",
       "      <td>0.836028</td>\n",
       "      <td>12.340800</td>\n",
       "      <td>245.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.460988</td>\n",
       "      <td>0.840977</td>\n",
       "      <td>12.833600</td>\n",
       "      <td>236.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.428578</td>\n",
       "      <td>0.847905</td>\n",
       "      <td>12.616600</td>\n",
       "      <td>240.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.847245</td>\n",
       "      <td>12.776900</td>\n",
       "      <td>237.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.480778</td>\n",
       "      <td>0.852854</td>\n",
       "      <td>12.886100</td>\n",
       "      <td>235.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.433856</td>\n",
       "      <td>0.851534</td>\n",
       "      <td>12.346100</td>\n",
       "      <td>245.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.475755</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>12.708600</td>\n",
       "      <td>238.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.440922</td>\n",
       "      <td>0.861432</td>\n",
       "      <td>12.747800</td>\n",
       "      <td>237.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.409249</td>\n",
       "      <td>0.863741</td>\n",
       "      <td>12.753300</td>\n",
       "      <td>237.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>0.414272</td>\n",
       "      <td>0.873309</td>\n",
       "      <td>12.575600</td>\n",
       "      <td>241.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.423832</td>\n",
       "      <td>0.860442</td>\n",
       "      <td>12.777300</td>\n",
       "      <td>237.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>12.524900</td>\n",
       "      <td>241.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.447428</td>\n",
       "      <td>0.859782</td>\n",
       "      <td>12.785700</td>\n",
       "      <td>237.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.420458</td>\n",
       "      <td>0.867370</td>\n",
       "      <td>12.664400</td>\n",
       "      <td>239.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.426779</td>\n",
       "      <td>0.866381</td>\n",
       "      <td>12.455600</td>\n",
       "      <td>243.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>0.417527</td>\n",
       "      <td>0.860112</td>\n",
       "      <td>12.637200</td>\n",
       "      <td>239.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.883207</td>\n",
       "      <td>12.854800</td>\n",
       "      <td>235.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.414185</td>\n",
       "      <td>0.867041</td>\n",
       "      <td>12.535900</td>\n",
       "      <td>241.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.340900</td>\n",
       "      <td>0.420982</td>\n",
       "      <td>0.877268</td>\n",
       "      <td>12.457600</td>\n",
       "      <td>243.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.435225</td>\n",
       "      <td>0.861762</td>\n",
       "      <td>12.526200</td>\n",
       "      <td>241.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.425008</td>\n",
       "      <td>0.875289</td>\n",
       "      <td>12.710100</td>\n",
       "      <td>238.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.437766</td>\n",
       "      <td>0.873969</td>\n",
       "      <td>12.965100</td>\n",
       "      <td>233.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.410769</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>12.674200</td>\n",
       "      <td>239.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.438633</td>\n",
       "      <td>0.878588</td>\n",
       "      <td>12.930300</td>\n",
       "      <td>234.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.431178</td>\n",
       "      <td>0.882217</td>\n",
       "      <td>12.472600</td>\n",
       "      <td>243.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.423083</td>\n",
       "      <td>0.880238</td>\n",
       "      <td>12.647100</td>\n",
       "      <td>239.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.470848</td>\n",
       "      <td>0.873309</td>\n",
       "      <td>12.683800</td>\n",
       "      <td>238.966000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-WordMix\n",
      "{'eval_loss': 3.08549165725708, 'eval_accuracy': 0.9410541945063103, 'eval_f1': 0.9402020302050313, 'eval_precision': 0.9399998801820804, 'eval_recall': 0.9404114798431673, 'eval_runtime': 27.4086, 'eval_samples_per_second': 245.726, 'epoch': 10.28, 'run': './results/bert-base-uncased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ddad744d1749d48a295a5d2f638628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='76000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 76000/143957 3:49:01 < 3:24:47, 5.53 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.815572</td>\n",
       "      <td>12.442600</td>\n",
       "      <td>243.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.481867</td>\n",
       "      <td>0.844276</td>\n",
       "      <td>12.301400</td>\n",
       "      <td>246.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>0.906961</td>\n",
       "      <td>12.776600</td>\n",
       "      <td>237.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.467660</td>\n",
       "      <td>0.883207</td>\n",
       "      <td>12.531100</td>\n",
       "      <td>241.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.414208</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>12.678300</td>\n",
       "      <td>239.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.470878</td>\n",
       "      <td>0.903332</td>\n",
       "      <td>12.472700</td>\n",
       "      <td>243.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>0.442325</td>\n",
       "      <td>0.886506</td>\n",
       "      <td>12.529100</td>\n",
       "      <td>241.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.445847</td>\n",
       "      <td>0.901023</td>\n",
       "      <td>12.504400</td>\n",
       "      <td>242.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.416120</td>\n",
       "      <td>0.923128</td>\n",
       "      <td>12.522100</td>\n",
       "      <td>242.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.431963</td>\n",
       "      <td>0.911910</td>\n",
       "      <td>12.410000</td>\n",
       "      <td>244.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.433493</td>\n",
       "      <td>0.901353</td>\n",
       "      <td>12.719800</td>\n",
       "      <td>238.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.400594</td>\n",
       "      <td>0.928406</td>\n",
       "      <td>12.549000</td>\n",
       "      <td>241.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.418666</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>12.583300</td>\n",
       "      <td>240.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.435676</td>\n",
       "      <td>0.924117</td>\n",
       "      <td>12.717600</td>\n",
       "      <td>238.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.890795</td>\n",
       "      <td>12.459500</td>\n",
       "      <td>243.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.416719</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>12.970300</td>\n",
       "      <td>233.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.431906</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>12.558000</td>\n",
       "      <td>241.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.440826</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>12.527700</td>\n",
       "      <td>241.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.428719</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>12.690800</td>\n",
       "      <td>238.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.431815</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>12.722400</td>\n",
       "      <td>238.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.429393</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>12.621500</td>\n",
       "      <td>240.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.424553</td>\n",
       "      <td>0.943913</td>\n",
       "      <td>12.394100</td>\n",
       "      <td>244.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.443005</td>\n",
       "      <td>0.928077</td>\n",
       "      <td>12.509400</td>\n",
       "      <td>242.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.442012</td>\n",
       "      <td>0.928077</td>\n",
       "      <td>12.778600</td>\n",
       "      <td>237.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.420778</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>12.715200</td>\n",
       "      <td>238.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.420211</td>\n",
       "      <td>0.945562</td>\n",
       "      <td>12.605300</td>\n",
       "      <td>240.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.454701</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>12.543600</td>\n",
       "      <td>241.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.416731</td>\n",
       "      <td>0.952491</td>\n",
       "      <td>12.493900</td>\n",
       "      <td>242.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.454351</td>\n",
       "      <td>0.940944</td>\n",
       "      <td>12.747900</td>\n",
       "      <td>237.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.304800</td>\n",
       "      <td>0.460622</td>\n",
       "      <td>0.914880</td>\n",
       "      <td>12.504100</td>\n",
       "      <td>242.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>12.516900</td>\n",
       "      <td>242.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.455229</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>12.741100</td>\n",
       "      <td>237.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>0.450349</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>12.295000</td>\n",
       "      <td>246.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.435165</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>12.670300</td>\n",
       "      <td>239.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.436128</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>12.666400</td>\n",
       "      <td>239.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.424063</td>\n",
       "      <td>0.935665</td>\n",
       "      <td>13.066200</td>\n",
       "      <td>231.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.441047</td>\n",
       "      <td>0.946882</td>\n",
       "      <td>12.399300</td>\n",
       "      <td>244.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.455874</td>\n",
       "      <td>0.945892</td>\n",
       "      <td>12.634100</td>\n",
       "      <td>239.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-TextMix\n",
      "{'eval_loss': 3.8327736854553223, 'eval_accuracy': 0.9447661469933185, 'eval_f1': 0.9439736786176606, 'eval_precision': 0.9437247452172359, 'eval_recall': 0.9442337117154788, 'eval_runtime': 27.2466, 'eval_samples_per_second': 247.187, 'epoch': 10.56, 'run': './results/roberta-base-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f82c429215548e5955bbef1d7ad9a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='126000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126000/143957 6:18:36 < 53:57, 5.55 it/s, Epoch 17/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>12.526400</td>\n",
       "      <td>241.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.424254</td>\n",
       "      <td>0.858792</td>\n",
       "      <td>12.363400</td>\n",
       "      <td>245.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.491020</td>\n",
       "      <td>0.898053</td>\n",
       "      <td>12.447100</td>\n",
       "      <td>243.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.429697</td>\n",
       "      <td>0.890795</td>\n",
       "      <td>12.651200</td>\n",
       "      <td>239.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.411547</td>\n",
       "      <td>0.886176</td>\n",
       "      <td>12.504900</td>\n",
       "      <td>242.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.452252</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>12.554500</td>\n",
       "      <td>241.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.468734</td>\n",
       "      <td>0.897064</td>\n",
       "      <td>12.324400</td>\n",
       "      <td>245.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.492204</td>\n",
       "      <td>0.905642</td>\n",
       "      <td>12.721400</td>\n",
       "      <td>238.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.436431</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>12.311500</td>\n",
       "      <td>246.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.440165</td>\n",
       "      <td>0.902342</td>\n",
       "      <td>12.566000</td>\n",
       "      <td>241.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.434057</td>\n",
       "      <td>0.905972</td>\n",
       "      <td>12.262200</td>\n",
       "      <td>247.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.430693</td>\n",
       "      <td>0.904652</td>\n",
       "      <td>12.766000</td>\n",
       "      <td>237.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.431650</td>\n",
       "      <td>0.916859</td>\n",
       "      <td>12.773700</td>\n",
       "      <td>237.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>0.431638</td>\n",
       "      <td>0.915539</td>\n",
       "      <td>12.359200</td>\n",
       "      <td>245.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.356700</td>\n",
       "      <td>0.440745</td>\n",
       "      <td>0.920818</td>\n",
       "      <td>12.657900</td>\n",
       "      <td>239.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.450137</td>\n",
       "      <td>0.925107</td>\n",
       "      <td>12.515000</td>\n",
       "      <td>242.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.433960</td>\n",
       "      <td>0.923458</td>\n",
       "      <td>12.698300</td>\n",
       "      <td>238.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>0.462664</td>\n",
       "      <td>0.916859</td>\n",
       "      <td>12.410100</td>\n",
       "      <td>244.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.453471</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>12.204300</td>\n",
       "      <td>248.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.443860</td>\n",
       "      <td>0.907621</td>\n",
       "      <td>12.684900</td>\n",
       "      <td>238.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.401528</td>\n",
       "      <td>0.934345</td>\n",
       "      <td>12.505500</td>\n",
       "      <td>242.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.470124</td>\n",
       "      <td>0.921478</td>\n",
       "      <td>12.651700</td>\n",
       "      <td>239.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.450447</td>\n",
       "      <td>0.916859</td>\n",
       "      <td>12.529200</td>\n",
       "      <td>241.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.468262</td>\n",
       "      <td>0.919828</td>\n",
       "      <td>12.569600</td>\n",
       "      <td>241.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.458280</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>12.573500</td>\n",
       "      <td>241.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.464860</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>12.689200</td>\n",
       "      <td>238.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.449014</td>\n",
       "      <td>0.936655</td>\n",
       "      <td>12.685300</td>\n",
       "      <td>238.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.448738</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>12.317300</td>\n",
       "      <td>246.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.465963</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>12.343100</td>\n",
       "      <td>245.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.459026</td>\n",
       "      <td>0.929066</td>\n",
       "      <td>12.484600</td>\n",
       "      <td>242.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.929396</td>\n",
       "      <td>12.427700</td>\n",
       "      <td>243.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.449202</td>\n",
       "      <td>0.941604</td>\n",
       "      <td>12.198700</td>\n",
       "      <td>248.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.286500</td>\n",
       "      <td>0.442214</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>12.496500</td>\n",
       "      <td>242.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.445875</td>\n",
       "      <td>0.937974</td>\n",
       "      <td>12.546600</td>\n",
       "      <td>241.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.467771</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>12.360200</td>\n",
       "      <td>245.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.485718</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>12.515000</td>\n",
       "      <td>242.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.467176</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.650700</td>\n",
       "      <td>239.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.466907</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>12.365400</td>\n",
       "      <td>245.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.458958</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>12.368200</td>\n",
       "      <td>245.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>0.479710</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>12.795600</td>\n",
       "      <td>236.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.471363</td>\n",
       "      <td>0.936984</td>\n",
       "      <td>12.592800</td>\n",
       "      <td>240.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.491172</td>\n",
       "      <td>0.932696</td>\n",
       "      <td>12.126400</td>\n",
       "      <td>249.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.471945</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>12.312000</td>\n",
       "      <td>246.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.491758</td>\n",
       "      <td>0.941604</td>\n",
       "      <td>12.273000</td>\n",
       "      <td>246.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.468466</td>\n",
       "      <td>0.943583</td>\n",
       "      <td>12.660000</td>\n",
       "      <td>239.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.481639</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>12.582200</td>\n",
       "      <td>240.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.490531</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>12.342100</td>\n",
       "      <td>245.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.491537</td>\n",
       "      <td>0.935665</td>\n",
       "      <td>12.578600</td>\n",
       "      <td>240.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.489337</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>12.771000</td>\n",
       "      <td>237.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.496388</td>\n",
       "      <td>0.941603</td>\n",
       "      <td>12.658700</td>\n",
       "      <td>239.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.498576</td>\n",
       "      <td>0.940943</td>\n",
       "      <td>12.579500</td>\n",
       "      <td>240.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.496068</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>12.270000</td>\n",
       "      <td>247.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.459386</td>\n",
       "      <td>0.949852</td>\n",
       "      <td>12.430500</td>\n",
       "      <td>243.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.477295</td>\n",
       "      <td>0.945892</td>\n",
       "      <td>12.487900</td>\n",
       "      <td>242.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.510233</td>\n",
       "      <td>0.941603</td>\n",
       "      <td>12.353100</td>\n",
       "      <td>245.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.473050</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>12.888700</td>\n",
       "      <td>235.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.501937</td>\n",
       "      <td>0.946882</td>\n",
       "      <td>12.446900</td>\n",
       "      <td>243.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.506804</td>\n",
       "      <td>0.941603</td>\n",
       "      <td>12.388000</td>\n",
       "      <td>244.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.523611</td>\n",
       "      <td>0.941274</td>\n",
       "      <td>12.415900</td>\n",
       "      <td>244.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.490650</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>12.560900</td>\n",
       "      <td>241.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.533257</td>\n",
       "      <td>0.936325</td>\n",
       "      <td>12.607100</td>\n",
       "      <td>240.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.527895</td>\n",
       "      <td>0.941933</td>\n",
       "      <td>12.389900</td>\n",
       "      <td>244.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.499061</td>\n",
       "      <td>0.941933</td>\n",
       "      <td>12.696000</td>\n",
       "      <td>238.737000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-SentMix\n",
      "{'eval_loss': 4.8613739013671875, 'eval_accuracy': 0.948626577579807, 'eval_f1': 0.9478128474574936, 'eval_precision': 0.9482302967811351, 'eval_recall': 0.9474205830674587, 'eval_runtime': 27.2536, 'eval_samples_per_second': 247.124, 'epoch': 17.5, 'run': './results/roberta-base-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d70832158843daa29eb3419a646ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='140000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140000/143957 6:52:46 < 11:40, 5.65 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>0.590623</td>\n",
       "      <td>0.726163</td>\n",
       "      <td>12.691900</td>\n",
       "      <td>238.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>0.774332</td>\n",
       "      <td>12.415300</td>\n",
       "      <td>244.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.486061</td>\n",
       "      <td>0.789838</td>\n",
       "      <td>12.690500</td>\n",
       "      <td>238.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.574582</td>\n",
       "      <td>0.727153</td>\n",
       "      <td>12.593000</td>\n",
       "      <td>240.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.549620</td>\n",
       "      <td>0.784889</td>\n",
       "      <td>13.063100</td>\n",
       "      <td>232.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.511033</td>\n",
       "      <td>0.821841</td>\n",
       "      <td>12.395000</td>\n",
       "      <td>244.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.523917</td>\n",
       "      <td>0.799406</td>\n",
       "      <td>12.883400</td>\n",
       "      <td>235.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.592733</td>\n",
       "      <td>0.758166</td>\n",
       "      <td>12.883700</td>\n",
       "      <td>235.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.469200</td>\n",
       "      <td>0.500173</td>\n",
       "      <td>0.782580</td>\n",
       "      <td>12.707800</td>\n",
       "      <td>238.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.449695</td>\n",
       "      <td>0.822831</td>\n",
       "      <td>12.924700</td>\n",
       "      <td>234.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.494889</td>\n",
       "      <td>0.797097</td>\n",
       "      <td>12.998500</td>\n",
       "      <td>233.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.486073</td>\n",
       "      <td>0.796437</td>\n",
       "      <td>12.767900</td>\n",
       "      <td>237.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.455631</td>\n",
       "      <td>0.828110</td>\n",
       "      <td>13.050800</td>\n",
       "      <td>232.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.497848</td>\n",
       "      <td>0.798416</td>\n",
       "      <td>12.669500</td>\n",
       "      <td>239.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.478342</td>\n",
       "      <td>0.823821</td>\n",
       "      <td>11.890100</td>\n",
       "      <td>254.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.488237</td>\n",
       "      <td>0.815242</td>\n",
       "      <td>11.655000</td>\n",
       "      <td>260.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.490103</td>\n",
       "      <td>0.815242</td>\n",
       "      <td>11.544600</td>\n",
       "      <td>262.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.465820</td>\n",
       "      <td>0.832399</td>\n",
       "      <td>11.470700</td>\n",
       "      <td>264.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.454209</td>\n",
       "      <td>0.839327</td>\n",
       "      <td>12.013600</td>\n",
       "      <td>252.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.465748</td>\n",
       "      <td>0.830089</td>\n",
       "      <td>11.403900</td>\n",
       "      <td>265.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.451498</td>\n",
       "      <td>0.843616</td>\n",
       "      <td>11.529600</td>\n",
       "      <td>262.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.458291</td>\n",
       "      <td>0.832728</td>\n",
       "      <td>11.651100</td>\n",
       "      <td>260.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.841307</td>\n",
       "      <td>12.425600</td>\n",
       "      <td>243.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.840647</td>\n",
       "      <td>12.574000</td>\n",
       "      <td>241.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.453590</td>\n",
       "      <td>0.838337</td>\n",
       "      <td>12.508500</td>\n",
       "      <td>242.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.498040</td>\n",
       "      <td>0.837347</td>\n",
       "      <td>12.437800</td>\n",
       "      <td>243.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.429352</td>\n",
       "      <td>0.858133</td>\n",
       "      <td>12.545700</td>\n",
       "      <td>241.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.467286</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>12.353900</td>\n",
       "      <td>245.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.462958</td>\n",
       "      <td>0.858792</td>\n",
       "      <td>12.413100</td>\n",
       "      <td>244.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.443583</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>12.636800</td>\n",
       "      <td>239.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.474184</td>\n",
       "      <td>0.851864</td>\n",
       "      <td>11.557000</td>\n",
       "      <td>262.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>0.451827</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>11.998500</td>\n",
       "      <td>252.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.449115</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>11.450100</td>\n",
       "      <td>264.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.474902</td>\n",
       "      <td>0.860112</td>\n",
       "      <td>11.758600</td>\n",
       "      <td>257.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.445357</td>\n",
       "      <td>0.864731</td>\n",
       "      <td>12.089000</td>\n",
       "      <td>250.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.463723</td>\n",
       "      <td>0.857473</td>\n",
       "      <td>12.215600</td>\n",
       "      <td>248.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.446850</td>\n",
       "      <td>0.857803</td>\n",
       "      <td>12.579200</td>\n",
       "      <td>240.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.456925</td>\n",
       "      <td>0.849555</td>\n",
       "      <td>12.175700</td>\n",
       "      <td>248.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.461468</td>\n",
       "      <td>0.851864</td>\n",
       "      <td>12.301400</td>\n",
       "      <td>246.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.473278</td>\n",
       "      <td>0.853184</td>\n",
       "      <td>11.900600</td>\n",
       "      <td>254.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.429670</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>11.792100</td>\n",
       "      <td>257.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.443913</td>\n",
       "      <td>0.870670</td>\n",
       "      <td>11.719400</td>\n",
       "      <td>258.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.358600</td>\n",
       "      <td>0.429345</td>\n",
       "      <td>0.873969</td>\n",
       "      <td>11.268700</td>\n",
       "      <td>268.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.437557</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>12.239700</td>\n",
       "      <td>247.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.478578</td>\n",
       "      <td>0.865061</td>\n",
       "      <td>12.170800</td>\n",
       "      <td>249.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.441597</td>\n",
       "      <td>0.870340</td>\n",
       "      <td>12.095100</td>\n",
       "      <td>250.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.452488</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>11.646700</td>\n",
       "      <td>260.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.473218</td>\n",
       "      <td>0.862422</td>\n",
       "      <td>11.766700</td>\n",
       "      <td>257.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>0.470193</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>11.874000</td>\n",
       "      <td>255.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.447548</td>\n",
       "      <td>0.876278</td>\n",
       "      <td>11.937100</td>\n",
       "      <td>253.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.443178</td>\n",
       "      <td>0.871660</td>\n",
       "      <td>11.483200</td>\n",
       "      <td>263.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.474541</td>\n",
       "      <td>0.860112</td>\n",
       "      <td>11.495200</td>\n",
       "      <td>263.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.446243</td>\n",
       "      <td>0.870670</td>\n",
       "      <td>11.602300</td>\n",
       "      <td>261.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.334700</td>\n",
       "      <td>0.453111</td>\n",
       "      <td>0.868690</td>\n",
       "      <td>11.571000</td>\n",
       "      <td>261.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.447507</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>11.627000</td>\n",
       "      <td>260.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.462216</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>11.598600</td>\n",
       "      <td>261.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.340700</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.879578</td>\n",
       "      <td>11.410500</td>\n",
       "      <td>265.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.458007</td>\n",
       "      <td>0.872319</td>\n",
       "      <td>11.699600</td>\n",
       "      <td>259.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.447517</td>\n",
       "      <td>0.869020</td>\n",
       "      <td>11.345400</td>\n",
       "      <td>267.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.477001</td>\n",
       "      <td>0.886176</td>\n",
       "      <td>12.193300</td>\n",
       "      <td>248.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.459216</td>\n",
       "      <td>0.868030</td>\n",
       "      <td>11.735400</td>\n",
       "      <td>258.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.490285</td>\n",
       "      <td>0.860772</td>\n",
       "      <td>11.420100</td>\n",
       "      <td>265.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.438705</td>\n",
       "      <td>0.878918</td>\n",
       "      <td>11.386400</td>\n",
       "      <td>266.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.463747</td>\n",
       "      <td>0.873639</td>\n",
       "      <td>11.445400</td>\n",
       "      <td>264.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.463638</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>11.461500</td>\n",
       "      <td>264.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.429671</td>\n",
       "      <td>0.873639</td>\n",
       "      <td>11.624900</td>\n",
       "      <td>260.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.451586</td>\n",
       "      <td>0.885846</td>\n",
       "      <td>11.344500</td>\n",
       "      <td>267.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.446912</td>\n",
       "      <td>0.885186</td>\n",
       "      <td>11.251700</td>\n",
       "      <td>269.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.428564</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>11.217900</td>\n",
       "      <td>270.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.451404</td>\n",
       "      <td>0.871990</td>\n",
       "      <td>11.500700</td>\n",
       "      <td>263.548000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-WordMix\n",
      "{'eval_loss': 3.155343770980835, 'eval_accuracy': 0.9371937639198218, 'eval_f1': 0.936056999465882, 'eval_precision': 0.9376239927656961, 'eval_recall': 0.9347750381628013, 'eval_runtime': 24.5179, 'eval_samples_per_second': 274.697, 'epoch': 19.45, 'run': './results/roberta-base-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e068e983184df29443a1a27c1e4b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensors must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-de585e8999e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m         )\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# test with ORIG data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    919\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                     \u001b[1;31m# Approximation of exponential moving average of square of gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m                     \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_approx_sq_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg_sq_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m                     \u001b[0mupdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36m_approx_sq_grad\u001b[1;34m(exp_avg_sq_row, exp_avg_sq_col)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mr_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq_row\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mexp_avg_sq_row\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mc_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_factor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_factor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tensors must be 2-D"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    for t in ts:  \n",
    "    \n",
    "        t_str = t.__class__.__name__\n",
    "        checkpoint = './results/' + MODEL_NAME + '-targeted-' + t_str\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "        dataset = load_dataset('glue', 'sst2', split='train[:90%]') \n",
    "        dataset.rename_column_('sentence', 'text')\n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[90%:]')\n",
    "        test_dataset.rename_column_('sentence', 'text') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 20\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "#         tmcb = TargetedMixturesCallback(\n",
    "#             dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "#             device=device\n",
    "#         )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "        tmc = TargetedMixturesCollator(\n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=t,\n",
    "            target_pairs=[(0,1),(1,0)],\n",
    "            target_prob=0.25,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            adafactor=True,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics_w_soft_target,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=tmc,\n",
    "            callbacks=[escb] # [tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.compute_metrics = compute_metrics\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        # trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>run</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.211820</td>\n",
       "      <td>0.931552</td>\n",
       "      <td>0.930009</td>\n",
       "      <td>0.934297</td>\n",
       "      <td>0.927184</td>\n",
       "      <td>27.3246</td>\n",
       "      <td>246.481</td>\n",
       "      <td>12.50</td>\n",
       "      <td>./results/bert-base-uncased-targeted-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.284340</td>\n",
       "      <td>0.937491</td>\n",
       "      <td>0.936605</td>\n",
       "      <td>0.936288</td>\n",
       "      <td>0.936941</td>\n",
       "      <td>27.3015</td>\n",
       "      <td>246.690</td>\n",
       "      <td>13.34</td>\n",
       "      <td>./results/bert-base-uncased-targeted-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.085492</td>\n",
       "      <td>0.941054</td>\n",
       "      <td>0.940202</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.940411</td>\n",
       "      <td>27.4086</td>\n",
       "      <td>245.726</td>\n",
       "      <td>10.28</td>\n",
       "      <td>./results/bert-base-uncased-targeted-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.832774</td>\n",
       "      <td>0.944766</td>\n",
       "      <td>0.943974</td>\n",
       "      <td>0.943725</td>\n",
       "      <td>0.944234</td>\n",
       "      <td>27.2466</td>\n",
       "      <td>247.187</td>\n",
       "      <td>10.56</td>\n",
       "      <td>./results/roberta-base-targeted-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.861374</td>\n",
       "      <td>0.948627</td>\n",
       "      <td>0.947813</td>\n",
       "      <td>0.948230</td>\n",
       "      <td>0.947421</td>\n",
       "      <td>27.2536</td>\n",
       "      <td>247.124</td>\n",
       "      <td>17.50</td>\n",
       "      <td>./results/roberta-base-targeted-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.155344</td>\n",
       "      <td>0.937194</td>\n",
       "      <td>0.936057</td>\n",
       "      <td>0.937624</td>\n",
       "      <td>0.934775</td>\n",
       "      <td>24.5179</td>\n",
       "      <td>274.697</td>\n",
       "      <td>19.45</td>\n",
       "      <td>./results/roberta-base-targeted-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy   eval_f1  eval_precision  eval_recall  \\\n",
       "0   4.211820       0.931552  0.930009        0.934297     0.927184   \n",
       "1   4.284340       0.937491  0.936605        0.936288     0.936941   \n",
       "2   3.085492       0.941054  0.940202        0.940000     0.940411   \n",
       "3   3.832774       0.944766  0.943974        0.943725     0.944234   \n",
       "4   4.861374       0.948627  0.947813        0.948230     0.947421   \n",
       "5   3.155344       0.937194  0.936057        0.937624     0.934775   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  epoch  \\\n",
       "0       27.3246                  246.481  12.50   \n",
       "1       27.3015                  246.690  13.34   \n",
       "2       27.4086                  245.726  10.28   \n",
       "3       27.2466                  247.187  10.56   \n",
       "4       27.2536                  247.124  17.50   \n",
       "5       24.5179                  274.697  19.45   \n",
       "\n",
       "                                            run  test  \n",
       "0  ./results/bert-base-uncased-targeted-TextMix  ORIG  \n",
       "1  ./results/bert-base-uncased-targeted-SentMix  ORIG  \n",
       "2  ./results/bert-base-uncased-targeted-WordMix  ORIG  \n",
       "3       ./results/roberta-base-targeted-TextMix  ORIG  \n",
       "4       ./results/roberta-base-targeted-SentMix  ORIG  \n",
       "5       ./results/roberta-base-targeted-WordMix  ORIG  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_targeted_r2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
