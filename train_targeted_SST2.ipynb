{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_core_web_sm' (2.3.1) requires spaCy >=2.3.0,<2.4.0 and is incompatible with the current version (3.0.3). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "        \n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "    if len(labels.shape) > 1: \n",
    "        acc = acc_at_k(labels, preds, k=2)\n",
    "        return { 'accuracy': acc }        \n",
    "    else:\n",
    "        acc = accuracy_score(labels, preds.argmax(-1))\n",
    "        return { 'accuracy': acc }        \n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        if len(labels.shape) > 1: \n",
    "            loss = CEwST_loss(logits, labels)\n",
    "        else:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, \n",
    "                 tokenize_fn, \n",
    "                 transform, \n",
    "                 transform_prob=1.0, \n",
    "                 target_pairs=[], \n",
    "                 target_prob=1.0, \n",
    "                 num_classes=2):\n",
    "        \n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.transform_prob = transform_prob\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        if torch.rand(1) < self.transform_prob:\n",
    "            batch = self.transform(\n",
    "                batch, \n",
    "                self.target_pairs,   \n",
    "                self.target_prob,\n",
    "                self.num_classes\n",
    "            )\n",
    "        text, labels = batch\n",
    "        labels = torch.tensor(labels)\n",
    "        if len(labels.shape) == 1:\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=self.num_classes)\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = labels\n",
    "        batch.pop('idx', None)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "ts = [TextMix(), SentMix(), WordMix()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "<ipython-input-5-48fcbb87dc06>:13: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  dataset.rename_column_('sentence', 'text')\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be82f10a3c5642488110535765d3a4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='110000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110000/143957 3:09:55 < 58:37, 9.65 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.473142</td>\n",
       "      <td>0.804355</td>\n",
       "      <td>8.185000</td>\n",
       "      <td>370.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.356218</td>\n",
       "      <td>0.891455</td>\n",
       "      <td>8.436100</td>\n",
       "      <td>359.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.343995</td>\n",
       "      <td>0.911250</td>\n",
       "      <td>8.508600</td>\n",
       "      <td>356.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.317531</td>\n",
       "      <td>0.917519</td>\n",
       "      <td>8.166500</td>\n",
       "      <td>371.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.344437</td>\n",
       "      <td>0.897064</td>\n",
       "      <td>8.717600</td>\n",
       "      <td>347.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.331945</td>\n",
       "      <td>0.914880</td>\n",
       "      <td>8.158300</td>\n",
       "      <td>371.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.303418</td>\n",
       "      <td>0.921808</td>\n",
       "      <td>8.574900</td>\n",
       "      <td>353.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.350693</td>\n",
       "      <td>0.919169</td>\n",
       "      <td>8.702100</td>\n",
       "      <td>348.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.310931</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>8.530900</td>\n",
       "      <td>355.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.316047</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>7.991400</td>\n",
       "      <td>379.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.293100</td>\n",
       "      <td>0.351474</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>8.557400</td>\n",
       "      <td>354.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>0.913230</td>\n",
       "      <td>8.228900</td>\n",
       "      <td>368.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.915869</td>\n",
       "      <td>8.397800</td>\n",
       "      <td>360.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.333625</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>8.174700</td>\n",
       "      <td>370.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.361237</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>8.525600</td>\n",
       "      <td>355.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.327686</td>\n",
       "      <td>0.912240</td>\n",
       "      <td>8.349200</td>\n",
       "      <td>363.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.350226</td>\n",
       "      <td>0.910921</td>\n",
       "      <td>8.482100</td>\n",
       "      <td>357.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.310648</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>7.917100</td>\n",
       "      <td>382.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>0.329274</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>8.252200</td>\n",
       "      <td>367.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>0.344182</td>\n",
       "      <td>0.932365</td>\n",
       "      <td>8.444700</td>\n",
       "      <td>358.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.344211</td>\n",
       "      <td>0.910921</td>\n",
       "      <td>8.356200</td>\n",
       "      <td>362.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.350773</td>\n",
       "      <td>0.933355</td>\n",
       "      <td>7.962200</td>\n",
       "      <td>380.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.348204</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>8.655600</td>\n",
       "      <td>350.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.372368</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>8.675800</td>\n",
       "      <td>349.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.339845</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>8.352200</td>\n",
       "      <td>362.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.355471</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>8.341800</td>\n",
       "      <td>363.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.377280</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>8.451300</td>\n",
       "      <td>358.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.345616</td>\n",
       "      <td>0.938634</td>\n",
       "      <td>8.301200</td>\n",
       "      <td>365.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.207600</td>\n",
       "      <td>0.326397</td>\n",
       "      <td>0.930386</td>\n",
       "      <td>9.256500</td>\n",
       "      <td>327.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.316058</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>8.327500</td>\n",
       "      <td>363.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.343629</td>\n",
       "      <td>0.941933</td>\n",
       "      <td>8.430000</td>\n",
       "      <td>359.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.342059</td>\n",
       "      <td>0.930386</td>\n",
       "      <td>8.307100</td>\n",
       "      <td>364.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.369924</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>8.284500</td>\n",
       "      <td>365.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.325264</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>8.221700</td>\n",
       "      <td>368.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.937974</td>\n",
       "      <td>8.347600</td>\n",
       "      <td>363.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.359288</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>8.584200</td>\n",
       "      <td>353.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.353727</td>\n",
       "      <td>0.940944</td>\n",
       "      <td>8.229900</td>\n",
       "      <td>368.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.366135</td>\n",
       "      <td>0.929726</td>\n",
       "      <td>8.392000</td>\n",
       "      <td>361.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>8.585800</td>\n",
       "      <td>353.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.351475</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>8.506900</td>\n",
       "      <td>356.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.404891</td>\n",
       "      <td>0.933355</td>\n",
       "      <td>8.497700</td>\n",
       "      <td>356.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.369462</td>\n",
       "      <td>0.927747</td>\n",
       "      <td>8.413200</td>\n",
       "      <td>360.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.334822</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>8.133600</td>\n",
       "      <td>372.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.357858</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>8.111500</td>\n",
       "      <td>373.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.342753</td>\n",
       "      <td>0.950841</td>\n",
       "      <td>7.909300</td>\n",
       "      <td>383.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.384022</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>8.357400</td>\n",
       "      <td>362.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.366192</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>7.892300</td>\n",
       "      <td>384.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.371946</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>7.953000</td>\n",
       "      <td>381.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.373515</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>7.929900</td>\n",
       "      <td>382.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.383778</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>8.120400</td>\n",
       "      <td>373.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.392381</td>\n",
       "      <td>0.941933</td>\n",
       "      <td>8.375500</td>\n",
       "      <td>361.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.369654</td>\n",
       "      <td>0.938634</td>\n",
       "      <td>8.456800</td>\n",
       "      <td>358.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.943583</td>\n",
       "      <td>8.298400</td>\n",
       "      <td>365.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.163800</td>\n",
       "      <td>0.373896</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>8.287600</td>\n",
       "      <td>365.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.407896</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>8.422000</td>\n",
       "      <td>359.889000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
      "{'eval_loss': 0.3205556571483612, 'eval_accuracy': 0.9478841870824053, 'eval_runtime': 21.8997, 'eval_samples_per_second': 307.539, 'epoch': 15.28, 'run': './results/bert-base-uncased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30eff48248445c280ea8422c6e2daed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='56000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56000/143957 1:36:10 < 2:31:04, 9.70 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.421433</td>\n",
       "      <td>0.832728</td>\n",
       "      <td>8.405700</td>\n",
       "      <td>360.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.340744</td>\n",
       "      <td>0.875289</td>\n",
       "      <td>8.596500</td>\n",
       "      <td>352.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.318136</td>\n",
       "      <td>0.893105</td>\n",
       "      <td>8.363600</td>\n",
       "      <td>362.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.332224</td>\n",
       "      <td>0.908941</td>\n",
       "      <td>8.331800</td>\n",
       "      <td>363.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>0.328997</td>\n",
       "      <td>0.895084</td>\n",
       "      <td>8.862300</td>\n",
       "      <td>342.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.335676</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>8.491700</td>\n",
       "      <td>356.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.337332</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>8.596300</td>\n",
       "      <td>352.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.343670</td>\n",
       "      <td>0.910591</td>\n",
       "      <td>8.453300</td>\n",
       "      <td>358.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.356221</td>\n",
       "      <td>0.911250</td>\n",
       "      <td>8.377900</td>\n",
       "      <td>361.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.349277</td>\n",
       "      <td>0.911910</td>\n",
       "      <td>8.287600</td>\n",
       "      <td>365.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.372920</td>\n",
       "      <td>0.903002</td>\n",
       "      <td>8.926000</td>\n",
       "      <td>339.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.367460</td>\n",
       "      <td>0.891455</td>\n",
       "      <td>8.481100</td>\n",
       "      <td>357.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.354783</td>\n",
       "      <td>0.913230</td>\n",
       "      <td>8.641800</td>\n",
       "      <td>350.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.334134</td>\n",
       "      <td>0.922138</td>\n",
       "      <td>8.454400</td>\n",
       "      <td>358.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.365730</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>8.471700</td>\n",
       "      <td>357.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.360511</td>\n",
       "      <td>0.921808</td>\n",
       "      <td>8.582900</td>\n",
       "      <td>353.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.355651</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>8.608500</td>\n",
       "      <td>352.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.319854</td>\n",
       "      <td>0.934015</td>\n",
       "      <td>8.158600</td>\n",
       "      <td>371.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.419039</td>\n",
       "      <td>0.907951</td>\n",
       "      <td>8.574800</td>\n",
       "      <td>353.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.371618</td>\n",
       "      <td>0.908281</td>\n",
       "      <td>8.625400</td>\n",
       "      <td>351.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.386134</td>\n",
       "      <td>0.919499</td>\n",
       "      <td>8.434100</td>\n",
       "      <td>359.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.395290</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>8.190700</td>\n",
       "      <td>370.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.359442</td>\n",
       "      <td>0.912240</td>\n",
       "      <td>8.704200</td>\n",
       "      <td>348.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.381564</td>\n",
       "      <td>0.911250</td>\n",
       "      <td>8.825600</td>\n",
       "      <td>343.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.368563</td>\n",
       "      <td>0.925767</td>\n",
       "      <td>8.301000</td>\n",
       "      <td>365.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.386893</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>8.410300</td>\n",
       "      <td>360.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.389561</td>\n",
       "      <td>0.921148</td>\n",
       "      <td>8.486700</td>\n",
       "      <td>357.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.378587</td>\n",
       "      <td>0.923458</td>\n",
       "      <td>8.610700</td>\n",
       "      <td>352.003000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-SentMix\n",
      "{'eval_loss': 0.2529617249965668, 'eval_accuracy': 0.948626577579807, 'eval_runtime': 21.7917, 'eval_samples_per_second': 309.063, 'epoch': 7.78, 'run': './results/bert-base-uncased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96a101aba504b51bda71ad3719e775f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='64000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 64000/143957 1:51:15 < 2:19:00, 9.59 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.478888</td>\n",
       "      <td>0.779941</td>\n",
       "      <td>8.456500</td>\n",
       "      <td>358.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>0.371759</td>\n",
       "      <td>0.831739</td>\n",
       "      <td>8.364500</td>\n",
       "      <td>362.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.395251</td>\n",
       "      <td>0.825470</td>\n",
       "      <td>8.437300</td>\n",
       "      <td>359.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.376948</td>\n",
       "      <td>0.839327</td>\n",
       "      <td>8.391800</td>\n",
       "      <td>361.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.475109</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>8.695100</td>\n",
       "      <td>348.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.360600</td>\n",
       "      <td>0.352527</td>\n",
       "      <td>0.866711</td>\n",
       "      <td>8.301900</td>\n",
       "      <td>365.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.436052</td>\n",
       "      <td>0.868690</td>\n",
       "      <td>8.546400</td>\n",
       "      <td>354.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.366088</td>\n",
       "      <td>0.848895</td>\n",
       "      <td>8.698500</td>\n",
       "      <td>348.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.404838</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>8.316500</td>\n",
       "      <td>364.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.386097</td>\n",
       "      <td>0.880567</td>\n",
       "      <td>8.456900</td>\n",
       "      <td>358.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.439107</td>\n",
       "      <td>0.870670</td>\n",
       "      <td>9.103200</td>\n",
       "      <td>332.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>0.425137</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>8.787700</td>\n",
       "      <td>344.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.390531</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>8.608600</td>\n",
       "      <td>352.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.427367</td>\n",
       "      <td>0.888486</td>\n",
       "      <td>8.451500</td>\n",
       "      <td>358.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.426461</td>\n",
       "      <td>0.855823</td>\n",
       "      <td>8.608500</td>\n",
       "      <td>352.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.450821</td>\n",
       "      <td>0.863411</td>\n",
       "      <td>8.627800</td>\n",
       "      <td>351.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.392873</td>\n",
       "      <td>0.874299</td>\n",
       "      <td>8.701200</td>\n",
       "      <td>348.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.379696</td>\n",
       "      <td>0.893435</td>\n",
       "      <td>8.377100</td>\n",
       "      <td>361.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.363206</td>\n",
       "      <td>0.883537</td>\n",
       "      <td>8.609200</td>\n",
       "      <td>352.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.360225</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>8.613200</td>\n",
       "      <td>351.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>0.392553</td>\n",
       "      <td>0.886836</td>\n",
       "      <td>8.717600</td>\n",
       "      <td>347.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.350672</td>\n",
       "      <td>0.895084</td>\n",
       "      <td>8.279000</td>\n",
       "      <td>366.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.391258</td>\n",
       "      <td>0.888156</td>\n",
       "      <td>8.606000</td>\n",
       "      <td>352.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.365388</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>8.921900</td>\n",
       "      <td>339.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.364586</td>\n",
       "      <td>0.892115</td>\n",
       "      <td>8.409500</td>\n",
       "      <td>360.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.406950</td>\n",
       "      <td>0.880238</td>\n",
       "      <td>8.458000</td>\n",
       "      <td>358.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.395639</td>\n",
       "      <td>0.883867</td>\n",
       "      <td>8.600700</td>\n",
       "      <td>352.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.385391</td>\n",
       "      <td>0.886506</td>\n",
       "      <td>8.734700</td>\n",
       "      <td>347.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.411351</td>\n",
       "      <td>0.891785</td>\n",
       "      <td>8.525400</td>\n",
       "      <td>355.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.401251</td>\n",
       "      <td>0.888156</td>\n",
       "      <td>8.522600</td>\n",
       "      <td>355.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.399313</td>\n",
       "      <td>0.887826</td>\n",
       "      <td>8.484900</td>\n",
       "      <td>357.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.403384</td>\n",
       "      <td>0.884856</td>\n",
       "      <td>8.520900</td>\n",
       "      <td>355.713000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-WordMix\n",
      "{'eval_loss': 0.2585805356502533, 'eval_accuracy': 0.9328878990348923, 'eval_runtime': 22.0245, 'eval_samples_per_second': 305.795, 'epoch': 8.89, 'run': './results/bert-base-uncased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9605cf8a81764f968b0a894bdd222443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='48000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48000/143957 1:28:20 < 2:56:37, 9.05 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.454626</td>\n",
       "      <td>0.859452</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>367.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.418405</td>\n",
       "      <td>0.874959</td>\n",
       "      <td>8.331000</td>\n",
       "      <td>363.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.390600</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>8.389200</td>\n",
       "      <td>361.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.446195</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>8.108000</td>\n",
       "      <td>373.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.364362</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>8.553900</td>\n",
       "      <td>354.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.395606</td>\n",
       "      <td>0.909271</td>\n",
       "      <td>7.917100</td>\n",
       "      <td>382.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.391262</td>\n",
       "      <td>0.887166</td>\n",
       "      <td>8.336300</td>\n",
       "      <td>363.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.392536</td>\n",
       "      <td>0.906961</td>\n",
       "      <td>8.421200</td>\n",
       "      <td>359.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.437222</td>\n",
       "      <td>0.889146</td>\n",
       "      <td>8.182100</td>\n",
       "      <td>370.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.384640</td>\n",
       "      <td>0.894424</td>\n",
       "      <td>7.955100</td>\n",
       "      <td>381.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.500120</td>\n",
       "      <td>0.877598</td>\n",
       "      <td>8.485500</td>\n",
       "      <td>357.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.414584</td>\n",
       "      <td>0.900693</td>\n",
       "      <td>8.216800</td>\n",
       "      <td>368.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.415126</td>\n",
       "      <td>0.908611</td>\n",
       "      <td>8.425400</td>\n",
       "      <td>359.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.357224</td>\n",
       "      <td>0.918179</td>\n",
       "      <td>8.199500</td>\n",
       "      <td>369.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.393680</td>\n",
       "      <td>0.894424</td>\n",
       "      <td>8.204000</td>\n",
       "      <td>369.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.400197</td>\n",
       "      <td>0.888156</td>\n",
       "      <td>8.245600</td>\n",
       "      <td>367.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.421882</td>\n",
       "      <td>0.874959</td>\n",
       "      <td>8.222300</td>\n",
       "      <td>368.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.445154</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>7.709200</td>\n",
       "      <td>393.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.433938</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>8.081500</td>\n",
       "      <td>375.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>0.468843</td>\n",
       "      <td>0.836358</td>\n",
       "      <td>8.239200</td>\n",
       "      <td>367.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.696317</td>\n",
       "      <td>0.531838</td>\n",
       "      <td>8.207200</td>\n",
       "      <td>369.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.691717</td>\n",
       "      <td>0.532498</td>\n",
       "      <td>8.009600</td>\n",
       "      <td>378.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.695677</td>\n",
       "      <td>0.526889</td>\n",
       "      <td>8.272300</td>\n",
       "      <td>366.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.695321</td>\n",
       "      <td>0.534477</td>\n",
       "      <td>8.332900</td>\n",
       "      <td>363.739000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-TextMix\n",
      "{'eval_loss': 0.28808724880218506, 'eval_accuracy': 0.9260579064587974, 'eval_runtime': 20.8713, 'eval_samples_per_second': 322.692, 'epoch': 6.67, 'run': './results/roberta-base-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da886289086841b7bb0e42d6059c7bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='26000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 26000/143957 47:52 < 3:37:12, 9.05 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>0.440461</td>\n",
       "      <td>0.830419</td>\n",
       "      <td>8.121500</td>\n",
       "      <td>373.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.369077</td>\n",
       "      <td>0.912240</td>\n",
       "      <td>8.291400</td>\n",
       "      <td>365.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.320892</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>8.399500</td>\n",
       "      <td>360.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.339751</td>\n",
       "      <td>0.918839</td>\n",
       "      <td>8.293800</td>\n",
       "      <td>365.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>0.389388</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>8.598500</td>\n",
       "      <td>352.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.334920</td>\n",
       "      <td>0.920488</td>\n",
       "      <td>8.307400</td>\n",
       "      <td>364.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.326016</td>\n",
       "      <td>0.904652</td>\n",
       "      <td>8.206800</td>\n",
       "      <td>369.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>0.373387</td>\n",
       "      <td>0.878588</td>\n",
       "      <td>8.373600</td>\n",
       "      <td>361.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.337807</td>\n",
       "      <td>0.908941</td>\n",
       "      <td>8.245700</td>\n",
       "      <td>367.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.339651</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>8.348900</td>\n",
       "      <td>363.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.535106</td>\n",
       "      <td>0.797756</td>\n",
       "      <td>8.794500</td>\n",
       "      <td>344.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.689794</td>\n",
       "      <td>0.552953</td>\n",
       "      <td>8.563500</td>\n",
       "      <td>353.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.687463</td>\n",
       "      <td>0.547674</td>\n",
       "      <td>8.427100</td>\n",
       "      <td>359.674000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-SentMix\n",
      "{'eval_loss': 0.2426948845386505, 'eval_accuracy': 0.9312546399406088, 'eval_runtime': 21.06, 'eval_samples_per_second': 319.8, 'epoch': 3.61, 'run': './results/roberta-base-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc90f6730863414c839b39b723c3cb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 52:01 < 3:35:28, 8.97 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.488562</td>\n",
       "      <td>0.750907</td>\n",
       "      <td>8.419100</td>\n",
       "      <td>360.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.409672</td>\n",
       "      <td>0.847575</td>\n",
       "      <td>8.262400</td>\n",
       "      <td>366.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.405550</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>8.357200</td>\n",
       "      <td>362.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.407799</td>\n",
       "      <td>0.861432</td>\n",
       "      <td>8.341900</td>\n",
       "      <td>363.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.494683</td>\n",
       "      <td>0.803365</td>\n",
       "      <td>8.473200</td>\n",
       "      <td>357.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.354543</td>\n",
       "      <td>0.857803</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>370.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.569196</td>\n",
       "      <td>0.820191</td>\n",
       "      <td>8.466300</td>\n",
       "      <td>358.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.471700</td>\n",
       "      <td>0.485788</td>\n",
       "      <td>0.759815</td>\n",
       "      <td>8.565700</td>\n",
       "      <td>353.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.442415</td>\n",
       "      <td>0.835038</td>\n",
       "      <td>8.376600</td>\n",
       "      <td>361.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.841966</td>\n",
       "      <td>8.421800</td>\n",
       "      <td>359.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.539127</td>\n",
       "      <td>0.797756</td>\n",
       "      <td>9.038100</td>\n",
       "      <td>335.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.598100</td>\n",
       "      <td>0.633483</td>\n",
       "      <td>0.688552</td>\n",
       "      <td>8.254100</td>\n",
       "      <td>367.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.573700</td>\n",
       "      <td>0.606789</td>\n",
       "      <td>0.719564</td>\n",
       "      <td>8.357600</td>\n",
       "      <td>362.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.567604</td>\n",
       "      <td>0.683933</td>\n",
       "      <td>8.402300</td>\n",
       "      <td>360.733000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-WordMix\n",
      "{'eval_loss': 0.3388954699039459, 'eval_accuracy': 0.9171492204899777, 'eval_runtime': 21.1711, 'eval_samples_per_second': 318.122, 'epoch': 3.89, 'run': './results/roberta-base-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4bd0fe523b48e6b4cf31ce99b81692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='32000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32000/143957 1:20:32 < 4:41:47, 6.62 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>0.457489</td>\n",
       "      <td>0.824810</td>\n",
       "      <td>12.867200</td>\n",
       "      <td>235.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.428434</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>13.636000</td>\n",
       "      <td>222.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.420024</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>12.817600</td>\n",
       "      <td>236.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.369465</td>\n",
       "      <td>0.899703</td>\n",
       "      <td>12.792000</td>\n",
       "      <td>236.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.406631</td>\n",
       "      <td>0.900363</td>\n",
       "      <td>13.596600</td>\n",
       "      <td>222.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.340363</td>\n",
       "      <td>0.913560</td>\n",
       "      <td>12.483000</td>\n",
       "      <td>242.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.900363</td>\n",
       "      <td>13.397700</td>\n",
       "      <td>226.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.395284</td>\n",
       "      <td>0.888816</td>\n",
       "      <td>13.397200</td>\n",
       "      <td>226.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.404085</td>\n",
       "      <td>0.881887</td>\n",
       "      <td>12.893800</td>\n",
       "      <td>235.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.380088</td>\n",
       "      <td>0.898053</td>\n",
       "      <td>12.811500</td>\n",
       "      <td>236.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.432670</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>14.004100</td>\n",
       "      <td>216.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.381436</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>12.971300</td>\n",
       "      <td>233.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.372276</td>\n",
       "      <td>0.888816</td>\n",
       "      <td>13.509000</td>\n",
       "      <td>224.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.404919</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>13.254600</td>\n",
       "      <td>228.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.386296</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>13.247400</td>\n",
       "      <td>228.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.316500</td>\n",
       "      <td>0.376261</td>\n",
       "      <td>0.892775</td>\n",
       "      <td>13.118700</td>\n",
       "      <td>231.045000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-targeted-TextMix\n",
      "{'eval_loss': 0.28366708755493164, 'eval_accuracy': 0.9250185597624351, 'eval_runtime': 33.0223, 'eval_samples_per_second': 203.953, 'epoch': 4.45, 'run': './results/xlnet-base-cased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b59b6996d34601b0dd5937b5dee535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='116000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116000/143957 4:51:24 < 1:10:13, 6.63 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.443552</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>13.314400</td>\n",
       "      <td>227.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.471930</td>\n",
       "      <td>0.865721</td>\n",
       "      <td>13.297400</td>\n",
       "      <td>227.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.401099</td>\n",
       "      <td>0.880238</td>\n",
       "      <td>13.455800</td>\n",
       "      <td>225.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.391989</td>\n",
       "      <td>0.897394</td>\n",
       "      <td>12.872600</td>\n",
       "      <td>235.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.385877</td>\n",
       "      <td>0.900033</td>\n",
       "      <td>13.733100</td>\n",
       "      <td>220.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.373992</td>\n",
       "      <td>0.903332</td>\n",
       "      <td>12.593000</td>\n",
       "      <td>240.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.491413</td>\n",
       "      <td>0.875619</td>\n",
       "      <td>13.282400</td>\n",
       "      <td>228.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>0.444837</td>\n",
       "      <td>0.875289</td>\n",
       "      <td>13.501100</td>\n",
       "      <td>224.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.411619</td>\n",
       "      <td>0.883207</td>\n",
       "      <td>13.263100</td>\n",
       "      <td>228.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.353200</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>0.875948</td>\n",
       "      <td>12.462000</td>\n",
       "      <td>243.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.398953</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>14.118700</td>\n",
       "      <td>214.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.448735</td>\n",
       "      <td>0.885846</td>\n",
       "      <td>13.199300</td>\n",
       "      <td>229.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.362883</td>\n",
       "      <td>0.903002</td>\n",
       "      <td>13.341200</td>\n",
       "      <td>227.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.335543</td>\n",
       "      <td>0.903662</td>\n",
       "      <td>12.968800</td>\n",
       "      <td>233.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.359095</td>\n",
       "      <td>0.903992</td>\n",
       "      <td>13.289500</td>\n",
       "      <td>228.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.387466</td>\n",
       "      <td>0.896404</td>\n",
       "      <td>13.117800</td>\n",
       "      <td>231.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.362216</td>\n",
       "      <td>0.876938</td>\n",
       "      <td>13.714000</td>\n",
       "      <td>221.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.368130</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>11.804200</td>\n",
       "      <td>256.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.369111</td>\n",
       "      <td>0.894424</td>\n",
       "      <td>13.088800</td>\n",
       "      <td>231.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.366644</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>13.599200</td>\n",
       "      <td>222.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.359003</td>\n",
       "      <td>0.902672</td>\n",
       "      <td>13.343600</td>\n",
       "      <td>227.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.359617</td>\n",
       "      <td>0.897723</td>\n",
       "      <td>12.521800</td>\n",
       "      <td>242.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.368584</td>\n",
       "      <td>0.898054</td>\n",
       "      <td>13.437600</td>\n",
       "      <td>225.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.379269</td>\n",
       "      <td>0.881557</td>\n",
       "      <td>14.181400</td>\n",
       "      <td>213.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.358407</td>\n",
       "      <td>0.908611</td>\n",
       "      <td>12.742600</td>\n",
       "      <td>237.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.362045</td>\n",
       "      <td>0.911910</td>\n",
       "      <td>12.569300</td>\n",
       "      <td>241.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.357313</td>\n",
       "      <td>0.888156</td>\n",
       "      <td>13.086300</td>\n",
       "      <td>231.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.335839</td>\n",
       "      <td>0.904322</td>\n",
       "      <td>13.353400</td>\n",
       "      <td>226.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.375920</td>\n",
       "      <td>0.903992</td>\n",
       "      <td>12.938500</td>\n",
       "      <td>234.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.366137</td>\n",
       "      <td>0.898053</td>\n",
       "      <td>13.014500</td>\n",
       "      <td>232.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.357060</td>\n",
       "      <td>0.911580</td>\n",
       "      <td>12.668600</td>\n",
       "      <td>239.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.364678</td>\n",
       "      <td>0.907951</td>\n",
       "      <td>12.965000</td>\n",
       "      <td>233.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.388991</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>13.345000</td>\n",
       "      <td>227.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.349131</td>\n",
       "      <td>0.898383</td>\n",
       "      <td>12.973600</td>\n",
       "      <td>233.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.329479</td>\n",
       "      <td>0.915869</td>\n",
       "      <td>12.790600</td>\n",
       "      <td>236.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.365091</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>14.157200</td>\n",
       "      <td>214.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.336619</td>\n",
       "      <td>0.914550</td>\n",
       "      <td>13.210000</td>\n",
       "      <td>229.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.900363</td>\n",
       "      <td>13.471900</td>\n",
       "      <td>224.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.331678</td>\n",
       "      <td>0.897064</td>\n",
       "      <td>14.001900</td>\n",
       "      <td>216.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.350692</td>\n",
       "      <td>0.915540</td>\n",
       "      <td>13.809100</td>\n",
       "      <td>219.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.315743</td>\n",
       "      <td>0.914880</td>\n",
       "      <td>13.358400</td>\n",
       "      <td>226.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.345118</td>\n",
       "      <td>0.911250</td>\n",
       "      <td>13.520300</td>\n",
       "      <td>224.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>0.338102</td>\n",
       "      <td>0.926757</td>\n",
       "      <td>12.860200</td>\n",
       "      <td>235.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.350805</td>\n",
       "      <td>0.909271</td>\n",
       "      <td>13.362900</td>\n",
       "      <td>226.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>0.316177</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>12.721500</td>\n",
       "      <td>238.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.916529</td>\n",
       "      <td>13.140400</td>\n",
       "      <td>230.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.343270</td>\n",
       "      <td>0.929396</td>\n",
       "      <td>12.723900</td>\n",
       "      <td>238.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.318752</td>\n",
       "      <td>0.932366</td>\n",
       "      <td>12.555700</td>\n",
       "      <td>241.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.325081</td>\n",
       "      <td>0.925107</td>\n",
       "      <td>12.982100</td>\n",
       "      <td>233.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.344442</td>\n",
       "      <td>0.929066</td>\n",
       "      <td>13.535600</td>\n",
       "      <td>223.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.328182</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>13.778700</td>\n",
       "      <td>219.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.332809</td>\n",
       "      <td>0.931046</td>\n",
       "      <td>13.240200</td>\n",
       "      <td>228.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.351067</td>\n",
       "      <td>0.920158</td>\n",
       "      <td>12.772100</td>\n",
       "      <td>237.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.219400</td>\n",
       "      <td>0.347393</td>\n",
       "      <td>0.920818</td>\n",
       "      <td>13.244600</td>\n",
       "      <td>228.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.414222</td>\n",
       "      <td>0.894424</td>\n",
       "      <td>13.863200</td>\n",
       "      <td>218.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.342935</td>\n",
       "      <td>0.924117</td>\n",
       "      <td>13.835800</td>\n",
       "      <td>219.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.345771</td>\n",
       "      <td>0.917849</td>\n",
       "      <td>13.216300</td>\n",
       "      <td>229.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.347951</td>\n",
       "      <td>0.930056</td>\n",
       "      <td>13.715600</td>\n",
       "      <td>220.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-targeted-SentMix\n",
      "{'eval_loss': 0.2852342128753662, 'eval_accuracy': 0.9475872308834447, 'eval_runtime': 33.0679, 'eval_samples_per_second': 203.672, 'epoch': 16.12, 'run': './results/xlnet-base-cased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1a761562db4d1eac1cd89b890cd171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='34000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 34000/143957 1:26:26 < 4:39:32, 6.56 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.601200</td>\n",
       "      <td>0.517693</td>\n",
       "      <td>0.787859</td>\n",
       "      <td>12.863400</td>\n",
       "      <td>235.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.418483</td>\n",
       "      <td>0.823491</td>\n",
       "      <td>13.102000</td>\n",
       "      <td>231.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.478009</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>13.123200</td>\n",
       "      <td>230.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.447416</td>\n",
       "      <td>0.849225</td>\n",
       "      <td>12.662300</td>\n",
       "      <td>239.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.458578</td>\n",
       "      <td>0.837018</td>\n",
       "      <td>13.430900</td>\n",
       "      <td>225.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.357776</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>12.900800</td>\n",
       "      <td>234.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.400482</td>\n",
       "      <td>0.860772</td>\n",
       "      <td>12.712600</td>\n",
       "      <td>238.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>0.443094</td>\n",
       "      <td>0.843946</td>\n",
       "      <td>13.218600</td>\n",
       "      <td>229.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.439822</td>\n",
       "      <td>0.833058</td>\n",
       "      <td>12.797500</td>\n",
       "      <td>236.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.490700</td>\n",
       "      <td>0.491829</td>\n",
       "      <td>0.816562</td>\n",
       "      <td>13.011800</td>\n",
       "      <td>232.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.674667</td>\n",
       "      <td>0.580666</td>\n",
       "      <td>13.804800</td>\n",
       "      <td>219.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.684529</td>\n",
       "      <td>0.564830</td>\n",
       "      <td>13.336300</td>\n",
       "      <td>227.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>0.689536</td>\n",
       "      <td>0.564170</td>\n",
       "      <td>13.279600</td>\n",
       "      <td>228.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.684503</td>\n",
       "      <td>0.567799</td>\n",
       "      <td>12.459200</td>\n",
       "      <td>243.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>0.688342</td>\n",
       "      <td>0.552293</td>\n",
       "      <td>13.256700</td>\n",
       "      <td>228.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.688478</td>\n",
       "      <td>0.555262</td>\n",
       "      <td>12.843600</td>\n",
       "      <td>235.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.688461</td>\n",
       "      <td>0.557242</td>\n",
       "      <td>13.142700</td>\n",
       "      <td>230.622000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-targeted-WordMix\n",
      "{'eval_loss': 0.30975812673568726, 'eval_accuracy': 0.9198218262806236, 'eval_runtime': 32.9464, 'eval_samples_per_second': 204.423, 'epoch': 4.72, 'run': './results/xlnet-base-cased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    for t in ts:  \n",
    "    \n",
    "        t_str = t.__class__.__name__\n",
    "        checkpoint = './results/' + MODEL_NAME + '-targeted-' + t_str\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "        dataset = load_dataset('glue', 'sst2', split='train[:90%]') \n",
    "        dataset.rename_column_('sentence', 'text')\n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[90%:]')\n",
    "        test_dataset.rename_column_('sentence', 'text') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 20\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "#         tmcb = TargetedMixturesCallback(\n",
    "#             dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "#             device=device\n",
    "#         )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "        tmc = TargetedMixturesCollator(\n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=t, \n",
    "            transform_prob=0.5,\n",
    "            target_pairs=[(0,1),(1,0)],\n",
    "            target_prob=0.5,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=tmc,\n",
    "            callbacks=[escb] # [tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        # trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>run</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.320556</td>\n",
       "      <td>0.947884</td>\n",
       "      <td>21.8997</td>\n",
       "      <td>307.539</td>\n",
       "      <td>15.28</td>\n",
       "      <td>./results/bert-base-uncased-targeted-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.252962</td>\n",
       "      <td>0.948627</td>\n",
       "      <td>21.7917</td>\n",
       "      <td>309.063</td>\n",
       "      <td>7.78</td>\n",
       "      <td>./results/bert-base-uncased-targeted-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.258581</td>\n",
       "      <td>0.932888</td>\n",
       "      <td>22.0245</td>\n",
       "      <td>305.795</td>\n",
       "      <td>8.89</td>\n",
       "      <td>./results/bert-base-uncased-targeted-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.288087</td>\n",
       "      <td>0.926058</td>\n",
       "      <td>20.8713</td>\n",
       "      <td>322.692</td>\n",
       "      <td>6.67</td>\n",
       "      <td>./results/roberta-base-targeted-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.242695</td>\n",
       "      <td>0.931255</td>\n",
       "      <td>21.0600</td>\n",
       "      <td>319.800</td>\n",
       "      <td>3.61</td>\n",
       "      <td>./results/roberta-base-targeted-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.338895</td>\n",
       "      <td>0.917149</td>\n",
       "      <td>21.1711</td>\n",
       "      <td>318.122</td>\n",
       "      <td>3.89</td>\n",
       "      <td>./results/roberta-base-targeted-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.283667</td>\n",
       "      <td>0.925019</td>\n",
       "      <td>33.0223</td>\n",
       "      <td>203.953</td>\n",
       "      <td>4.45</td>\n",
       "      <td>./results/xlnet-base-cased-targeted-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.285234</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>33.0679</td>\n",
       "      <td>203.672</td>\n",
       "      <td>16.12</td>\n",
       "      <td>./results/xlnet-base-cased-targeted-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.309758</td>\n",
       "      <td>0.919822</td>\n",
       "      <td>32.9464</td>\n",
       "      <td>204.423</td>\n",
       "      <td>4.72</td>\n",
       "      <td>./results/xlnet-base-cased-targeted-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  epoch  \\\n",
       "0   0.320556       0.947884       21.8997                  307.539  15.28   \n",
       "1   0.252962       0.948627       21.7917                  309.063   7.78   \n",
       "2   0.258581       0.932888       22.0245                  305.795   8.89   \n",
       "3   0.288087       0.926058       20.8713                  322.692   6.67   \n",
       "4   0.242695       0.931255       21.0600                  319.800   3.61   \n",
       "5   0.338895       0.917149       21.1711                  318.122   3.89   \n",
       "6   0.283667       0.925019       33.0223                  203.953   4.45   \n",
       "7   0.285234       0.947587       33.0679                  203.672  16.12   \n",
       "8   0.309758       0.919822       32.9464                  204.423   4.72   \n",
       "\n",
       "                                            run  test  \n",
       "0  ./results/bert-base-uncased-targeted-TextMix  ORIG  \n",
       "1  ./results/bert-base-uncased-targeted-SentMix  ORIG  \n",
       "2  ./results/bert-base-uncased-targeted-WordMix  ORIG  \n",
       "3       ./results/roberta-base-targeted-TextMix  ORIG  \n",
       "4       ./results/roberta-base-targeted-SentMix  ORIG  \n",
       "5       ./results/roberta-base-targeted-WordMix  ORIG  \n",
       "6   ./results/xlnet-base-cased-targeted-TextMix  ORIG  \n",
       "7   ./results/xlnet-base-cased-targeted-SentMix  ORIG  \n",
       "8   ./results/xlnet-base-cased-targeted-WordMix  ORIG  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_targeted_r3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
