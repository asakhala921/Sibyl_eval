{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'sst2').shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate 100 test suites\n",
    "# test_suites = {}\n",
    "# for i in range(100):\n",
    "#     n = 100\n",
    "#     j = random.randint(0, len(dataset['train']['sentence']) - 1 - n)\n",
    "#     X, y = dataset['train']['sentence'][j:j+n], dataset['train']['label'][j:j+n]\n",
    "#     test_suite = {'data': [str(x) for x in X], 'target': y}\n",
    "#     test_suites[i] = test_suite\n",
    "# pkl_save(test_suites, 'assets/SST2/test_suites.pkl')\n",
    "test_suites = pkl_load('assets/SST2/test_suites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "INV_test_suites = transform_test_suites(test_suites, num_transforms=2, task='sentiment', tran='INV')\n",
    "pkl_save(INV_test_suites, 'assets/SST2/INV_test_suites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:09<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "SIB_test_suites = transform_test_suites(test_suites, num_transforms=2, task='sentiment', tran='SIB')\n",
    "pkl_save(SIB_test_suites, 'assets/SST2/SIB_test_suites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "both_test_suites = transform_test_suites(test_suites, num_transforms=2, task='sentiment', tran=None)\n",
    "pkl_save(both_test_suites, 'assets/SST2/both_test_suites.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suites = pkl_load('assets/SST2/test_suites.pkl')\n",
    "INV_test_suites = pkl_load('assets/SST2/INV_test_suites.pkl')\n",
    "SIB_test_suites = pkl_load('assets/SST2/SIB_test_suites.pkl')\n",
    "both_test_suites = pkl_load('assets/SST2/both_test_suites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_pred, y_true):\n",
    "    total = y_true.size(0)\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def load_huggingface_model(model_name, device, max_length=500):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "    def pipeline(sentence):\n",
    "        encode = tokenizer(sentence, \n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=max_length, \n",
    "                           return_tensors=\"pt\").to(device)\n",
    "        logits = model(**encode)[0]\n",
    "        soft_m = torch.softmax(logits, dim=1)\n",
    "        return soft_m\n",
    "    return pipeline\n",
    "\n",
    "MODEL_NAME = \"textattack/bert-base-uncased-SST-2\"\n",
    "model = load_huggingface_model(MODEL_NAME, device, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = {\n",
    "    'test_suites' : test_suites,\n",
    "    'INV_test_suites' : INV_test_suites,\n",
    "    'SIB_test_suites' : SIB_test_suites,\n",
    "    'both_test_suites' : both_test_suites\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting test_suites...\n",
      "test suite 0 acc: 0.99\n",
      "test suite 1 acc: 1.0\n",
      "test suite 2 acc: 1.0\n",
      "test suite 3 acc: 0.99\n",
      "test suite 4 acc: 0.99\n",
      "test suite 5 acc: 0.98\n",
      "test suite 6 acc: 0.96\n",
      "test suite 7 acc: 0.97\n",
      "test suite 8 acc: 0.99\n",
      "test suite 9 acc: 1.0\n",
      "test suite 10 acc: 0.99\n",
      "test suite 11 acc: 0.99\n",
      "test suite 12 acc: 0.99\n",
      "test suite 13 acc: 0.99\n",
      "test suite 14 acc: 1.0\n",
      "test suite 15 acc: 1.0\n",
      "test suite 16 acc: 0.99\n",
      "test suite 17 acc: 0.99\n",
      "test suite 18 acc: 0.96\n",
      "test suite 19 acc: 0.98\n",
      "test suite 20 acc: 0.99\n",
      "test suite 21 acc: 0.99\n",
      "test suite 22 acc: 0.96\n",
      "test suite 23 acc: 0.98\n",
      "test suite 24 acc: 0.99\n",
      "test suite 25 acc: 0.97\n",
      "test suite 26 acc: 0.98\n",
      "test suite 27 acc: 0.98\n",
      "test suite 28 acc: 0.98\n",
      "test suite 29 acc: 0.96\n",
      "test suite 30 acc: 0.99\n",
      "test suite 31 acc: 1.0\n",
      "test suite 32 acc: 0.99\n",
      "test suite 33 acc: 1.0\n",
      "test suite 34 acc: 0.99\n",
      "test suite 35 acc: 0.98\n",
      "test suite 36 acc: 0.96\n",
      "test suite 37 acc: 0.97\n",
      "test suite 38 acc: 0.98\n",
      "test suite 39 acc: 1.0\n",
      "test suite 40 acc: 0.98\n",
      "test suite 41 acc: 1.0\n",
      "test suite 42 acc: 0.99\n",
      "test suite 43 acc: 0.99\n",
      "test suite 44 acc: 1.0\n",
      "test suite 45 acc: 0.99\n",
      "test suite 46 acc: 0.99\n",
      "test suite 47 acc: 1.0\n",
      "test suite 48 acc: 0.96\n",
      "test suite 49 acc: 0.97\n",
      "test suite 50 acc: 0.98\n",
      "test suite 51 acc: 0.98\n",
      "test suite 52 acc: 0.99\n",
      "test suite 53 acc: 1.0\n",
      "test suite 54 acc: 0.97\n",
      "test suite 55 acc: 0.99\n",
      "test suite 56 acc: 0.98\n",
      "test suite 57 acc: 0.99\n",
      "test suite 58 acc: 1.0\n",
      "test suite 59 acc: 0.99\n",
      "test suite 60 acc: 0.99\n",
      "test suite 61 acc: 0.99\n",
      "test suite 62 acc: 0.98\n",
      "test suite 63 acc: 1.0\n",
      "test suite 64 acc: 0.99\n",
      "test suite 65 acc: 0.98\n",
      "test suite 66 acc: 0.99\n",
      "test suite 67 acc: 0.98\n",
      "test suite 68 acc: 0.98\n",
      "test suite 69 acc: 0.99\n",
      "test suite 70 acc: 0.98\n",
      "test suite 71 acc: 0.99\n",
      "test suite 72 acc: 0.99\n",
      "test suite 73 acc: 0.96\n",
      "test suite 74 acc: 0.98\n",
      "test suite 75 acc: 0.96\n",
      "test suite 76 acc: 0.99\n",
      "test suite 77 acc: 1.0\n",
      "test suite 78 acc: 0.98\n",
      "test suite 79 acc: 1.0\n",
      "test suite 80 acc: 0.97\n",
      "test suite 81 acc: 0.97\n",
      "test suite 82 acc: 1.0\n",
      "test suite 83 acc: 1.0\n",
      "test suite 84 acc: 0.99\n",
      "test suite 85 acc: 0.99\n",
      "test suite 86 acc: 1.0\n",
      "test suite 87 acc: 0.99\n",
      "test suite 88 acc: 0.97\n",
      "test suite 89 acc: 1.0\n",
      "test suite 90 acc: 0.98\n",
      "test suite 91 acc: 0.99\n",
      "test suite 92 acc: 0.99\n",
      "test suite 93 acc: 1.0\n",
      "test suite 94 acc: 0.99\n",
      "test suite 95 acc: 0.99\n",
      "test suite 96 acc: 0.98\n",
      "test suite 97 acc: 1.0\n",
      "test suite 98 acc: 0.99\n",
      "test suite 99 acc: 0.99\n",
      "saving assets/SST2/test_suites_w_acc.pkl\n",
      "starting INV_test_suites...\n",
      "test suite 0 acc: 0.8\n",
      "test suite 1 acc: 0.76\n",
      "test suite 2 acc: 0.78\n",
      "test suite 3 acc: 0.89\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ba5b428ebe03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'starting {}...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_suites\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-b38e8874d58a>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         encode = tokenizer(sentence, \n\u001b[0m\u001b[0;32m     12\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                            \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m             return self.batch_encode_plus(\n\u001b[0m\u001b[0;32m   2318\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2500\u001b[0m         )\n\u001b[0;32m   2501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2502\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2503\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2504\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    373\u001b[0m         )\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[0;32m    376\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "for t_name, test_suites in tss.items():\n",
    "    print('starting {}...'.format(t_name))\n",
    "    for idx, t in test_suites.items():\n",
    "        logits = model([str(x) for x in t['data']])\n",
    "        y_pred = torch.argmax(logits, dim=1)\n",
    "        y_true = torch.tensor(t['target'])\n",
    "        acc = get_acc(y_pred, y_true)\n",
    "        t['performance'] = {\n",
    "            'MODEL_NAME' : MODEL_NAME,\n",
    "            'acc' : acc\n",
    "        }\n",
    "        print('test suite {} acc: {}'.format(idx, acc))\n",
    "    file_path = 'assets/SST2/' + t_name + '_w_acc.pkl'\n",
    "    pkl_save(test_suites, file_path)\n",
    "    print('saving {}'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample some transforms to see if they're reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./assets/csv_examples/sst2_inv_examples.csv\"\n",
    "    \n",
    "# writing to csv file  \n",
    "with open(filename, 'w') as csvfile:  \n",
    "    # creating a csv writer object  \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "    csvwriter.writerow(['text', 'label', 'sound good?'])\n",
    "    # writing the fields  \n",
    "    for i in range(50):\n",
    "        row = [INV_test_suites[i]['data'][i], INV_test_suites[i]['target'][i], True ]#, INV_test_suites[i]['ts'][i] ]\n",
    "        csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./assets/csv_examples/sst2_sib_examples.csv\"\n",
    "    \n",
    "# writing to csv file  \n",
    "with open(filename, 'w') as csvfile:  \n",
    "    # creating a csv writer object  \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "    csvwriter.writerow(['text', 'label', 'sound good?'])\n",
    "    # writing the fields  \n",
    "    for i in range(50):\n",
    "        row = [SIB_test_suites[i]['data'][i], SIB_test_suites[i]['target'][i], True ]#, INV_test_suites[i]['ts'][i] ]\n",
    "        csvwriter.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
