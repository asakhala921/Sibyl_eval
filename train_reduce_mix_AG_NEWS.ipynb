{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix, SibylCollator\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "        \n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "    if len(labels.shape) > 1: \n",
    "        acc = acc_at_k(labels, preds, k=2)\n",
    "        return { 'accuracy': acc }        \n",
    "    else:\n",
    "        acc = accuracy_score(labels, preds.argmax(-1))\n",
    "        return { 'accuracy': acc }        \n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        if len(labels.shape) > 1: \n",
    "            loss = CEwST_loss(logits, labels)\n",
    "        else:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, \n",
    "                 tokenize_fn, \n",
    "                 transform, \n",
    "                 transform_prob=1.0, \n",
    "                 target_pairs=[], \n",
    "                 target_prob=1.0, \n",
    "                 num_classes=2):\n",
    "        \n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.transform_prob = transform_prob\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        if torch.rand(1) < self.transform_prob:\n",
    "            batch = self.transform(\n",
    "                batch, \n",
    "                self.target_pairs,   \n",
    "                self.target_prob,\n",
    "                self.num_classes\n",
    "            )\n",
    "        text, labels = batch\n",
    "        labels = torch.tensor(labels)\n",
    "        if len(labels.shape) == 1:\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=self.num_classes)\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = labels\n",
    "        batch.pop('idx', None)\n",
    "        batch.pop('label', None)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "# ts = ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']\n",
    "ts = ['SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "<ipython-input-5-e5223057dba2>:43: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  test_dataset.rename_column_('label', 'labels')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57984384404b47de8fb1be2c8cf20358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=0 and num_sampled_SIB=2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 3:29:05 < 16:13:49, 5.58 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.408098</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>12.086000</td>\n",
       "      <td>99.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.363069</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>11.517400</td>\n",
       "      <td>104.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.404053</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>9.974200</td>\n",
       "      <td>120.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.342812</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>10.254100</td>\n",
       "      <td>117.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.361725</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>11.025600</td>\n",
       "      <td>108.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.370168</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>11.118700</td>\n",
       "      <td>107.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.475786</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>10.412100</td>\n",
       "      <td>115.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.382800</td>\n",
       "      <td>0.496682</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>11.797800</td>\n",
       "      <td>101.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.412427</td>\n",
       "      <td>0.919167</td>\n",
       "      <td>10.352000</td>\n",
       "      <td>115.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.576396</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>11.630600</td>\n",
       "      <td>103.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.454553</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>11.982600</td>\n",
       "      <td>100.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.411059</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>11.631900</td>\n",
       "      <td>103.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.402332</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>11.047000</td>\n",
       "      <td>108.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.458407</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>10.959400</td>\n",
       "      <td>109.495000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-SibylCollator-SIB\n",
      "{'eval_loss': 0.2793702483177185, 'eval_accuracy': 0.9313157894736842, 'eval_runtime': 113.758, 'eval_samples_per_second': 66.808, 'epoch': 3.54, 'run': './results/bert-base-uncased-SibylCollator-SIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14244a1dbed4e51a3254e0647444350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=1 and num_sampled_SIB=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 3:24:09 < 17:19:42, 5.31 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>0.395702</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>13.739900</td>\n",
       "      <td>87.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>14.625100</td>\n",
       "      <td>82.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.303254</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>11.824300</td>\n",
       "      <td>101.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.354248</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>14.403900</td>\n",
       "      <td>83.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.413023</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>12.273200</td>\n",
       "      <td>97.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.455974</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>13.438200</td>\n",
       "      <td>89.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.372374</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>11.820400</td>\n",
       "      <td>101.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.417169</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>14.542500</td>\n",
       "      <td>82.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.352229</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>12.342300</td>\n",
       "      <td>97.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.303002</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>14.855900</td>\n",
       "      <td>80.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.400919</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>13.512600</td>\n",
       "      <td>88.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.380200</td>\n",
       "      <td>0.443512</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>13.360700</td>\n",
       "      <td>89.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.449683</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>14.174000</td>\n",
       "      <td>84.662000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-SibylCollator-INVSIB\n",
      "{'eval_loss': 0.29319846630096436, 'eval_accuracy': 0.9277631578947368, 'eval_runtime': 112.877, 'eval_samples_per_second': 67.33, 'epoch': 3.28, 'run': './results/bert-base-uncased-SibylCollator-INVSIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf31a133bdb4abb850d64ec993a7a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 2:59:43 < 13:57:04, 6.49 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.349690</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>10.192400</td>\n",
       "      <td>117.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.405158</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>10.314100</td>\n",
       "      <td>116.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.271779</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>9.511900</td>\n",
       "      <td>126.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.321811</td>\n",
       "      <td>0.931667</td>\n",
       "      <td>9.126100</td>\n",
       "      <td>131.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.354268</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>9.851600</td>\n",
       "      <td>121.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.347446</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>9.888000</td>\n",
       "      <td>121.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.399166</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>9.208500</td>\n",
       "      <td>130.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.407695</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>9.782400</td>\n",
       "      <td>122.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.376396</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>9.232700</td>\n",
       "      <td>129.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.388800</td>\n",
       "      <td>0.439017</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>9.954900</td>\n",
       "      <td>120.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>0.381392</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>10.146700</td>\n",
       "      <td>118.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.465899</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>9.750400</td>\n",
       "      <td>123.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>0.400228</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>9.813200</td>\n",
       "      <td>122.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.435938</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>9.574600</td>\n",
       "      <td>125.332000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-SibylCollator-TextMix\n",
      "{'eval_loss': 0.3259882926940918, 'eval_accuracy': 0.9278947368421052, 'eval_runtime': 113.6751, 'eval_samples_per_second': 66.857, 'epoch': 3.54, 'run': './results/bert-base-uncased-SibylCollator-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e72a48d0b18412a9e873e6bedb2272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 2:46:31 < 14:08:02, 6.51 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.392575</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>10.564700</td>\n",
       "      <td>113.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.363633</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>10.383600</td>\n",
       "      <td>115.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.366146</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>9.650300</td>\n",
       "      <td>124.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>0.304321</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>9.607600</td>\n",
       "      <td>124.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.348876</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>9.722500</td>\n",
       "      <td>123.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.407518</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>10.011800</td>\n",
       "      <td>119.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>9.492200</td>\n",
       "      <td>126.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>10.487300</td>\n",
       "      <td>114.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.361293</td>\n",
       "      <td>0.925833</td>\n",
       "      <td>9.799100</td>\n",
       "      <td>122.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.551624</td>\n",
       "      <td>0.894167</td>\n",
       "      <td>10.366400</td>\n",
       "      <td>115.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.364300</td>\n",
       "      <td>0.402636</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>10.697400</td>\n",
       "      <td>112.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.388460</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>10.121300</td>\n",
       "      <td>118.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.402867</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>9.928400</td>\n",
       "      <td>120.865000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-SibylCollator-SentMix\n",
      "{'eval_loss': 0.32576125860214233, 'eval_accuracy': 0.925, 'eval_runtime': 113.0304, 'eval_samples_per_second': 67.239, 'epoch': 3.28, 'run': './results/bert-base-uncased-SibylCollator-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324d955b400843b2ae4fa2dea5f5a3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 2:46:32 < 14:08:05, 6.50 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.386591</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>10.149300</td>\n",
       "      <td>118.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.382369</td>\n",
       "      <td>0.900833</td>\n",
       "      <td>10.327300</td>\n",
       "      <td>116.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.271895</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>9.961100</td>\n",
       "      <td>120.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.398223</td>\n",
       "      <td>0.894167</td>\n",
       "      <td>9.548400</td>\n",
       "      <td>125.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.390300</td>\n",
       "      <td>0.374484</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>9.856900</td>\n",
       "      <td>121.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.424645</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>10.118300</td>\n",
       "      <td>118.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.355440</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>9.305800</td>\n",
       "      <td>128.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>0.508034</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>10.170200</td>\n",
       "      <td>117.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.468702</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>9.350900</td>\n",
       "      <td>128.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.376516</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>10.380000</td>\n",
       "      <td>115.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.470751</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>10.320600</td>\n",
       "      <td>116.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.448255</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>10.100600</td>\n",
       "      <td>118.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.515707</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>9.849700</td>\n",
       "      <td>121.831000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-SibylCollator-WordMix\n",
      "{'eval_loss': 0.2749074697494507, 'eval_accuracy': 0.9261842105263158, 'eval_runtime': 112.7674, 'eval_samples_per_second': 67.395, 'epoch': 3.28, 'run': './results/bert-base-uncased-SibylCollator-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd512888b63a481190db5aaffef4dc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=0 and num_sampled_SIB=2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 3:20:41 < 17:02:02, 5.40 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.427227</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>11.877100</td>\n",
       "      <td>101.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.392916</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>11.257200</td>\n",
       "      <td>106.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.371712</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>10.192600</td>\n",
       "      <td>117.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.383402</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>9.833800</td>\n",
       "      <td>122.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.392763</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>10.413000</td>\n",
       "      <td>115.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.354645</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>10.617500</td>\n",
       "      <td>113.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.383187</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>9.676200</td>\n",
       "      <td>124.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.392008</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>11.533100</td>\n",
       "      <td>104.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>9.566100</td>\n",
       "      <td>125.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.530887</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>11.662900</td>\n",
       "      <td>102.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>0.517585</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>11.575100</td>\n",
       "      <td>103.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.728386</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>11.287900</td>\n",
       "      <td>106.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.069800</td>\n",
       "      <td>1.631443</td>\n",
       "      <td>0.210833</td>\n",
       "      <td>10.776400</td>\n",
       "      <td>111.354000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-SibylCollator-SIB\n",
      "{'eval_loss': 0.3537070155143738, 'eval_accuracy': 0.9221052631578948, 'eval_runtime': 110.1731, 'eval_samples_per_second': 68.982, 'epoch': 3.28, 'run': './results/roberta-base-SibylCollator-SIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e70df2131543ce8a7855c5026d045a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=1 and num_sampled_SIB=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='95000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95000/396000 5:03:37 < 16:02:00, 5.21 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.372203</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>14.606100</td>\n",
       "      <td>82.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.355270</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>14.708200</td>\n",
       "      <td>81.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>11.608400</td>\n",
       "      <td>103.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.339994</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>11.645600</td>\n",
       "      <td>103.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.377006</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>11.960300</td>\n",
       "      <td>100.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.327424</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>14.967700</td>\n",
       "      <td>80.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.347402</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>11.594300</td>\n",
       "      <td>103.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.410775</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>12.791100</td>\n",
       "      <td>93.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.429100</td>\n",
       "      <td>0.370319</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>11.906900</td>\n",
       "      <td>100.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.640442</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>13.209300</td>\n",
       "      <td>90.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.547364</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>16.177600</td>\n",
       "      <td>74.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.543100</td>\n",
       "      <td>0.449668</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>12.333000</td>\n",
       "      <td>97.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.471609</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>13.243400</td>\n",
       "      <td>90.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.651829</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>12.159400</td>\n",
       "      <td>98.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.555930</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>15.454600</td>\n",
       "      <td>77.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.678407</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>11.420700</td>\n",
       "      <td>105.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>1.138165</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>13.522700</td>\n",
       "      <td>88.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>1.441200</td>\n",
       "      <td>1.673930</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>13.798900</td>\n",
       "      <td>86.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>1.619900</td>\n",
       "      <td>1.623904</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>11.090300</td>\n",
       "      <td>108.203000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-SibylCollator-INVSIB\n",
      "{'eval_loss': 0.35579848289489746, 'eval_accuracy': 0.9282894736842106, 'eval_runtime': 107.7642, 'eval_samples_per_second': 70.524, 'epoch': 4.8, 'run': './results/roberta-base-SibylCollator-INVSIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150af32db38e48f59fd6f68591e83fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 2:49:13 < 14:21:44, 6.40 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.479483</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>10.337200</td>\n",
       "      <td>116.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.388258</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>10.113500</td>\n",
       "      <td>118.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.350900</td>\n",
       "      <td>0.309474</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>9.517900</td>\n",
       "      <td>126.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.378926</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>9.489000</td>\n",
       "      <td>126.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.434968</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>9.732900</td>\n",
       "      <td>123.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.313707</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>10.085900</td>\n",
       "      <td>118.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.365700</td>\n",
       "      <td>0.342437</td>\n",
       "      <td>0.915833</td>\n",
       "      <td>9.064800</td>\n",
       "      <td>132.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.390600</td>\n",
       "      <td>0.441295</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>10.139600</td>\n",
       "      <td>118.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.479449</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>9.310900</td>\n",
       "      <td>128.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.448701</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>10.347800</td>\n",
       "      <td>115.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>0.474944</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.126100</td>\n",
       "      <td>118.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.481697</td>\n",
       "      <td>0.888333</td>\n",
       "      <td>9.831100</td>\n",
       "      <td>122.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.419164</td>\n",
       "      <td>0.901667</td>\n",
       "      <td>9.724200</td>\n",
       "      <td>123.403000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-SibylCollator-TextMix\n",
      "{'eval_loss': 0.27471262216567993, 'eval_accuracy': 0.9311842105263158, 'eval_runtime': 109.032, 'eval_samples_per_second': 69.704, 'epoch': 3.28, 'run': './results/roberta-base-SibylCollator-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20556d1e5914ef9b2b8ac4cd2cdd202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='65000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65000/396000 2:51:31 < 14:33:31, 6.32 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>0.420288</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>9.081800</td>\n",
       "      <td>132.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.421195</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>9.165300</td>\n",
       "      <td>130.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.309274</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>8.474300</td>\n",
       "      <td>141.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.389796</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>8.538600</td>\n",
       "      <td>140.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.454680</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>8.739600</td>\n",
       "      <td>137.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.385515</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>9.091800</td>\n",
       "      <td>131.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.382800</td>\n",
       "      <td>0.357522</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>9.058600</td>\n",
       "      <td>132.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.433461</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>9.376000</td>\n",
       "      <td>127.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.489935</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>8.382700</td>\n",
       "      <td>143.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.479182</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>8.948100</td>\n",
       "      <td>134.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.398121</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>9.215600</td>\n",
       "      <td>130.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.456953</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>8.839800</td>\n",
       "      <td>135.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.687961</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>8.832000</td>\n",
       "      <td>135.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-SibylCollator-SentMix\n",
      "{'eval_loss': 0.3329855799674988, 'eval_accuracy': 0.921578947368421, 'eval_runtime': 110.017, 'eval_samples_per_second': 69.08, 'epoch': 3.28, 'run': './results/roberta-base-SibylCollator-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc249ad3a6c344a1a306f003a0109e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 3:07:16 < 14:32:10, 6.23 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.445255</td>\n",
       "      <td>0.888333</td>\n",
       "      <td>9.905600</td>\n",
       "      <td>121.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.439030</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>10.096800</td>\n",
       "      <td>118.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.288345</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>9.660700</td>\n",
       "      <td>124.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>0.312353</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>9.190000</td>\n",
       "      <td>130.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.365795</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>9.553400</td>\n",
       "      <td>125.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.566723</td>\n",
       "      <td>0.889167</td>\n",
       "      <td>9.696600</td>\n",
       "      <td>123.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.889167</td>\n",
       "      <td>8.931200</td>\n",
       "      <td>134.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.403719</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>9.824300</td>\n",
       "      <td>122.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.540749</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>9.013500</td>\n",
       "      <td>133.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.376355</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>9.931600</td>\n",
       "      <td>120.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.755180</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>10.123200</td>\n",
       "      <td>118.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.885833</td>\n",
       "      <td>9.816200</td>\n",
       "      <td>122.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.628894</td>\n",
       "      <td>0.878333</td>\n",
       "      <td>9.538300</td>\n",
       "      <td>125.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.574621</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>9.219200</td>\n",
       "      <td>130.163000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-SibylCollator-WordMix\n",
      "{'eval_loss': 0.2905588448047638, 'eval_accuracy': 0.9277631578947368, 'eval_runtime': 111.5945, 'eval_samples_per_second': 68.104, 'epoch': 3.54, 'run': './results/roberta-base-SibylCollator-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a63009ac0974bf7bf9472ab1fb1e774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=0 and num_sampled_SIB=2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 4:58:45 < 23:11:25, 3.90 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.467838</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>28.662900</td>\n",
       "      <td>41.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.402469</td>\n",
       "      <td>0.915833</td>\n",
       "      <td>26.281300</td>\n",
       "      <td>45.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.421212</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>23.424300</td>\n",
       "      <td>51.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.385358</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>22.281800</td>\n",
       "      <td>53.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.350900</td>\n",
       "      <td>0.394391</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>24.775700</td>\n",
       "      <td>48.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.488422</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>26.326500</td>\n",
       "      <td>45.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.361023</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>21.664900</td>\n",
       "      <td>55.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.437200</td>\n",
       "      <td>0.501662</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>27.601600</td>\n",
       "      <td>43.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>0.621925</td>\n",
       "      <td>0.872500</td>\n",
       "      <td>22.464800</td>\n",
       "      <td>53.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.705200</td>\n",
       "      <td>1.264954</td>\n",
       "      <td>0.459167</td>\n",
       "      <td>26.263300</td>\n",
       "      <td>45.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.176500</td>\n",
       "      <td>1.429851</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>27.396300</td>\n",
       "      <td>43.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.412900</td>\n",
       "      <td>1.980742</td>\n",
       "      <td>0.259167</td>\n",
       "      <td>24.670600</td>\n",
       "      <td>48.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.563900</td>\n",
       "      <td>2.259208</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>24.565700</td>\n",
       "      <td>48.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>2.293864</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>24.279800</td>\n",
       "      <td>49.424000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SibylCollator-SIB\n",
      "{'eval_loss': 0.2926712930202484, 'eval_accuracy': 0.9301315789473684, 'eval_runtime': 241.974, 'eval_samples_per_second': 31.408, 'epoch': 3.54, 'run': './results/xlnet-base-cased-SibylCollator-SIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9168974dabc64cbbbe8cee9f736d03eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with num_sampled_INV=1 and num_sampled_SIB=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='75000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 75000/396000 5:25:07 < 23:11:35, 3.84 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.472401</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>28.635500</td>\n",
       "      <td>41.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.426812</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>26.587900</td>\n",
       "      <td>45.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.396017</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>22.489500</td>\n",
       "      <td>53.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.373974</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>21.930300</td>\n",
       "      <td>54.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.376855</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>25.935100</td>\n",
       "      <td>46.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>25.460800</td>\n",
       "      <td>47.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.399317</td>\n",
       "      <td>0.919167</td>\n",
       "      <td>22.364300</td>\n",
       "      <td>53.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.552003</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>28.179400</td>\n",
       "      <td>42.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.560435</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>24.169800</td>\n",
       "      <td>49.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>26.153200</td>\n",
       "      <td>45.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.872104</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>28.145600</td>\n",
       "      <td>42.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>1.064784</td>\n",
       "      <td>0.690833</td>\n",
       "      <td>25.809500</td>\n",
       "      <td>46.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.125100</td>\n",
       "      <td>1.435244</td>\n",
       "      <td>0.325833</td>\n",
       "      <td>27.128700</td>\n",
       "      <td>44.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.437100</td>\n",
       "      <td>1.625271</td>\n",
       "      <td>0.224167</td>\n",
       "      <td>24.622800</td>\n",
       "      <td>48.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.622900</td>\n",
       "      <td>1.653769</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>28.354300</td>\n",
       "      <td>42.322000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SibylCollator-INVSIB\n",
      "{'eval_loss': 0.35648804903030396, 'eval_accuracy': 0.9244736842105263, 'eval_runtime': 242.7723, 'eval_samples_per_second': 31.305, 'epoch': 3.79, 'run': './results/xlnet-base-cased-SibylCollator-INVSIB', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcd0207858346adae9d1cb1945ea58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 4:01:08 < 18:43:05, 4.84 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.469769</td>\n",
       "      <td>0.901667</td>\n",
       "      <td>20.104300</td>\n",
       "      <td>59.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.364384</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>19.656900</td>\n",
       "      <td>61.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>0.378757</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>17.202900</td>\n",
       "      <td>69.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.418190</td>\n",
       "      <td>0.925833</td>\n",
       "      <td>17.277000</td>\n",
       "      <td>69.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.456225</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>18.321600</td>\n",
       "      <td>65.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.382556</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>18.929400</td>\n",
       "      <td>63.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>16.891000</td>\n",
       "      <td>71.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.489775</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>19.390500</td>\n",
       "      <td>61.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.448786</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>17.967800</td>\n",
       "      <td>66.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.654796</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>18.693500</td>\n",
       "      <td>64.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>1.026817</td>\n",
       "      <td>0.590833</td>\n",
       "      <td>19.702700</td>\n",
       "      <td>60.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.355200</td>\n",
       "      <td>1.568913</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>19.315100</td>\n",
       "      <td>62.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.598200</td>\n",
       "      <td>1.581793</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>18.239200</td>\n",
       "      <td>65.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.515500</td>\n",
       "      <td>2.549121</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>18.060700</td>\n",
       "      <td>66.442000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SibylCollator-TextMix\n",
      "{'eval_loss': 0.3926885724067688, 'eval_accuracy': 0.9217105263157894, 'eval_runtime': 242.6244, 'eval_samples_per_second': 31.324, 'epoch': 3.54, 'run': './results/xlnet-base-cased-SibylCollator-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48dab4bf73f479dbb42f98f8ad9a2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 4:03:53 < 18:55:53, 4.78 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.437895</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>20.755300</td>\n",
       "      <td>57.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.433069</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>21.143300</td>\n",
       "      <td>56.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.434952</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>19.209200</td>\n",
       "      <td>62.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.327831</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>18.382100</td>\n",
       "      <td>65.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.330013</td>\n",
       "      <td>0.919167</td>\n",
       "      <td>18.719000</td>\n",
       "      <td>64.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.424502</td>\n",
       "      <td>0.915833</td>\n",
       "      <td>19.590200</td>\n",
       "      <td>61.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.455167</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>18.333700</td>\n",
       "      <td>65.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.598535</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>21.479000</td>\n",
       "      <td>55.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.572554</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>18.991900</td>\n",
       "      <td>63.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.531843</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>20.950600</td>\n",
       "      <td>57.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.747800</td>\n",
       "      <td>1.074876</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>22.366900</td>\n",
       "      <td>53.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.251100</td>\n",
       "      <td>1.558186</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>20.875300</td>\n",
       "      <td>57.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.596200</td>\n",
       "      <td>1.567774</td>\n",
       "      <td>0.349167</td>\n",
       "      <td>20.073900</td>\n",
       "      <td>59.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.590400</td>\n",
       "      <td>1.566305</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>20.114200</td>\n",
       "      <td>59.659000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SibylCollator-SentMix\n",
      "{'eval_loss': 0.32802721858024597, 'eval_accuracy': 0.9264473684210527, 'eval_runtime': 246.4713, 'eval_samples_per_second': 30.835, 'epoch': 3.54, 'run': './results/xlnet-base-cased-SibylCollator-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a910e6fc74942a5b374536117beec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SibylCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='396000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/396000 4:07:43 < 19:13:45, 4.71 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.601203</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>19.431300</td>\n",
       "      <td>61.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.449367</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>19.561800</td>\n",
       "      <td>61.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.357637</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>17.811200</td>\n",
       "      <td>67.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.354163</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>17.347000</td>\n",
       "      <td>69.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>17.396400</td>\n",
       "      <td>68.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.502811</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>18.857200</td>\n",
       "      <td>63.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.444306</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>16.856000</td>\n",
       "      <td>71.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>1.129131</td>\n",
       "      <td>0.724167</td>\n",
       "      <td>19.036200</td>\n",
       "      <td>63.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>0.739449</td>\n",
       "      <td>0.848333</td>\n",
       "      <td>17.801600</td>\n",
       "      <td>67.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.729014</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>18.960200</td>\n",
       "      <td>63.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.746900</td>\n",
       "      <td>0.747913</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>19.845200</td>\n",
       "      <td>60.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.843411</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>18.605400</td>\n",
       "      <td>64.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.984900</td>\n",
       "      <td>1.150933</td>\n",
       "      <td>0.524167</td>\n",
       "      <td>18.961100</td>\n",
       "      <td>63.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.421200</td>\n",
       "      <td>2.296668</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>17.175300</td>\n",
       "      <td>69.868000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-SibylCollator-WordMix\n",
      "{'eval_loss': 0.33383435010910034, 'eval_accuracy': 0.9285526315789474, 'eval_runtime': 247.9231, 'eval_samples_per_second': 30.655, 'epoch': 3.54, 'run': './results/xlnet-base-cased-SibylCollator-WordMix', 'test': 'ORIG'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "        \n",
    "    for t in ts: \n",
    "        \n",
    "        transform = None\n",
    "        num_sampled_INV = 0\n",
    "        num_sampled_SIB = 0\n",
    "        label_type = \"soft\"\n",
    "        \n",
    "        if t == \"INV\":\n",
    "            num_sampled_INV = 2\n",
    "            label_type = \"hard\"\n",
    "        elif t == \"SIB\":\n",
    "            num_sampled_SIB = 2\n",
    "        elif t == 'INVSIB':\n",
    "            num_sampled_INV = 1\n",
    "            num_sampled_SIB = 1\n",
    "            label_type = None\n",
    "        elif t == \"TextMix\":\n",
    "            transform = TextMix()\n",
    "        elif t == \"SentMix\":\n",
    "            transform = SentMix()\n",
    "        elif t == \"WordMix\":\n",
    "            transform = WordMix()\n",
    "        \n",
    "        checkpoint = './results/' + MODEL_NAME + '-SibylCollator-' + t\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5).to(device)\n",
    "\n",
    "        dataset = load_dataset('ag_news', split='train') \n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('ag_news', split='test') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 6\n",
    "        eval_batch_size  = 32\n",
    "        num_epoch = 20\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "#         tmcb = TargetedMixturesCallback(\n",
    "#             dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "#             device=device\n",
    "#         )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "#         tmc = TargetedMixturesCollator(\n",
    "#             tokenize_fn=tokenize_fn, \n",
    "#             transform=t, \n",
    "#             transform_prob=0.5,\n",
    "#             target_pairs=[],\n",
    "#             target_prob=0.5,\n",
    "#             num_classes=4\n",
    "#         )\n",
    "        sibyl_collator = SibylCollator( \n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=transform, \n",
    "            num_sampled_INV=num_sampled_INV, \n",
    "            num_sampled_SIB=num_sampled_SIB, \n",
    "            task_type=\"topic\", \n",
    "            tran_type=None, \n",
    "            label_type=label_type,\n",
    "            one_hot=label_type == \"soft\",\n",
    "            transform_prob=0.5,\n",
    "            target_pairs=[],\n",
    "            target_prob=0.0,\n",
    "            reduce_mixed=True,\n",
    "            num_classes=4\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\\\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=5000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=sibyl_collator if t != \"ORIG\" else DefaultCollator(),\n",
    "            callbacks=[escb] # [tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        # trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_AG_NEWS_SibylCollator.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>run</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.279370</td>\n",
       "      <td>0.931316</td>\n",
       "      <td>113.7580</td>\n",
       "      <td>66.808</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/bert-base-uncased-SibylCollator-SIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.293198</td>\n",
       "      <td>0.927763</td>\n",
       "      <td>112.8770</td>\n",
       "      <td>67.330</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/bert-base-uncased-SibylCollator-INVSIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.325988</td>\n",
       "      <td>0.927895</td>\n",
       "      <td>113.6751</td>\n",
       "      <td>66.857</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/bert-base-uncased-SibylCollator-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.325761</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>113.0304</td>\n",
       "      <td>67.239</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/bert-base-uncased-SibylCollator-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274907</td>\n",
       "      <td>0.926184</td>\n",
       "      <td>112.7674</td>\n",
       "      <td>67.395</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/bert-base-uncased-SibylCollator-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.353707</td>\n",
       "      <td>0.922105</td>\n",
       "      <td>110.1731</td>\n",
       "      <td>68.982</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/roberta-base-SibylCollator-SIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.355798</td>\n",
       "      <td>0.928289</td>\n",
       "      <td>107.7642</td>\n",
       "      <td>70.524</td>\n",
       "      <td>4.80</td>\n",
       "      <td>./results/roberta-base-SibylCollator-INVSIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.274713</td>\n",
       "      <td>0.931184</td>\n",
       "      <td>109.0320</td>\n",
       "      <td>69.704</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/roberta-base-SibylCollator-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.332986</td>\n",
       "      <td>0.921579</td>\n",
       "      <td>110.0170</td>\n",
       "      <td>69.080</td>\n",
       "      <td>3.28</td>\n",
       "      <td>./results/roberta-base-SibylCollator-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.290559</td>\n",
       "      <td>0.927763</td>\n",
       "      <td>111.5945</td>\n",
       "      <td>68.104</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/roberta-base-SibylCollator-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.292671</td>\n",
       "      <td>0.930132</td>\n",
       "      <td>241.9740</td>\n",
       "      <td>31.408</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/xlnet-base-cased-SibylCollator-SIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.356488</td>\n",
       "      <td>0.924474</td>\n",
       "      <td>242.7723</td>\n",
       "      <td>31.305</td>\n",
       "      <td>3.79</td>\n",
       "      <td>./results/xlnet-base-cased-SibylCollator-INVSIB</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.392689</td>\n",
       "      <td>0.921711</td>\n",
       "      <td>242.6244</td>\n",
       "      <td>31.324</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/xlnet-base-cased-SibylCollator-TextMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.328027</td>\n",
       "      <td>0.926447</td>\n",
       "      <td>246.4713</td>\n",
       "      <td>30.835</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/xlnet-base-cased-SibylCollator-SentMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.333834</td>\n",
       "      <td>0.928553</td>\n",
       "      <td>247.9231</td>\n",
       "      <td>30.655</td>\n",
       "      <td>3.54</td>\n",
       "      <td>./results/xlnet-base-cased-SibylCollator-WordMix</td>\n",
       "      <td>ORIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  epoch  \\\n",
       "0    0.279370       0.931316      113.7580                   66.808   3.54   \n",
       "1    0.293198       0.927763      112.8770                   67.330   3.28   \n",
       "2    0.325988       0.927895      113.6751                   66.857   3.54   \n",
       "3    0.325761       0.925000      113.0304                   67.239   3.28   \n",
       "4    0.274907       0.926184      112.7674                   67.395   3.28   \n",
       "5    0.353707       0.922105      110.1731                   68.982   3.28   \n",
       "6    0.355798       0.928289      107.7642                   70.524   4.80   \n",
       "7    0.274713       0.931184      109.0320                   69.704   3.28   \n",
       "8    0.332986       0.921579      110.0170                   69.080   3.28   \n",
       "9    0.290559       0.927763      111.5945                   68.104   3.54   \n",
       "10   0.292671       0.930132      241.9740                   31.408   3.54   \n",
       "11   0.356488       0.924474      242.7723                   31.305   3.79   \n",
       "12   0.392689       0.921711      242.6244                   31.324   3.54   \n",
       "13   0.328027       0.926447      246.4713                   30.835   3.54   \n",
       "14   0.333834       0.928553      247.9231                   30.655   3.54   \n",
       "\n",
       "                                                  run  test  \n",
       "0       ./results/bert-base-uncased-SibylCollator-SIB  ORIG  \n",
       "1    ./results/bert-base-uncased-SibylCollator-INVSIB  ORIG  \n",
       "2   ./results/bert-base-uncased-SibylCollator-TextMix  ORIG  \n",
       "3   ./results/bert-base-uncased-SibylCollator-SentMix  ORIG  \n",
       "4   ./results/bert-base-uncased-SibylCollator-WordMix  ORIG  \n",
       "5            ./results/roberta-base-SibylCollator-SIB  ORIG  \n",
       "6         ./results/roberta-base-SibylCollator-INVSIB  ORIG  \n",
       "7        ./results/roberta-base-SibylCollator-TextMix  ORIG  \n",
       "8        ./results/roberta-base-SibylCollator-SentMix  ORIG  \n",
       "9        ./results/roberta-base-SibylCollator-WordMix  ORIG  \n",
       "10       ./results/xlnet-base-cased-SibylCollator-SIB  ORIG  \n",
       "11    ./results/xlnet-base-cased-SibylCollator-INVSIB  ORIG  \n",
       "12   ./results/xlnet-base-cased-SibylCollator-TextMix  ORIG  \n",
       "13   ./results/xlnet-base-cased-SibylCollator-SentMix  ORIG  \n",
       "14   ./results/xlnet-base-cased-SibylCollator-WordMix  ORIG  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
