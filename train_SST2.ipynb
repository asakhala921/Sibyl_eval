{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "utjMLdmqsUuA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WT_xnGBpTNuZ"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(y, nb_classes=2):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.expand_dims(np.array(y), 0)\n",
    "    res = np.eye(nb_classes)[np.array(y).reshape(-1)]\n",
    "    return res.reshape(list(y.shape)+[nb_classes])[0]\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    if acc.item() > 1:\n",
    "        print(y_true.shape, y_true)\n",
    "        print(y_pred.shape, y_pred)\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class Trainer_w_soft_target(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "# ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nq1i8QiBtY0e"
   },
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "59b96dd300ae4a56b2d58ca0de0bb7f6",
      "e3c71b92f2a0484bb70123a86f855f9d",
      "dd27134d896448b6b385ce45da7556ee",
      "c92040617a854d8d96cc8ba678f0b271",
      "367cfe385d38427dbdf4325a70c0dc2e",
      "f86a86b6b9c64cc989e89b27693d8a2f",
      "14dc24ef59d341a88d1bdb69f3cacde6",
      "46cc6e7671244f1d9167855a45a36af8",
      "81821a97e02445539ccabcc2091cba95",
      "212a6447739d45b5b57e5815ff538f57",
      "4b1f7506e1a44d109d918b1f16d6bb75",
      "f177b91513ec4039bc7c6e61d15db9a2",
      "00554915f7ad47b9b80d65294dbfdd37",
      "d5f641be531341dbb007330b7e245169",
      "7158ce3f315647aba9299519fcec4be7",
      "0768ccf7a2e64f819c7401d7db258d16"
     ]
    },
    "id": "T-krnPy6TDSB",
    "outputId": "7161b883-f15a-449f-86fa-e8bbbfc6e9c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "rename_column_ is deprecated and will be removed in the next major version of datasets. Use the dataset.rename_column method instead.\n",
      "Loading cached split indices for dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-32a6a23548aa6274.arrow and C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-0d3572f739d46d53.arrow\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-6375dba9ab516494.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-d4c4531e1e963225.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='71978' max='71978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71978/71978 3:25:54, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.294396</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>0.913627</td>\n",
       "      <td>0.912439</td>\n",
       "      <td>0.916180</td>\n",
       "      <td>12.323600</td>\n",
       "      <td>245.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.344711</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>0.903933</td>\n",
       "      <td>0.915343</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>12.336600</td>\n",
       "      <td>245.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.313438</td>\n",
       "      <td>0.917849</td>\n",
       "      <td>0.917460</td>\n",
       "      <td>0.916657</td>\n",
       "      <td>0.921431</td>\n",
       "      <td>12.356900</td>\n",
       "      <td>245.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.313195</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.927734</td>\n",
       "      <td>0.928604</td>\n",
       "      <td>0.926985</td>\n",
       "      <td>12.329900</td>\n",
       "      <td>245.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.322501</td>\n",
       "      <td>0.931706</td>\n",
       "      <td>0.931101</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>0.932669</td>\n",
       "      <td>12.339700</td>\n",
       "      <td>245.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.280137</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.930789</td>\n",
       "      <td>0.929666</td>\n",
       "      <td>0.932519</td>\n",
       "      <td>12.331300</td>\n",
       "      <td>245.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.287617</td>\n",
       "      <td>0.929396</td>\n",
       "      <td>0.928151</td>\n",
       "      <td>0.931116</td>\n",
       "      <td>0.926112</td>\n",
       "      <td>12.324200</td>\n",
       "      <td>245.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.299354</td>\n",
       "      <td>0.935995</td>\n",
       "      <td>0.935232</td>\n",
       "      <td>0.935107</td>\n",
       "      <td>0.935359</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>246.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.254932</td>\n",
       "      <td>0.937974</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.937330</td>\n",
       "      <td>0.937070</td>\n",
       "      <td>12.344400</td>\n",
       "      <td>245.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.278389</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>0.939119</td>\n",
       "      <td>0.937941</td>\n",
       "      <td>0.940978</td>\n",
       "      <td>12.325100</td>\n",
       "      <td>245.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.324927</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>0.938959</td>\n",
       "      <td>0.941303</td>\n",
       "      <td>0.937239</td>\n",
       "      <td>12.351200</td>\n",
       "      <td>245.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.313950</td>\n",
       "      <td>0.940944</td>\n",
       "      <td>0.940117</td>\n",
       "      <td>0.940969</td>\n",
       "      <td>0.939378</td>\n",
       "      <td>12.329200</td>\n",
       "      <td>245.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.315475</td>\n",
       "      <td>0.938634</td>\n",
       "      <td>0.938102</td>\n",
       "      <td>0.936978</td>\n",
       "      <td>0.939792</td>\n",
       "      <td>12.313300</td>\n",
       "      <td>246.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.295947</td>\n",
       "      <td>0.943253</td>\n",
       "      <td>0.942576</td>\n",
       "      <td>0.942450</td>\n",
       "      <td>0.942706</td>\n",
       "      <td>12.317600</td>\n",
       "      <td>246.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.284834</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.943515</td>\n",
       "      <td>0.943898</td>\n",
       "      <td>0.943158</td>\n",
       "      <td>12.325800</td>\n",
       "      <td>245.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.295772</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.943507</td>\n",
       "      <td>0.943965</td>\n",
       "      <td>0.943085</td>\n",
       "      <td>12.309600</td>\n",
       "      <td>246.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.299351</td>\n",
       "      <td>0.941274</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>0.942174</td>\n",
       "      <td>0.938942</td>\n",
       "      <td>12.338800</td>\n",
       "      <td>245.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.258154</td>\n",
       "      <td>0.946552</td>\n",
       "      <td>0.946020</td>\n",
       "      <td>0.945184</td>\n",
       "      <td>0.947073</td>\n",
       "      <td>12.326500</td>\n",
       "      <td>245.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.289969</td>\n",
       "      <td>0.941274</td>\n",
       "      <td>0.940331</td>\n",
       "      <td>0.942360</td>\n",
       "      <td>0.938795</td>\n",
       "      <td>12.325500</td>\n",
       "      <td>245.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.270863</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>0.942439</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>0.944244</td>\n",
       "      <td>12.321400</td>\n",
       "      <td>245.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.334992</td>\n",
       "      <td>0.938964</td>\n",
       "      <td>0.937968</td>\n",
       "      <td>0.940146</td>\n",
       "      <td>0.936347</td>\n",
       "      <td>12.332400</td>\n",
       "      <td>245.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.314550</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>0.942152</td>\n",
       "      <td>0.942763</td>\n",
       "      <td>0.941602</td>\n",
       "      <td>12.321300</td>\n",
       "      <td>245.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.329249</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>0.945569</td>\n",
       "      <td>0.945537</td>\n",
       "      <td>0.945602</td>\n",
       "      <td>12.332500</td>\n",
       "      <td>245.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.310804</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>0.942161</td>\n",
       "      <td>0.942694</td>\n",
       "      <td>0.941675</td>\n",
       "      <td>12.320700</td>\n",
       "      <td>246.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.313399</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.943735</td>\n",
       "      <td>0.942684</td>\n",
       "      <td>0.945213</td>\n",
       "      <td>12.332200</td>\n",
       "      <td>245.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>0.946897</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>12.329200</td>\n",
       "      <td>245.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.294494</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>0.946707</td>\n",
       "      <td>0.945759</td>\n",
       "      <td>0.947961</td>\n",
       "      <td>12.334100</td>\n",
       "      <td>245.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.287819</td>\n",
       "      <td>0.949192</td>\n",
       "      <td>0.948578</td>\n",
       "      <td>0.948513</td>\n",
       "      <td>0.948644</td>\n",
       "      <td>12.337000</td>\n",
       "      <td>245.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.318475</td>\n",
       "      <td>0.950181</td>\n",
       "      <td>0.949655</td>\n",
       "      <td>0.949006</td>\n",
       "      <td>0.950417</td>\n",
       "      <td>12.339700</td>\n",
       "      <td>245.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.325823</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>0.946980</td>\n",
       "      <td>0.946383</td>\n",
       "      <td>0.947672</td>\n",
       "      <td>12.338200</td>\n",
       "      <td>245.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.330946</td>\n",
       "      <td>0.948532</td>\n",
       "      <td>0.948005</td>\n",
       "      <td>0.947247</td>\n",
       "      <td>0.948930</td>\n",
       "      <td>12.356500</td>\n",
       "      <td>245.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.302342</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.950759</td>\n",
       "      <td>0.950355</td>\n",
       "      <td>12.353200</td>\n",
       "      <td>245.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.326715</td>\n",
       "      <td>0.950841</td>\n",
       "      <td>0.950266</td>\n",
       "      <td>0.950045</td>\n",
       "      <td>0.950498</td>\n",
       "      <td>12.332700</td>\n",
       "      <td>245.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.323814</td>\n",
       "      <td>0.950841</td>\n",
       "      <td>0.950237</td>\n",
       "      <td>0.950270</td>\n",
       "      <td>0.950204</td>\n",
       "      <td>12.345300</td>\n",
       "      <td>245.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.318130</td>\n",
       "      <td>0.952491</td>\n",
       "      <td>0.951918</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>0.951984</td>\n",
       "      <td>12.342900</td>\n",
       "      <td>245.567000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+ORIG\n",
      "{'eval_loss': 0.35198429226875305, 'eval_accuracy': 0.9453600593912398, 'eval_f1': 0.9446036526435704, 'eval_precision': 0.9441538801196128, 'eval_recall': 0.9450924194929575, 'eval_runtime': 27.4117, 'eval_samples_per_second': 245.698, 'epoch': 10.0, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "remove_columns_ is deprecated and will be removed in the next major version of datasets. Use the dataset.remove_columns method instead.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c271b7e7ef60404081a3f10868fcef89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064b6079949444d788082f97912066f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 3:57:07 < 16:22:06, 1.97 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.406990</td>\n",
       "      <td>0.827945</td>\n",
       "      <td>0.820940</td>\n",
       "      <td>0.841553</td>\n",
       "      <td>0.816177</td>\n",
       "      <td>102.903000</td>\n",
       "      <td>58.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.321684</td>\n",
       "      <td>0.857638</td>\n",
       "      <td>0.854407</td>\n",
       "      <td>0.860959</td>\n",
       "      <td>0.851381</td>\n",
       "      <td>102.851300</td>\n",
       "      <td>58.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.407728</td>\n",
       "      <td>0.865556</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.865522</td>\n",
       "      <td>0.862047</td>\n",
       "      <td>102.943100</td>\n",
       "      <td>58.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.327549</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>0.868234</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.869261</td>\n",
       "      <td>102.863500</td>\n",
       "      <td>58.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.322734</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>0.865325</td>\n",
       "      <td>0.872318</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>102.801700</td>\n",
       "      <td>58.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.331175</td>\n",
       "      <td>0.858628</td>\n",
       "      <td>0.856278</td>\n",
       "      <td>0.858877</td>\n",
       "      <td>0.854622</td>\n",
       "      <td>102.872100</td>\n",
       "      <td>58.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.463466</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.852733</td>\n",
       "      <td>0.824510</td>\n",
       "      <td>102.831600</td>\n",
       "      <td>58.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.484120</td>\n",
       "      <td>0.847080</td>\n",
       "      <td>0.845307</td>\n",
       "      <td>0.845472</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>102.848800</td>\n",
       "      <td>58.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.399292</td>\n",
       "      <td>0.855823</td>\n",
       "      <td>0.854787</td>\n",
       "      <td>0.853915</td>\n",
       "      <td>0.856388</td>\n",
       "      <td>102.903100</td>\n",
       "      <td>58.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.391869</td>\n",
       "      <td>0.867865</td>\n",
       "      <td>0.865806</td>\n",
       "      <td>0.867879</td>\n",
       "      <td>0.864382</td>\n",
       "      <td>102.979500</td>\n",
       "      <td>58.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.394821</td>\n",
       "      <td>0.863906</td>\n",
       "      <td>0.861865</td>\n",
       "      <td>0.863603</td>\n",
       "      <td>0.860624</td>\n",
       "      <td>102.772500</td>\n",
       "      <td>58.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.401942</td>\n",
       "      <td>0.854668</td>\n",
       "      <td>0.851325</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.848273</td>\n",
       "      <td>102.748000</td>\n",
       "      <td>58.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.460946</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.842051</td>\n",
       "      <td>0.842620</td>\n",
       "      <td>0.846430</td>\n",
       "      <td>102.739700</td>\n",
       "      <td>59.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.427034</td>\n",
       "      <td>0.846255</td>\n",
       "      <td>0.841235</td>\n",
       "      <td>0.855450</td>\n",
       "      <td>0.836809</td>\n",
       "      <td>102.777300</td>\n",
       "      <td>58.982000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+INV\n",
      "{'eval_loss': 0.22934286296367645, 'eval_accuracy': 0.9272457312546399, 'eval_f1': 0.9265553141535987, 'eval_precision': 0.9250828171225598, 'eval_recall': 0.929157122983028, 'eval_runtime': 27.3978, 'eval_samples_per_second': 245.823, 'epoch': 1.94, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+INV'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89d2432d9a246a183d10c117b96ce55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55c0f0cf73a43059f77829bc187cf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='140000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140000/143957 16:00:21 < 27:08, 2.43 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.757427</td>\n",
       "      <td>65.886600</td>\n",
       "      <td>92.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.465720</td>\n",
       "      <td>0.810361</td>\n",
       "      <td>65.838900</td>\n",
       "      <td>92.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.458156</td>\n",
       "      <td>0.811933</td>\n",
       "      <td>65.895200</td>\n",
       "      <td>91.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.463236</td>\n",
       "      <td>0.831102</td>\n",
       "      <td>65.912900</td>\n",
       "      <td>91.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.524379</td>\n",
       "      <td>0.838256</td>\n",
       "      <td>65.919200</td>\n",
       "      <td>91.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.457948</td>\n",
       "      <td>0.832105</td>\n",
       "      <td>65.895200</td>\n",
       "      <td>91.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.469572</td>\n",
       "      <td>0.834950</td>\n",
       "      <td>65.889100</td>\n",
       "      <td>92.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.487929</td>\n",
       "      <td>0.817310</td>\n",
       "      <td>65.862800</td>\n",
       "      <td>92.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.437296</td>\n",
       "      <td>0.822175</td>\n",
       "      <td>65.870900</td>\n",
       "      <td>92.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>0.450881</td>\n",
       "      <td>0.852468</td>\n",
       "      <td>65.888300</td>\n",
       "      <td>92.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.452222</td>\n",
       "      <td>0.852745</td>\n",
       "      <td>65.891100</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.457462</td>\n",
       "      <td>0.848311</td>\n",
       "      <td>65.864900</td>\n",
       "      <td>92.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.473472</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>65.870800</td>\n",
       "      <td>92.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.497613</td>\n",
       "      <td>0.834541</td>\n",
       "      <td>65.883600</td>\n",
       "      <td>92.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.482476</td>\n",
       "      <td>0.847077</td>\n",
       "      <td>65.862500</td>\n",
       "      <td>92.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.463877</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>65.841700</td>\n",
       "      <td>92.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.859980</td>\n",
       "      <td>65.863400</td>\n",
       "      <td>92.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.849980</td>\n",
       "      <td>65.890300</td>\n",
       "      <td>92.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>0.848031</td>\n",
       "      <td>65.900800</td>\n",
       "      <td>91.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.446638</td>\n",
       "      <td>0.847604</td>\n",
       "      <td>65.893300</td>\n",
       "      <td>91.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.484398</td>\n",
       "      <td>0.852203</td>\n",
       "      <td>65.880900</td>\n",
       "      <td>92.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.485812</td>\n",
       "      <td>0.855335</td>\n",
       "      <td>65.866800</td>\n",
       "      <td>92.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.479075</td>\n",
       "      <td>0.856346</td>\n",
       "      <td>65.845200</td>\n",
       "      <td>92.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.396400</td>\n",
       "      <td>0.458188</td>\n",
       "      <td>0.859016</td>\n",
       "      <td>65.859400</td>\n",
       "      <td>92.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.481552</td>\n",
       "      <td>0.856796</td>\n",
       "      <td>65.879800</td>\n",
       "      <td>92.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.466943</td>\n",
       "      <td>0.860783</td>\n",
       "      <td>65.863000</td>\n",
       "      <td>92.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.488041</td>\n",
       "      <td>0.862608</td>\n",
       "      <td>65.870300</td>\n",
       "      <td>92.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.480285</td>\n",
       "      <td>0.856488</td>\n",
       "      <td>65.888200</td>\n",
       "      <td>92.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.475124</td>\n",
       "      <td>0.863445</td>\n",
       "      <td>65.842700</td>\n",
       "      <td>92.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.464710</td>\n",
       "      <td>0.861171</td>\n",
       "      <td>65.836400</td>\n",
       "      <td>92.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.462993</td>\n",
       "      <td>0.866113</td>\n",
       "      <td>65.848700</td>\n",
       "      <td>92.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.472919</td>\n",
       "      <td>0.862462</td>\n",
       "      <td>65.844500</td>\n",
       "      <td>92.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.455124</td>\n",
       "      <td>0.865126</td>\n",
       "      <td>65.841700</td>\n",
       "      <td>92.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.447674</td>\n",
       "      <td>0.868819</td>\n",
       "      <td>65.834700</td>\n",
       "      <td>92.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.452757</td>\n",
       "      <td>0.867462</td>\n",
       "      <td>65.862100</td>\n",
       "      <td>92.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.448192</td>\n",
       "      <td>0.861497</td>\n",
       "      <td>65.824600</td>\n",
       "      <td>92.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.468447</td>\n",
       "      <td>0.864350</td>\n",
       "      <td>65.826200</td>\n",
       "      <td>92.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.453527</td>\n",
       "      <td>0.871815</td>\n",
       "      <td>65.835200</td>\n",
       "      <td>92.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.367700</td>\n",
       "      <td>0.436565</td>\n",
       "      <td>0.872784</td>\n",
       "      <td>65.809200</td>\n",
       "      <td>92.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.442980</td>\n",
       "      <td>0.870816</td>\n",
       "      <td>65.851100</td>\n",
       "      <td>92.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.871515</td>\n",
       "      <td>65.867600</td>\n",
       "      <td>92.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.451518</td>\n",
       "      <td>0.874201</td>\n",
       "      <td>65.840300</td>\n",
       "      <td>92.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.359600</td>\n",
       "      <td>0.460559</td>\n",
       "      <td>0.873344</td>\n",
       "      <td>65.843500</td>\n",
       "      <td>92.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.439052</td>\n",
       "      <td>0.875269</td>\n",
       "      <td>65.851500</td>\n",
       "      <td>92.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.469187</td>\n",
       "      <td>0.872730</td>\n",
       "      <td>65.840600</td>\n",
       "      <td>92.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.451762</td>\n",
       "      <td>0.867816</td>\n",
       "      <td>65.876400</td>\n",
       "      <td>92.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.462571</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>65.831800</td>\n",
       "      <td>92.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.875569</td>\n",
       "      <td>65.839500</td>\n",
       "      <td>92.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.466540</td>\n",
       "      <td>0.873815</td>\n",
       "      <td>65.860400</td>\n",
       "      <td>92.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.465065</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>65.844700</td>\n",
       "      <td>92.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.439032</td>\n",
       "      <td>0.875411</td>\n",
       "      <td>65.827100</td>\n",
       "      <td>92.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.458853</td>\n",
       "      <td>0.873919</td>\n",
       "      <td>65.848800</td>\n",
       "      <td>92.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.477599</td>\n",
       "      <td>0.877162</td>\n",
       "      <td>65.829000</td>\n",
       "      <td>92.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.469658</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>65.858700</td>\n",
       "      <td>92.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.480182</td>\n",
       "      <td>0.877905</td>\n",
       "      <td>65.860100</td>\n",
       "      <td>92.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.455310</td>\n",
       "      <td>0.878544</td>\n",
       "      <td>65.883100</td>\n",
       "      <td>92.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.457942</td>\n",
       "      <td>0.877476</td>\n",
       "      <td>65.823700</td>\n",
       "      <td>92.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.469931</td>\n",
       "      <td>0.877858</td>\n",
       "      <td>65.808200</td>\n",
       "      <td>92.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.461848</td>\n",
       "      <td>0.879407</td>\n",
       "      <td>65.833200</td>\n",
       "      <td>92.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.458868</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>65.862800</td>\n",
       "      <td>92.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.463811</td>\n",
       "      <td>0.880715</td>\n",
       "      <td>65.859000</td>\n",
       "      <td>92.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.474374</td>\n",
       "      <td>0.879247</td>\n",
       "      <td>65.878600</td>\n",
       "      <td>92.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.468091</td>\n",
       "      <td>0.878813</td>\n",
       "      <td>65.864600</td>\n",
       "      <td>92.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.459646</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>65.811600</td>\n",
       "      <td>92.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.460001</td>\n",
       "      <td>0.880732</td>\n",
       "      <td>65.822000</td>\n",
       "      <td>92.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.470576</td>\n",
       "      <td>0.877083</td>\n",
       "      <td>65.818000</td>\n",
       "      <td>92.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.464005</td>\n",
       "      <td>0.879346</td>\n",
       "      <td>65.850800</td>\n",
       "      <td>92.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.880379</td>\n",
       "      <td>65.846100</td>\n",
       "      <td>92.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.471649</td>\n",
       "      <td>0.878936</td>\n",
       "      <td>65.871700</td>\n",
       "      <td>92.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.473408</td>\n",
       "      <td>0.878001</td>\n",
       "      <td>65.832100</td>\n",
       "      <td>92.083000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+SIB\n",
      "{'eval_loss': 3.8303322792053223, 'eval_accuracy': 0.940014847809948, 'eval_f1': 0.9392432285855458, 'eval_precision': 0.938451741012869, 'eval_recall': 0.9401792273357352, 'eval_runtime': 27.419, 'eval_samples_per_second': 245.633, 'epoch': 9.72, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+SIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb41136929b4099835c105e00792d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918ff9b525024624af842c8f4388affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='48000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48000/143957 6:46:20 < 13:32:20, 1.97 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.812479</td>\n",
       "      <td>102.736300</td>\n",
       "      <td>59.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.423294</td>\n",
       "      <td>0.832323</td>\n",
       "      <td>102.765800</td>\n",
       "      <td>58.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.453584</td>\n",
       "      <td>0.842464</td>\n",
       "      <td>102.804500</td>\n",
       "      <td>58.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.846708</td>\n",
       "      <td>102.925200</td>\n",
       "      <td>58.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.440391</td>\n",
       "      <td>0.850654</td>\n",
       "      <td>102.831400</td>\n",
       "      <td>58.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.435332</td>\n",
       "      <td>0.843656</td>\n",
       "      <td>102.873300</td>\n",
       "      <td>58.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.468316</td>\n",
       "      <td>0.851173</td>\n",
       "      <td>102.855500</td>\n",
       "      <td>58.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.427456</td>\n",
       "      <td>0.861764</td>\n",
       "      <td>102.790200</td>\n",
       "      <td>58.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.426851</td>\n",
       "      <td>0.861085</td>\n",
       "      <td>102.819300</td>\n",
       "      <td>58.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.450499</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>102.878000</td>\n",
       "      <td>58.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.863581</td>\n",
       "      <td>102.810600</td>\n",
       "      <td>58.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.442699</td>\n",
       "      <td>0.854382</td>\n",
       "      <td>102.885600</td>\n",
       "      <td>58.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.430385</td>\n",
       "      <td>0.861839</td>\n",
       "      <td>102.840000</td>\n",
       "      <td>58.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.414435</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>102.818900</td>\n",
       "      <td>58.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.422008</td>\n",
       "      <td>0.862346</td>\n",
       "      <td>102.828700</td>\n",
       "      <td>58.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.464207</td>\n",
       "      <td>0.858090</td>\n",
       "      <td>102.829500</td>\n",
       "      <td>58.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.545828</td>\n",
       "      <td>0.832932</td>\n",
       "      <td>102.808200</td>\n",
       "      <td>58.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.433369</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>102.875300</td>\n",
       "      <td>58.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.409994</td>\n",
       "      <td>0.862190</td>\n",
       "      <td>102.829400</td>\n",
       "      <td>58.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>0.861647</td>\n",
       "      <td>103.074400</td>\n",
       "      <td>58.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.412270</td>\n",
       "      <td>0.865184</td>\n",
       "      <td>102.864000</td>\n",
       "      <td>58.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.427260</td>\n",
       "      <td>0.862817</td>\n",
       "      <td>102.836600</td>\n",
       "      <td>58.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.442447</td>\n",
       "      <td>0.864514</td>\n",
       "      <td>102.770200</td>\n",
       "      <td>58.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.449532</td>\n",
       "      <td>0.863822</td>\n",
       "      <td>102.769800</td>\n",
       "      <td>58.986000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+INVSIB\n",
      "{'eval_loss': 2.4658586978912354, 'eval_accuracy': 0.9349665924276169, 'eval_f1': 0.9340570007822142, 'eval_precision': 0.9336723618090452, 'eval_recall': 0.9344706039030594, 'eval_runtime': 27.414, 'eval_samples_per_second': 245.677, 'epoch': 3.33, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+INVSIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f35e46fa3f74dcebc789dd2f671f46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626b0281fdd7494cb9c30928faa47574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='78000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 78000/143957 5:32:56 < 4:41:32, 3.90 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.383299</td>\n",
       "      <td>0.851534</td>\n",
       "      <td>36.791500</td>\n",
       "      <td>164.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.316063</td>\n",
       "      <td>0.887001</td>\n",
       "      <td>36.756800</td>\n",
       "      <td>164.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.338949</td>\n",
       "      <td>0.902507</td>\n",
       "      <td>36.816700</td>\n",
       "      <td>164.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.301207</td>\n",
       "      <td>0.910261</td>\n",
       "      <td>36.813900</td>\n",
       "      <td>164.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.377753</td>\n",
       "      <td>0.900033</td>\n",
       "      <td>36.785900</td>\n",
       "      <td>164.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.317735</td>\n",
       "      <td>0.910426</td>\n",
       "      <td>36.809200</td>\n",
       "      <td>164.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.293654</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>36.786600</td>\n",
       "      <td>164.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.329496</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>36.679000</td>\n",
       "      <td>165.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.315154</td>\n",
       "      <td>0.922963</td>\n",
       "      <td>36.798100</td>\n",
       "      <td>164.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.922963</td>\n",
       "      <td>36.794400</td>\n",
       "      <td>164.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>0.294206</td>\n",
       "      <td>0.932531</td>\n",
       "      <td>36.785800</td>\n",
       "      <td>164.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.287785</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>36.807900</td>\n",
       "      <td>164.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>0.927252</td>\n",
       "      <td>36.755700</td>\n",
       "      <td>164.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.291584</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>36.768800</td>\n",
       "      <td>164.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.295040</td>\n",
       "      <td>0.936490</td>\n",
       "      <td>36.843800</td>\n",
       "      <td>164.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.291778</td>\n",
       "      <td>0.930221</td>\n",
       "      <td>36.775200</td>\n",
       "      <td>164.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>0.933520</td>\n",
       "      <td>36.815600</td>\n",
       "      <td>164.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.281116</td>\n",
       "      <td>0.937809</td>\n",
       "      <td>36.779500</td>\n",
       "      <td>164.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>0.284303</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>36.849300</td>\n",
       "      <td>164.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.290451</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>36.798100</td>\n",
       "      <td>164.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.284573</td>\n",
       "      <td>0.934180</td>\n",
       "      <td>36.798300</td>\n",
       "      <td>164.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.287662</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>36.809500</td>\n",
       "      <td>164.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.267845</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>36.830900</td>\n",
       "      <td>164.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.275531</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>36.832900</td>\n",
       "      <td>164.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.275815</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>36.795100</td>\n",
       "      <td>164.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.278756</td>\n",
       "      <td>0.944573</td>\n",
       "      <td>36.828000</td>\n",
       "      <td>164.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.278806</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>36.818500</td>\n",
       "      <td>164.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.264328</td>\n",
       "      <td>0.950181</td>\n",
       "      <td>36.813900</td>\n",
       "      <td>164.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>0.954965</td>\n",
       "      <td>36.788400</td>\n",
       "      <td>164.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.270241</td>\n",
       "      <td>0.953151</td>\n",
       "      <td>36.809200</td>\n",
       "      <td>164.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.254355</td>\n",
       "      <td>0.949357</td>\n",
       "      <td>36.788600</td>\n",
       "      <td>164.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.264572</td>\n",
       "      <td>0.954635</td>\n",
       "      <td>36.781600</td>\n",
       "      <td>164.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.248106</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>36.830100</td>\n",
       "      <td>164.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.255386</td>\n",
       "      <td>0.954306</td>\n",
       "      <td>36.810900</td>\n",
       "      <td>164.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.249787</td>\n",
       "      <td>0.951006</td>\n",
       "      <td>36.807200</td>\n",
       "      <td>164.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.267284</td>\n",
       "      <td>0.952656</td>\n",
       "      <td>36.611300</td>\n",
       "      <td>165.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.268296</td>\n",
       "      <td>0.953811</td>\n",
       "      <td>36.779500</td>\n",
       "      <td>164.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.254187</td>\n",
       "      <td>0.950511</td>\n",
       "      <td>36.835400</td>\n",
       "      <td>164.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.258635</td>\n",
       "      <td>0.954141</td>\n",
       "      <td>36.793700</td>\n",
       "      <td>164.756000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+TextMix\n",
      "{'eval_loss': 3.459319591522217, 'eval_accuracy': 0.9459539717891611, 'eval_f1': 0.9451746127107483, 'eval_precision': 0.9449550204900994, 'eval_recall': 0.9454027160028868, 'eval_runtime': 27.4066, 'eval_samples_per_second': 245.743, 'epoch': 5.42, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+TextMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c7e27b1d5a4a16805f5c17b78dd87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bf8b6dfbbb40e7a6a8cf975213b15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='212000' max='215936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [212000/215936 15:53:50 < 17:42, 3.70 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.338679</td>\n",
       "      <td>0.717515</td>\n",
       "      <td>54.981200</td>\n",
       "      <td>165.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.307838</td>\n",
       "      <td>0.746250</td>\n",
       "      <td>54.950000</td>\n",
       "      <td>165.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.270661</td>\n",
       "      <td>0.769150</td>\n",
       "      <td>54.908100</td>\n",
       "      <td>165.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>0.784238</td>\n",
       "      <td>55.018900</td>\n",
       "      <td>165.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.253895</td>\n",
       "      <td>0.786836</td>\n",
       "      <td>54.982700</td>\n",
       "      <td>165.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.248588</td>\n",
       "      <td>0.792271</td>\n",
       "      <td>54.977200</td>\n",
       "      <td>165.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.252973</td>\n",
       "      <td>0.797020</td>\n",
       "      <td>54.983700</td>\n",
       "      <td>165.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.240353</td>\n",
       "      <td>0.791805</td>\n",
       "      <td>54.986000</td>\n",
       "      <td>165.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.256544</td>\n",
       "      <td>0.791607</td>\n",
       "      <td>54.948600</td>\n",
       "      <td>165.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.242523</td>\n",
       "      <td>0.790439</td>\n",
       "      <td>54.987000</td>\n",
       "      <td>165.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.270195</td>\n",
       "      <td>0.784417</td>\n",
       "      <td>54.934800</td>\n",
       "      <td>165.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.236661</td>\n",
       "      <td>0.790370</td>\n",
       "      <td>54.951200</td>\n",
       "      <td>165.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.231535</td>\n",
       "      <td>0.799047</td>\n",
       "      <td>54.962200</td>\n",
       "      <td>165.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.231629</td>\n",
       "      <td>0.797779</td>\n",
       "      <td>54.969100</td>\n",
       "      <td>165.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.249375</td>\n",
       "      <td>0.794901</td>\n",
       "      <td>54.960300</td>\n",
       "      <td>165.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>0.228163</td>\n",
       "      <td>0.793371</td>\n",
       "      <td>54.972400</td>\n",
       "      <td>165.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.242225</td>\n",
       "      <td>0.801091</td>\n",
       "      <td>54.984600</td>\n",
       "      <td>165.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.235281</td>\n",
       "      <td>0.797899</td>\n",
       "      <td>55.017700</td>\n",
       "      <td>165.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.218444</td>\n",
       "      <td>0.807991</td>\n",
       "      <td>54.975200</td>\n",
       "      <td>165.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.226697</td>\n",
       "      <td>0.798381</td>\n",
       "      <td>55.053000</td>\n",
       "      <td>165.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.221267</td>\n",
       "      <td>0.805551</td>\n",
       "      <td>55.044800</td>\n",
       "      <td>165.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.211106</td>\n",
       "      <td>0.809725</td>\n",
       "      <td>54.989300</td>\n",
       "      <td>165.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.232575</td>\n",
       "      <td>0.801899</td>\n",
       "      <td>55.006500</td>\n",
       "      <td>165.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.217191</td>\n",
       "      <td>0.803153</td>\n",
       "      <td>54.994000</td>\n",
       "      <td>165.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.221724</td>\n",
       "      <td>0.806477</td>\n",
       "      <td>54.991800</td>\n",
       "      <td>165.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.221510</td>\n",
       "      <td>0.810512</td>\n",
       "      <td>55.002600</td>\n",
       "      <td>165.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.210657</td>\n",
       "      <td>0.812368</td>\n",
       "      <td>55.008000</td>\n",
       "      <td>165.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.212473</td>\n",
       "      <td>0.808708</td>\n",
       "      <td>55.027500</td>\n",
       "      <td>165.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.216090</td>\n",
       "      <td>0.808363</td>\n",
       "      <td>54.984500</td>\n",
       "      <td>165.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.218911</td>\n",
       "      <td>0.809860</td>\n",
       "      <td>55.030400</td>\n",
       "      <td>165.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.219492</td>\n",
       "      <td>0.811131</td>\n",
       "      <td>54.929300</td>\n",
       "      <td>165.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.221005</td>\n",
       "      <td>0.809513</td>\n",
       "      <td>55.019800</td>\n",
       "      <td>165.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.224415</td>\n",
       "      <td>0.808560</td>\n",
       "      <td>54.983200</td>\n",
       "      <td>165.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.213482</td>\n",
       "      <td>0.811772</td>\n",
       "      <td>55.009200</td>\n",
       "      <td>165.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.211058</td>\n",
       "      <td>0.813729</td>\n",
       "      <td>54.985600</td>\n",
       "      <td>165.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.211311</td>\n",
       "      <td>0.812620</td>\n",
       "      <td>54.981500</td>\n",
       "      <td>165.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.205965</td>\n",
       "      <td>0.811053</td>\n",
       "      <td>54.943800</td>\n",
       "      <td>165.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.202033</td>\n",
       "      <td>0.816649</td>\n",
       "      <td>55.007900</td>\n",
       "      <td>165.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.215735</td>\n",
       "      <td>0.812895</td>\n",
       "      <td>54.969300</td>\n",
       "      <td>165.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.222286</td>\n",
       "      <td>0.809929</td>\n",
       "      <td>55.003500</td>\n",
       "      <td>165.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.213776</td>\n",
       "      <td>0.814858</td>\n",
       "      <td>54.958800</td>\n",
       "      <td>165.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.812272</td>\n",
       "      <td>55.052900</td>\n",
       "      <td>165.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.210590</td>\n",
       "      <td>0.813390</td>\n",
       "      <td>54.999900</td>\n",
       "      <td>165.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.199148</td>\n",
       "      <td>0.817947</td>\n",
       "      <td>55.023800</td>\n",
       "      <td>165.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.198142</td>\n",
       "      <td>0.815124</td>\n",
       "      <td>54.986700</td>\n",
       "      <td>165.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.217549</td>\n",
       "      <td>0.815911</td>\n",
       "      <td>55.053900</td>\n",
       "      <td>165.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.206882</td>\n",
       "      <td>0.816863</td>\n",
       "      <td>54.993500</td>\n",
       "      <td>165.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.211085</td>\n",
       "      <td>0.815282</td>\n",
       "      <td>55.021300</td>\n",
       "      <td>165.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>0.202157</td>\n",
       "      <td>0.818167</td>\n",
       "      <td>54.985000</td>\n",
       "      <td>165.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.200291</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>55.033500</td>\n",
       "      <td>165.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.181600</td>\n",
       "      <td>0.214503</td>\n",
       "      <td>0.816861</td>\n",
       "      <td>55.032900</td>\n",
       "      <td>165.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.205969</td>\n",
       "      <td>0.819398</td>\n",
       "      <td>55.002800</td>\n",
       "      <td>165.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.194013</td>\n",
       "      <td>0.818977</td>\n",
       "      <td>54.989900</td>\n",
       "      <td>165.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.189799</td>\n",
       "      <td>0.817200</td>\n",
       "      <td>55.036300</td>\n",
       "      <td>165.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.195235</td>\n",
       "      <td>0.817552</td>\n",
       "      <td>55.029700</td>\n",
       "      <td>165.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.199285</td>\n",
       "      <td>0.820705</td>\n",
       "      <td>55.038800</td>\n",
       "      <td>165.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.190568</td>\n",
       "      <td>0.817422</td>\n",
       "      <td>54.993400</td>\n",
       "      <td>165.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.193035</td>\n",
       "      <td>0.822239</td>\n",
       "      <td>55.011300</td>\n",
       "      <td>165.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.191958</td>\n",
       "      <td>0.821814</td>\n",
       "      <td>55.202100</td>\n",
       "      <td>164.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.193580</td>\n",
       "      <td>0.822120</td>\n",
       "      <td>54.991200</td>\n",
       "      <td>165.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.191495</td>\n",
       "      <td>0.819188</td>\n",
       "      <td>54.978700</td>\n",
       "      <td>165.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.822548</td>\n",
       "      <td>54.984400</td>\n",
       "      <td>165.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.188704</td>\n",
       "      <td>0.822744</td>\n",
       "      <td>54.978200</td>\n",
       "      <td>165.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>0.823918</td>\n",
       "      <td>54.984500</td>\n",
       "      <td>165.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.818768</td>\n",
       "      <td>54.944800</td>\n",
       "      <td>165.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.199585</td>\n",
       "      <td>0.822427</td>\n",
       "      <td>55.002800</td>\n",
       "      <td>165.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.194584</td>\n",
       "      <td>0.822993</td>\n",
       "      <td>54.964600</td>\n",
       "      <td>165.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.198394</td>\n",
       "      <td>0.824196</td>\n",
       "      <td>55.003600</td>\n",
       "      <td>165.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.195736</td>\n",
       "      <td>0.823315</td>\n",
       "      <td>54.941600</td>\n",
       "      <td>165.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.192812</td>\n",
       "      <td>0.823483</td>\n",
       "      <td>54.990700</td>\n",
       "      <td>165.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.193613</td>\n",
       "      <td>0.825188</td>\n",
       "      <td>54.981100</td>\n",
       "      <td>165.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.193765</td>\n",
       "      <td>0.825914</td>\n",
       "      <td>54.969100</td>\n",
       "      <td>165.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.189597</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>54.945500</td>\n",
       "      <td>165.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.824493</td>\n",
       "      <td>55.022800</td>\n",
       "      <td>165.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.196374</td>\n",
       "      <td>0.826065</td>\n",
       "      <td>54.987400</td>\n",
       "      <td>165.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.186028</td>\n",
       "      <td>0.826228</td>\n",
       "      <td>55.008500</td>\n",
       "      <td>165.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.185937</td>\n",
       "      <td>0.825585</td>\n",
       "      <td>54.957200</td>\n",
       "      <td>165.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>0.191471</td>\n",
       "      <td>0.826763</td>\n",
       "      <td>55.457700</td>\n",
       "      <td>163.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.183537</td>\n",
       "      <td>0.825375</td>\n",
       "      <td>54.983400</td>\n",
       "      <td>165.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.191743</td>\n",
       "      <td>0.825412</td>\n",
       "      <td>55.000500</td>\n",
       "      <td>165.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.152700</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>0.826301</td>\n",
       "      <td>54.973600</td>\n",
       "      <td>165.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.195205</td>\n",
       "      <td>0.826502</td>\n",
       "      <td>54.988000</td>\n",
       "      <td>165.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.185180</td>\n",
       "      <td>0.827759</td>\n",
       "      <td>54.911900</td>\n",
       "      <td>165.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.183038</td>\n",
       "      <td>0.827036</td>\n",
       "      <td>54.957800</td>\n",
       "      <td>165.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.152100</td>\n",
       "      <td>0.182586</td>\n",
       "      <td>0.826421</td>\n",
       "      <td>55.010300</td>\n",
       "      <td>165.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.184384</td>\n",
       "      <td>0.828116</td>\n",
       "      <td>55.031200</td>\n",
       "      <td>165.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.187889</td>\n",
       "      <td>0.827051</td>\n",
       "      <td>55.015300</td>\n",
       "      <td>165.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.190174</td>\n",
       "      <td>0.827573</td>\n",
       "      <td>55.030700</td>\n",
       "      <td>165.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.187445</td>\n",
       "      <td>0.826859</td>\n",
       "      <td>54.983100</td>\n",
       "      <td>165.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.182508</td>\n",
       "      <td>0.826956</td>\n",
       "      <td>54.983100</td>\n",
       "      <td>165.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.827854</td>\n",
       "      <td>54.948900</td>\n",
       "      <td>165.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.182045</td>\n",
       "      <td>0.828148</td>\n",
       "      <td>55.028300</td>\n",
       "      <td>165.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.187327</td>\n",
       "      <td>0.827868</td>\n",
       "      <td>54.967700</td>\n",
       "      <td>165.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.182305</td>\n",
       "      <td>0.828129</td>\n",
       "      <td>54.949200</td>\n",
       "      <td>165.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.178599</td>\n",
       "      <td>0.829253</td>\n",
       "      <td>54.979200</td>\n",
       "      <td>165.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.181937</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>54.968900</td>\n",
       "      <td>165.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.183634</td>\n",
       "      <td>0.829037</td>\n",
       "      <td>55.008900</td>\n",
       "      <td>165.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.188024</td>\n",
       "      <td>0.829559</td>\n",
       "      <td>54.966300</td>\n",
       "      <td>165.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.187243</td>\n",
       "      <td>0.828810</td>\n",
       "      <td>54.952400</td>\n",
       "      <td>165.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.829265</td>\n",
       "      <td>54.942200</td>\n",
       "      <td>165.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.186057</td>\n",
       "      <td>0.828246</td>\n",
       "      <td>55.039500</td>\n",
       "      <td>165.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>0.189876</td>\n",
       "      <td>0.828628</td>\n",
       "      <td>54.962000</td>\n",
       "      <td>165.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.184191</td>\n",
       "      <td>0.828479</td>\n",
       "      <td>54.989600</td>\n",
       "      <td>165.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.183313</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>54.933900</td>\n",
       "      <td>165.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.185938</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>55.256100</td>\n",
       "      <td>164.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.186391</td>\n",
       "      <td>0.828301</td>\n",
       "      <td>54.993800</td>\n",
       "      <td>165.346000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+SentMix\n",
      "{'eval_loss': 5.534175395965576, 'eval_accuracy': 0.9505567928730512, 'eval_f1': 0.9498420051845728, 'eval_precision': 0.9496353869170993, 'eval_recall': 0.950056000151792, 'eval_runtime': 27.4023, 'eval_samples_per_second': 245.783, 'epoch': 9.82, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+SentMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb8051b625f445594da4a5f3efe8721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c3ed9adadd45abb38d8ee727b9d84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='148000' max='287916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [148000/287916 13:05:36 < 12:22:42, 3.14 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.311611</td>\n",
       "      <td>0.585290</td>\n",
       "      <td>84.327000</td>\n",
       "      <td>143.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.288362</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>84.327900</td>\n",
       "      <td>143.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.256058</td>\n",
       "      <td>0.626746</td>\n",
       "      <td>84.362800</td>\n",
       "      <td>143.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.238809</td>\n",
       "      <td>0.639301</td>\n",
       "      <td>84.387000</td>\n",
       "      <td>143.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.232733</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>84.310400</td>\n",
       "      <td>143.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.245010</td>\n",
       "      <td>0.641539</td>\n",
       "      <td>84.384900</td>\n",
       "      <td>143.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.219021</td>\n",
       "      <td>0.648233</td>\n",
       "      <td>84.388300</td>\n",
       "      <td>143.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.219463</td>\n",
       "      <td>0.646787</td>\n",
       "      <td>84.358600</td>\n",
       "      <td>143.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.224067</td>\n",
       "      <td>0.653201</td>\n",
       "      <td>84.353700</td>\n",
       "      <td>143.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.223048</td>\n",
       "      <td>0.639807</td>\n",
       "      <td>84.384100</td>\n",
       "      <td>143.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.222187</td>\n",
       "      <td>0.650228</td>\n",
       "      <td>84.311300</td>\n",
       "      <td>143.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.212346</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>84.374800</td>\n",
       "      <td>143.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.234921</td>\n",
       "      <td>0.654860</td>\n",
       "      <td>84.341900</td>\n",
       "      <td>143.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.221553</td>\n",
       "      <td>0.658568</td>\n",
       "      <td>84.361800</td>\n",
       "      <td>143.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.230052</td>\n",
       "      <td>0.643734</td>\n",
       "      <td>84.351600</td>\n",
       "      <td>143.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.245284</td>\n",
       "      <td>0.639310</td>\n",
       "      <td>84.344600</td>\n",
       "      <td>143.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.218392</td>\n",
       "      <td>0.656565</td>\n",
       "      <td>84.404500</td>\n",
       "      <td>143.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.224345</td>\n",
       "      <td>0.650552</td>\n",
       "      <td>84.326800</td>\n",
       "      <td>143.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.241106</td>\n",
       "      <td>0.662040</td>\n",
       "      <td>84.325900</td>\n",
       "      <td>143.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.224799</td>\n",
       "      <td>0.663446</td>\n",
       "      <td>84.389500</td>\n",
       "      <td>143.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.228505</td>\n",
       "      <td>0.658720</td>\n",
       "      <td>84.338000</td>\n",
       "      <td>143.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.212326</td>\n",
       "      <td>0.663297</td>\n",
       "      <td>84.367400</td>\n",
       "      <td>143.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.206668</td>\n",
       "      <td>0.664442</td>\n",
       "      <td>84.319700</td>\n",
       "      <td>143.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.219368</td>\n",
       "      <td>0.646635</td>\n",
       "      <td>84.348600</td>\n",
       "      <td>143.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.205515</td>\n",
       "      <td>0.663338</td>\n",
       "      <td>84.354800</td>\n",
       "      <td>143.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.214780</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>84.378200</td>\n",
       "      <td>143.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.211577</td>\n",
       "      <td>0.659560</td>\n",
       "      <td>84.355800</td>\n",
       "      <td>143.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.219199</td>\n",
       "      <td>0.661042</td>\n",
       "      <td>84.370700</td>\n",
       "      <td>143.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.201159</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>84.359500</td>\n",
       "      <td>143.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.199132</td>\n",
       "      <td>0.663108</td>\n",
       "      <td>84.621000</td>\n",
       "      <td>143.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.202119</td>\n",
       "      <td>0.665309</td>\n",
       "      <td>84.396000</td>\n",
       "      <td>143.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.197138</td>\n",
       "      <td>0.664452</td>\n",
       "      <td>84.387000</td>\n",
       "      <td>143.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.205131</td>\n",
       "      <td>0.667432</td>\n",
       "      <td>84.410000</td>\n",
       "      <td>143.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.227524</td>\n",
       "      <td>0.651359</td>\n",
       "      <td>84.393000</td>\n",
       "      <td>143.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.198568</td>\n",
       "      <td>0.659629</td>\n",
       "      <td>84.400900</td>\n",
       "      <td>143.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.189900</td>\n",
       "      <td>0.200648</td>\n",
       "      <td>0.670018</td>\n",
       "      <td>84.450900</td>\n",
       "      <td>143.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.198820</td>\n",
       "      <td>0.670901</td>\n",
       "      <td>84.359700</td>\n",
       "      <td>143.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.204351</td>\n",
       "      <td>0.668604</td>\n",
       "      <td>84.407000</td>\n",
       "      <td>143.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.198983</td>\n",
       "      <td>0.662545</td>\n",
       "      <td>84.415800</td>\n",
       "      <td>143.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.192567</td>\n",
       "      <td>0.668669</td>\n",
       "      <td>84.341100</td>\n",
       "      <td>143.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.201292</td>\n",
       "      <td>0.667921</td>\n",
       "      <td>84.414400</td>\n",
       "      <td>143.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>0.193893</td>\n",
       "      <td>0.670075</td>\n",
       "      <td>84.443500</td>\n",
       "      <td>143.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.196290</td>\n",
       "      <td>0.667382</td>\n",
       "      <td>84.366500</td>\n",
       "      <td>143.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>0.198253</td>\n",
       "      <td>0.672604</td>\n",
       "      <td>84.438700</td>\n",
       "      <td>143.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.192537</td>\n",
       "      <td>0.671880</td>\n",
       "      <td>84.406000</td>\n",
       "      <td>143.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.201907</td>\n",
       "      <td>0.673975</td>\n",
       "      <td>84.364000</td>\n",
       "      <td>143.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.193991</td>\n",
       "      <td>0.673640</td>\n",
       "      <td>84.392700</td>\n",
       "      <td>143.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.193752</td>\n",
       "      <td>0.672169</td>\n",
       "      <td>84.321100</td>\n",
       "      <td>143.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.197251</td>\n",
       "      <td>0.673386</td>\n",
       "      <td>84.363600</td>\n",
       "      <td>143.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>84.368900</td>\n",
       "      <td>143.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.194426</td>\n",
       "      <td>0.672171</td>\n",
       "      <td>84.404300</td>\n",
       "      <td>143.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.247631</td>\n",
       "      <td>0.654904</td>\n",
       "      <td>84.371300</td>\n",
       "      <td>143.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.200755</td>\n",
       "      <td>0.667431</td>\n",
       "      <td>84.383500</td>\n",
       "      <td>143.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.189103</td>\n",
       "      <td>0.676526</td>\n",
       "      <td>84.362300</td>\n",
       "      <td>143.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.197243</td>\n",
       "      <td>0.675997</td>\n",
       "      <td>84.365100</td>\n",
       "      <td>143.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.187584</td>\n",
       "      <td>0.677155</td>\n",
       "      <td>84.319000</td>\n",
       "      <td>143.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.188594</td>\n",
       "      <td>0.676469</td>\n",
       "      <td>84.381200</td>\n",
       "      <td>143.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.189824</td>\n",
       "      <td>0.674178</td>\n",
       "      <td>84.389300</td>\n",
       "      <td>143.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.186075</td>\n",
       "      <td>0.677627</td>\n",
       "      <td>84.363300</td>\n",
       "      <td>143.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.196809</td>\n",
       "      <td>0.676583</td>\n",
       "      <td>84.327900</td>\n",
       "      <td>143.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.202728</td>\n",
       "      <td>0.677276</td>\n",
       "      <td>84.424600</td>\n",
       "      <td>143.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.186838</td>\n",
       "      <td>0.672005</td>\n",
       "      <td>84.360800</td>\n",
       "      <td>143.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.187009</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>84.348500</td>\n",
       "      <td>143.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.193995</td>\n",
       "      <td>0.680790</td>\n",
       "      <td>84.376000</td>\n",
       "      <td>143.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.188868</td>\n",
       "      <td>0.677476</td>\n",
       "      <td>84.451500</td>\n",
       "      <td>143.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.187855</td>\n",
       "      <td>0.676546</td>\n",
       "      <td>84.367400</td>\n",
       "      <td>143.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.185525</td>\n",
       "      <td>0.678222</td>\n",
       "      <td>84.406100</td>\n",
       "      <td>143.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.182403</td>\n",
       "      <td>0.679432</td>\n",
       "      <td>84.381500</td>\n",
       "      <td>143.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.184693</td>\n",
       "      <td>0.679144</td>\n",
       "      <td>84.395000</td>\n",
       "      <td>143.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.181442</td>\n",
       "      <td>0.680138</td>\n",
       "      <td>84.366700</td>\n",
       "      <td>143.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.188912</td>\n",
       "      <td>0.678709</td>\n",
       "      <td>85.275700</td>\n",
       "      <td>142.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.189647</td>\n",
       "      <td>0.679564</td>\n",
       "      <td>84.452700</td>\n",
       "      <td>143.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.184924</td>\n",
       "      <td>0.680021</td>\n",
       "      <td>84.479900</td>\n",
       "      <td>143.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.196389</td>\n",
       "      <td>0.680360</td>\n",
       "      <td>84.438100</td>\n",
       "      <td>143.573000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+WordMix\n",
      "{'eval_loss': 3.940965414047241, 'eval_accuracy': 0.9447661469933185, 'eval_f1': 0.9439615883843995, 'eval_precision': 0.9438034857270928, 'eval_recall': 0.9441240294619689, 'eval_runtime': 27.4068, 'eval_samples_per_second': 245.742, 'epoch': 5.14, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+WordMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Loading cached split indices for dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-16356167c47d9ed8.arrow and C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-dec3809c51f6522d.arrow\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-46c582f645de95d7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-75e86d08cfc14e0a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='24000' max='71978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24000/71978 1:12:13 < 2:24:24, 5.54 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.393213</td>\n",
       "      <td>0.892445</td>\n",
       "      <td>0.890267</td>\n",
       "      <td>0.894217</td>\n",
       "      <td>0.887833</td>\n",
       "      <td>11.440800</td>\n",
       "      <td>264.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.333075</td>\n",
       "      <td>0.903992</td>\n",
       "      <td>0.902587</td>\n",
       "      <td>0.903386</td>\n",
       "      <td>0.901897</td>\n",
       "      <td>11.417800</td>\n",
       "      <td>265.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.315249</td>\n",
       "      <td>0.901023</td>\n",
       "      <td>0.900414</td>\n",
       "      <td>0.899323</td>\n",
       "      <td>0.903664</td>\n",
       "      <td>11.433400</td>\n",
       "      <td>265.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.371964</td>\n",
       "      <td>0.891125</td>\n",
       "      <td>0.890248</td>\n",
       "      <td>0.889106</td>\n",
       "      <td>0.892361</td>\n",
       "      <td>11.419900</td>\n",
       "      <td>265.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.335921</td>\n",
       "      <td>0.901353</td>\n",
       "      <td>0.899877</td>\n",
       "      <td>0.900829</td>\n",
       "      <td>0.899073</td>\n",
       "      <td>11.420600</td>\n",
       "      <td>265.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.327914</td>\n",
       "      <td>0.902342</td>\n",
       "      <td>0.900638</td>\n",
       "      <td>0.902936</td>\n",
       "      <td>0.898985</td>\n",
       "      <td>11.437800</td>\n",
       "      <td>264.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.478937</td>\n",
       "      <td>0.823821</td>\n",
       "      <td>0.823655</td>\n",
       "      <td>0.827554</td>\n",
       "      <td>0.830885</td>\n",
       "      <td>11.409500</td>\n",
       "      <td>265.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.687046</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.437800</td>\n",
       "      <td>264.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.686840</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.413900</td>\n",
       "      <td>265.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.687298</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.419900</td>\n",
       "      <td>265.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.686944</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.416700</td>\n",
       "      <td>265.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.686932</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.428700</td>\n",
       "      <td>265.210000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+ORIG\n",
      "{'eval_loss': 0.3174646496772766, 'eval_accuracy': 0.9091314031180401, 'eval_f1': 0.9075567763438361, 'eval_precision': 0.9086340328344555, 'eval_recall': 0.9066408912625086, 'eval_runtime': 27.2926, 'eval_samples_per_second': 246.77, 'epoch': 3.33, 'run': 'pretrained/roberta-base-sst2-ORIG+ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951e5dcd8fbd424185fa81eb11a81e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d087fb41804a458872c725b7d14d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='24000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24000/143957 3:25:48 < 17:08:47, 1.94 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.448942</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>0.836365</td>\n",
       "      <td>0.850960</td>\n",
       "      <td>0.831964</td>\n",
       "      <td>102.688700</td>\n",
       "      <td>59.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.315076</td>\n",
       "      <td>0.859122</td>\n",
       "      <td>0.855947</td>\n",
       "      <td>0.862412</td>\n",
       "      <td>0.852935</td>\n",
       "      <td>102.821200</td>\n",
       "      <td>58.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.377053</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>0.848505</td>\n",
       "      <td>0.856635</td>\n",
       "      <td>0.845124</td>\n",
       "      <td>102.766700</td>\n",
       "      <td>58.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.336144</td>\n",
       "      <td>0.856318</td>\n",
       "      <td>0.854471</td>\n",
       "      <td>0.855151</td>\n",
       "      <td>0.853896</td>\n",
       "      <td>102.789700</td>\n",
       "      <td>58.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.355890</td>\n",
       "      <td>0.854833</td>\n",
       "      <td>0.852745</td>\n",
       "      <td>0.854133</td>\n",
       "      <td>0.851713</td>\n",
       "      <td>102.741900</td>\n",
       "      <td>59.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.433356</td>\n",
       "      <td>0.835698</td>\n",
       "      <td>0.835161</td>\n",
       "      <td>0.835242</td>\n",
       "      <td>0.838944</td>\n",
       "      <td>102.761400</td>\n",
       "      <td>58.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.487246</td>\n",
       "      <td>0.817717</td>\n",
       "      <td>0.816675</td>\n",
       "      <td>0.815976</td>\n",
       "      <td>0.818821</td>\n",
       "      <td>102.814100</td>\n",
       "      <td>58.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>0.767074</td>\n",
       "      <td>0.765293</td>\n",
       "      <td>0.764710</td>\n",
       "      <td>0.766371</td>\n",
       "      <td>102.732400</td>\n",
       "      <td>59.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.687856</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>102.789100</td>\n",
       "      <td>58.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.687660</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>102.811900</td>\n",
       "      <td>58.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688133</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>102.945500</td>\n",
       "      <td>58.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.687639</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>102.611200</td>\n",
       "      <td>59.077000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+INV\n",
      "{'eval_loss': 0.21874354779720306, 'eval_accuracy': 0.9131403118040089, 'eval_f1': 0.9119959859193577, 'eval_precision': 0.911353330569336, 'eval_recall': 0.9127387949576415, 'eval_runtime': 28.2108, 'eval_samples_per_second': 238.738, 'epoch': 1.67, 'run': 'pretrained/roberta-base-sst2-ORIG+INV'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b48c79fcb24da280f88919346506c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d377d48e6b406eb0bbe0cc25f27c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='26000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 26000/143957 2:36:49 < 11:51:31, 2.76 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>0.534998</td>\n",
       "      <td>0.798791</td>\n",
       "      <td>59.762400</td>\n",
       "      <td>101.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.514567</td>\n",
       "      <td>0.806445</td>\n",
       "      <td>58.622700</td>\n",
       "      <td>103.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.494694</td>\n",
       "      <td>0.815696</td>\n",
       "      <td>58.551600</td>\n",
       "      <td>103.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.511400</td>\n",
       "      <td>0.456588</td>\n",
       "      <td>0.768023</td>\n",
       "      <td>57.990300</td>\n",
       "      <td>104.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.798421</td>\n",
       "      <td>57.948100</td>\n",
       "      <td>104.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.521803</td>\n",
       "      <td>0.777465</td>\n",
       "      <td>57.804800</td>\n",
       "      <td>104.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.513260</td>\n",
       "      <td>0.782137</td>\n",
       "      <td>57.687700</td>\n",
       "      <td>105.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.551190</td>\n",
       "      <td>0.797481</td>\n",
       "      <td>57.958200</td>\n",
       "      <td>104.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.478570</td>\n",
       "      <td>0.805379</td>\n",
       "      <td>57.904600</td>\n",
       "      <td>104.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.562723</td>\n",
       "      <td>0.735629</td>\n",
       "      <td>57.980100</td>\n",
       "      <td>104.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>0.610744</td>\n",
       "      <td>0.721164</td>\n",
       "      <td>57.946600</td>\n",
       "      <td>104.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>0.694475</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>58.022000</td>\n",
       "      <td>104.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.698793</td>\n",
       "      <td>0.450956</td>\n",
       "      <td>54.728900</td>\n",
       "      <td>110.764000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+SIB\n",
      "{'eval_loss': 2.6049084663391113, 'eval_accuracy': 0.912249443207127, 'eval_f1': 0.9103195355109415, 'eval_precision': 0.9140740892870709, 'eval_recall': 0.907812938550014, 'eval_runtime': 24.3833, 'eval_samples_per_second': 276.214, 'epoch': 1.81, 'run': 'pretrained/roberta-base-sst2-ORIG+SIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb80d8ad1a1c4bc6b695ec723cc07802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7de970ea8842e29b0aea62a2daf6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 3:39:15 < 15:08:05, 2.13 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.484918</td>\n",
       "      <td>0.819946</td>\n",
       "      <td>93.401200</td>\n",
       "      <td>64.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>0.482283</td>\n",
       "      <td>0.837243</td>\n",
       "      <td>93.387400</td>\n",
       "      <td>64.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.456596</td>\n",
       "      <td>0.835451</td>\n",
       "      <td>93.353000</td>\n",
       "      <td>64.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.472700</td>\n",
       "      <td>0.455429</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>93.501400</td>\n",
       "      <td>64.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.509132</td>\n",
       "      <td>0.828326</td>\n",
       "      <td>93.421200</td>\n",
       "      <td>64.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.477208</td>\n",
       "      <td>0.831463</td>\n",
       "      <td>93.286700</td>\n",
       "      <td>64.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.447735</td>\n",
       "      <td>0.837782</td>\n",
       "      <td>93.325700</td>\n",
       "      <td>64.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.485917</td>\n",
       "      <td>0.828162</td>\n",
       "      <td>93.434900</td>\n",
       "      <td>64.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.617844</td>\n",
       "      <td>0.549610</td>\n",
       "      <td>93.569400</td>\n",
       "      <td>64.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>0.537561</td>\n",
       "      <td>0.775004</td>\n",
       "      <td>93.461000</td>\n",
       "      <td>64.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.695275</td>\n",
       "      <td>0.449951</td>\n",
       "      <td>93.482600</td>\n",
       "      <td>64.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.579954</td>\n",
       "      <td>0.788546</td>\n",
       "      <td>93.401800</td>\n",
       "      <td>64.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.628847</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>93.374100</td>\n",
       "      <td>64.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.690409</td>\n",
       "      <td>0.573254</td>\n",
       "      <td>93.377700</td>\n",
       "      <td>64.919000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+INVSIB\n",
      "{'eval_loss': 2.4703521728515625, 'eval_accuracy': 0.9162583518930958, 'eval_f1': 0.9151405000270763, 'eval_precision': 0.9145500801449354, 'eval_recall': 0.9158120013059841, 'eval_runtime': 24.3979, 'eval_samples_per_second': 276.048, 'epoch': 1.94, 'run': 'pretrained/roberta-base-sst2-ORIG+INVSIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfd49c381f5488f89b4a2ba01cd93ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75acd9e40aa847b993f00ad1e7733254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='50000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 50000/143957 3:31:58 < 6:38:20, 3.93 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.354478</td>\n",
       "      <td>0.866876</td>\n",
       "      <td>32.771700</td>\n",
       "      <td>184.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.345161</td>\n",
       "      <td>0.896569</td>\n",
       "      <td>32.786700</td>\n",
       "      <td>184.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.354805</td>\n",
       "      <td>0.916694</td>\n",
       "      <td>32.804100</td>\n",
       "      <td>184.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.308590</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>32.791400</td>\n",
       "      <td>184.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.913395</td>\n",
       "      <td>33.337800</td>\n",
       "      <td>181.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.400968</td>\n",
       "      <td>0.901683</td>\n",
       "      <td>32.786400</td>\n",
       "      <td>184.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.317257</td>\n",
       "      <td>0.916364</td>\n",
       "      <td>32.783800</td>\n",
       "      <td>184.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.351028</td>\n",
       "      <td>0.912735</td>\n",
       "      <td>35.965400</td>\n",
       "      <td>168.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.334570</td>\n",
       "      <td>0.913065</td>\n",
       "      <td>35.982500</td>\n",
       "      <td>168.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.341216</td>\n",
       "      <td>0.922633</td>\n",
       "      <td>35.967100</td>\n",
       "      <td>168.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.325215</td>\n",
       "      <td>0.916859</td>\n",
       "      <td>35.986300</td>\n",
       "      <td>168.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.333753</td>\n",
       "      <td>0.916034</td>\n",
       "      <td>35.973800</td>\n",
       "      <td>168.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.329860</td>\n",
       "      <td>0.919334</td>\n",
       "      <td>35.978800</td>\n",
       "      <td>168.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.311344</td>\n",
       "      <td>0.920158</td>\n",
       "      <td>35.997000</td>\n",
       "      <td>168.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.331605</td>\n",
       "      <td>0.923952</td>\n",
       "      <td>35.994300</td>\n",
       "      <td>168.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.365634</td>\n",
       "      <td>0.910261</td>\n",
       "      <td>35.989900</td>\n",
       "      <td>168.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.349862</td>\n",
       "      <td>0.908776</td>\n",
       "      <td>35.990500</td>\n",
       "      <td>168.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.362277</td>\n",
       "      <td>0.894424</td>\n",
       "      <td>35.994100</td>\n",
       "      <td>168.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.924150</td>\n",
       "      <td>0.503299</td>\n",
       "      <td>36.003500</td>\n",
       "      <td>168.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.331347</td>\n",
       "      <td>0.910920</td>\n",
       "      <td>36.027100</td>\n",
       "      <td>168.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.346155</td>\n",
       "      <td>0.890960</td>\n",
       "      <td>36.018900</td>\n",
       "      <td>168.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.331600</td>\n",
       "      <td>0.331430</td>\n",
       "      <td>0.912735</td>\n",
       "      <td>36.162600</td>\n",
       "      <td>167.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.345008</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>35.962400</td>\n",
       "      <td>168.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.338769</td>\n",
       "      <td>0.913890</td>\n",
       "      <td>35.982500</td>\n",
       "      <td>168.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.346860</td>\n",
       "      <td>0.913395</td>\n",
       "      <td>35.990700</td>\n",
       "      <td>168.432000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+TextMix\n",
      "{'eval_loss': 2.455782413482666, 'eval_accuracy': 0.9287305122494433, 'eval_f1': 0.9277940538465882, 'eval_precision': 0.9271141219390674, 'eval_recall': 0.9285801164645637, 'eval_runtime': 26.735, 'eval_samples_per_second': 251.917, 'epoch': 3.47, 'run': 'pretrained/roberta-base-sst2-ORIG+TextMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d20f29693b47c9ba3746d8d88a37d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8eb8aa1b27444cfb09ab960ce4cbad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='70000' max='215936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70000/215936 5:16:49 < 11:00:33, 3.68 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.306651</td>\n",
       "      <td>0.736133</td>\n",
       "      <td>53.898700</td>\n",
       "      <td>168.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.325114</td>\n",
       "      <td>0.735637</td>\n",
       "      <td>53.924700</td>\n",
       "      <td>168.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.281911</td>\n",
       "      <td>0.781712</td>\n",
       "      <td>53.899000</td>\n",
       "      <td>168.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.277646</td>\n",
       "      <td>0.786360</td>\n",
       "      <td>54.072800</td>\n",
       "      <td>168.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.299366</td>\n",
       "      <td>0.793283</td>\n",
       "      <td>53.952000</td>\n",
       "      <td>168.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.261602</td>\n",
       "      <td>0.786774</td>\n",
       "      <td>54.073400</td>\n",
       "      <td>168.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.274705</td>\n",
       "      <td>0.785402</td>\n",
       "      <td>53.955700</td>\n",
       "      <td>168.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.254407</td>\n",
       "      <td>0.796274</td>\n",
       "      <td>53.904700</td>\n",
       "      <td>168.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.785323</td>\n",
       "      <td>53.859600</td>\n",
       "      <td>168.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.307247</td>\n",
       "      <td>0.775591</td>\n",
       "      <td>53.907600</td>\n",
       "      <td>168.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.267855</td>\n",
       "      <td>0.783688</td>\n",
       "      <td>53.937900</td>\n",
       "      <td>168.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.261744</td>\n",
       "      <td>0.792298</td>\n",
       "      <td>53.910900</td>\n",
       "      <td>168.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.253583</td>\n",
       "      <td>0.795577</td>\n",
       "      <td>53.904300</td>\n",
       "      <td>168.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.244288</td>\n",
       "      <td>0.798700</td>\n",
       "      <td>53.904400</td>\n",
       "      <td>168.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.258782</td>\n",
       "      <td>0.791819</td>\n",
       "      <td>53.956100</td>\n",
       "      <td>168.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.237512</td>\n",
       "      <td>0.797475</td>\n",
       "      <td>53.895200</td>\n",
       "      <td>168.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.273183</td>\n",
       "      <td>0.787834</td>\n",
       "      <td>53.922200</td>\n",
       "      <td>168.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.266308</td>\n",
       "      <td>0.784167</td>\n",
       "      <td>53.892200</td>\n",
       "      <td>168.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261473</td>\n",
       "      <td>0.792937</td>\n",
       "      <td>53.887700</td>\n",
       "      <td>168.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.274670</td>\n",
       "      <td>0.783474</td>\n",
       "      <td>53.930100</td>\n",
       "      <td>168.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.256193</td>\n",
       "      <td>0.799025</td>\n",
       "      <td>53.926100</td>\n",
       "      <td>168.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.244535</td>\n",
       "      <td>0.787918</td>\n",
       "      <td>53.921900</td>\n",
       "      <td>168.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.250145</td>\n",
       "      <td>0.800370</td>\n",
       "      <td>53.909600</td>\n",
       "      <td>168.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.258576</td>\n",
       "      <td>0.797738</td>\n",
       "      <td>53.908600</td>\n",
       "      <td>168.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.244654</td>\n",
       "      <td>0.801964</td>\n",
       "      <td>53.918300</td>\n",
       "      <td>168.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.244616</td>\n",
       "      <td>0.797370</td>\n",
       "      <td>53.891200</td>\n",
       "      <td>168.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.236712</td>\n",
       "      <td>0.800042</td>\n",
       "      <td>53.894200</td>\n",
       "      <td>168.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.256972</td>\n",
       "      <td>0.788340</td>\n",
       "      <td>53.941300</td>\n",
       "      <td>168.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.261806</td>\n",
       "      <td>0.794917</td>\n",
       "      <td>53.915700</td>\n",
       "      <td>168.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.262511</td>\n",
       "      <td>0.786040</td>\n",
       "      <td>53.917600</td>\n",
       "      <td>168.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.257093</td>\n",
       "      <td>0.793976</td>\n",
       "      <td>53.908400</td>\n",
       "      <td>168.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.257019</td>\n",
       "      <td>0.794615</td>\n",
       "      <td>53.907800</td>\n",
       "      <td>168.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.276317</td>\n",
       "      <td>0.787409</td>\n",
       "      <td>53.903400</td>\n",
       "      <td>168.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.252973</td>\n",
       "      <td>0.787044</td>\n",
       "      <td>53.880900</td>\n",
       "      <td>168.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.251535</td>\n",
       "      <td>0.791668</td>\n",
       "      <td>53.899700</td>\n",
       "      <td>168.702000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+SentMix\n",
      "{'eval_loss': 2.464665651321411, 'eval_accuracy': 0.9315515961395694, 'eval_f1': 0.9304062881630002, 'eval_precision': 0.9312374332768985, 'eval_recall': 0.9296701369994445, 'eval_runtime': 26.722, 'eval_samples_per_second': 252.039, 'epoch': 3.24, 'run': 'pretrained/roberta-base-sst2-ORIG+SentMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654e047c416246409221bd0e683a38cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d1ae2d763b4397b129824e7e84c22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-5361578f715a8ede.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='42000' max='287916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42000/287916 3:47:48 < 22:13:54, 3.07 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.279260</td>\n",
       "      <td>0.607026</td>\n",
       "      <td>85.304500</td>\n",
       "      <td>142.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.270901</td>\n",
       "      <td>0.626571</td>\n",
       "      <td>85.265900</td>\n",
       "      <td>142.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.269105</td>\n",
       "      <td>0.633852</td>\n",
       "      <td>85.277700</td>\n",
       "      <td>142.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.249096</td>\n",
       "      <td>0.642622</td>\n",
       "      <td>85.281300</td>\n",
       "      <td>142.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.262596</td>\n",
       "      <td>0.647533</td>\n",
       "      <td>85.298500</td>\n",
       "      <td>142.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.254958</td>\n",
       "      <td>0.650659</td>\n",
       "      <td>85.337300</td>\n",
       "      <td>142.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.254526</td>\n",
       "      <td>0.648893</td>\n",
       "      <td>85.301400</td>\n",
       "      <td>142.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.232856</td>\n",
       "      <td>0.651980</td>\n",
       "      <td>85.297200</td>\n",
       "      <td>142.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.229717</td>\n",
       "      <td>0.647144</td>\n",
       "      <td>85.872300</td>\n",
       "      <td>141.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.260700</td>\n",
       "      <td>0.237214</td>\n",
       "      <td>0.647800</td>\n",
       "      <td>85.283100</td>\n",
       "      <td>142.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.230155</td>\n",
       "      <td>0.655663</td>\n",
       "      <td>85.300900</td>\n",
       "      <td>142.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.240896</td>\n",
       "      <td>0.642410</td>\n",
       "      <td>85.289400</td>\n",
       "      <td>142.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.259182</td>\n",
       "      <td>0.648839</td>\n",
       "      <td>85.308000</td>\n",
       "      <td>142.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.255794</td>\n",
       "      <td>0.648713</td>\n",
       "      <td>85.313900</td>\n",
       "      <td>142.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.249633</td>\n",
       "      <td>0.641441</td>\n",
       "      <td>85.292500</td>\n",
       "      <td>142.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.251410</td>\n",
       "      <td>0.649533</td>\n",
       "      <td>85.299000</td>\n",
       "      <td>142.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.254838</td>\n",
       "      <td>0.648362</td>\n",
       "      <td>85.285800</td>\n",
       "      <td>142.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.247309</td>\n",
       "      <td>0.646598</td>\n",
       "      <td>85.299700</td>\n",
       "      <td>142.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.250576</td>\n",
       "      <td>0.649481</td>\n",
       "      <td>85.299800</td>\n",
       "      <td>142.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.256122</td>\n",
       "      <td>0.649554</td>\n",
       "      <td>85.282800</td>\n",
       "      <td>142.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.241173</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>85.308500</td>\n",
       "      <td>142.108000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/roberta-base-sst2-ORIG+WordMix\n",
      "{'eval_loss': 2.6165454387664795, 'eval_accuracy': 0.927691165553081, 'eval_f1': 0.9267435587803126, 'eval_precision': 0.9260538696514973, 'eval_recall': 0.9275435274313929, 'eval_runtime': 26.7068, 'eval_samples_per_second': 252.183, 'epoch': 1.46, 'run': 'pretrained/roberta-base-sst2-ORIG+WordMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Loading cached split indices for dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-16356167c47d9ed8.arrow and C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-dec3809c51f6522d.arrow\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-028efc0f74ecb44d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-b28264c65607a268.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-09c1e84e8f5fbf93.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='24000' max='71978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24000/71978 1:38:19 < 3:16:33, 4.07 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.312352</td>\n",
       "      <td>0.892775</td>\n",
       "      <td>0.890635</td>\n",
       "      <td>0.894401</td>\n",
       "      <td>0.888280</td>\n",
       "      <td>18.829900</td>\n",
       "      <td>160.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.381782</td>\n",
       "      <td>0.908611</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.906697</td>\n",
       "      <td>0.910409</td>\n",
       "      <td>18.848700</td>\n",
       "      <td>160.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.429949</td>\n",
       "      <td>0.904982</td>\n",
       "      <td>0.903807</td>\n",
       "      <td>0.903632</td>\n",
       "      <td>0.903989</td>\n",
       "      <td>18.909300</td>\n",
       "      <td>160.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.435633</td>\n",
       "      <td>0.897724</td>\n",
       "      <td>0.897017</td>\n",
       "      <td>0.895852</td>\n",
       "      <td>0.899796</td>\n",
       "      <td>18.856500</td>\n",
       "      <td>160.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.417101</td>\n",
       "      <td>0.874299</td>\n",
       "      <td>0.872970</td>\n",
       "      <td>0.872275</td>\n",
       "      <td>0.873853</td>\n",
       "      <td>18.834400</td>\n",
       "      <td>160.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.688235</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.806600</td>\n",
       "      <td>161.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>0.817552</td>\n",
       "      <td>0.816136</td>\n",
       "      <td>0.815259</td>\n",
       "      <td>0.818033</td>\n",
       "      <td>18.879100</td>\n",
       "      <td>160.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>0.632806</td>\n",
       "      <td>0.736391</td>\n",
       "      <td>0.716224</td>\n",
       "      <td>0.758463</td>\n",
       "      <td>0.715455</td>\n",
       "      <td>18.884200</td>\n",
       "      <td>160.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.721050</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.808100</td>\n",
       "      <td>161.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.705244</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.841300</td>\n",
       "      <td>160.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>0.686834</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.845100</td>\n",
       "      <td>160.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.692800</td>\n",
       "      <td>0.686995</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.791000</td>\n",
       "      <td>161.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/xlnet-base-cased-sst2-ORIG+ORIG\n",
      "{'eval_loss': 0.3458961248397827, 'eval_accuracy': 0.9167037861915367, 'eval_f1': 0.9158046376389559, 'eval_precision': 0.914547097908931, 'eval_recall': 0.9176716771154936, 'eval_runtime': 41.1171, 'eval_samples_per_second': 163.8, 'epoch': 3.33, 'run': 'pretrained/xlnet-base-cased-sst2-ORIG+ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4d5054d68944d790ebb78469c77119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9460889f35e9403f9033f38516fa7451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-09c1e84e8f5fbf93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='32000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32000/143957 7:31:29 < 26:19:41, 1.18 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.396362</td>\n",
       "      <td>0.837347</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.843483</td>\n",
       "      <td>0.828887</td>\n",
       "      <td>242.259700</td>\n",
       "      <td>25.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.387593</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.833751</td>\n",
       "      <td>0.853355</td>\n",
       "      <td>0.828791</td>\n",
       "      <td>242.061200</td>\n",
       "      <td>25.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.393578</td>\n",
       "      <td>0.837842</td>\n",
       "      <td>0.836866</td>\n",
       "      <td>0.836063</td>\n",
       "      <td>0.838960</td>\n",
       "      <td>242.105400</td>\n",
       "      <td>25.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.332733</td>\n",
       "      <td>0.847080</td>\n",
       "      <td>0.846830</td>\n",
       "      <td>0.848546</td>\n",
       "      <td>0.852221</td>\n",
       "      <td>242.057100</td>\n",
       "      <td>25.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.443054</td>\n",
       "      <td>0.838502</td>\n",
       "      <td>0.830953</td>\n",
       "      <td>0.857970</td>\n",
       "      <td>0.825417</td>\n",
       "      <td>242.134900</td>\n",
       "      <td>25.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.351211</td>\n",
       "      <td>0.854174</td>\n",
       "      <td>0.850674</td>\n",
       "      <td>0.858121</td>\n",
       "      <td>0.847441</td>\n",
       "      <td>242.078000</td>\n",
       "      <td>25.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.437923</td>\n",
       "      <td>0.833718</td>\n",
       "      <td>0.830494</td>\n",
       "      <td>0.834524</td>\n",
       "      <td>0.828368</td>\n",
       "      <td>242.078100</td>\n",
       "      <td>25.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.533394</td>\n",
       "      <td>0.762455</td>\n",
       "      <td>0.737825</td>\n",
       "      <td>0.817856</td>\n",
       "      <td>0.738426</td>\n",
       "      <td>242.062000</td>\n",
       "      <td>25.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.626900</td>\n",
       "      <td>0.534666</td>\n",
       "      <td>0.791818</td>\n",
       "      <td>0.790153</td>\n",
       "      <td>0.789529</td>\n",
       "      <td>0.791181</td>\n",
       "      <td>242.150200</td>\n",
       "      <td>25.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.632205</td>\n",
       "      <td>0.693665</td>\n",
       "      <td>0.693097</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.705953</td>\n",
       "      <td>242.171800</td>\n",
       "      <td>25.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.687649</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.110100</td>\n",
       "      <td>25.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.687647</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.131300</td>\n",
       "      <td>25.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.687639</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.240400</td>\n",
       "      <td>25.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.695937</td>\n",
       "      <td>0.447542</td>\n",
       "      <td>0.309174</td>\n",
       "      <td>0.223771</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.396100</td>\n",
       "      <td>25.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.687884</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.171300</td>\n",
       "      <td>25.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.687797</td>\n",
       "      <td>0.552458</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.276229</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>242.100800</td>\n",
       "      <td>25.039000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/xlnet-base-cased-sst2-ORIG+INV\n",
      "{'eval_loss': 0.265116423368454, 'eval_accuracy': 0.9101707498144024, 'eval_f1': 0.9080109152230734, 'eval_precision': 0.9131023574909436, 'eval_recall': 0.9048988632067636, 'eval_runtime': 41.31, 'eval_samples_per_second': 163.036, 'epoch': 2.22, 'run': 'pretrained/xlnet-base-cased-sst2-ORIG+INV'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb386190e914eb0b4ab1ddf4ce87ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65af60e66d9e48239a9029f3232d2eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-09c1e84e8f5fbf93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 4:31:02 < 18:42:34, 1.72 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.616100</td>\n",
       "      <td>0.505184</td>\n",
       "      <td>0.778020</td>\n",
       "      <td>123.694200</td>\n",
       "      <td>49.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.495983</td>\n",
       "      <td>0.789968</td>\n",
       "      <td>123.906300</td>\n",
       "      <td>48.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.439925</td>\n",
       "      <td>0.821562</td>\n",
       "      <td>123.624400</td>\n",
       "      <td>49.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.445891</td>\n",
       "      <td>0.825906</td>\n",
       "      <td>123.657300</td>\n",
       "      <td>49.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.559553</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>123.619800</td>\n",
       "      <td>49.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.494700</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>0.805589</td>\n",
       "      <td>123.675300</td>\n",
       "      <td>49.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.512846</td>\n",
       "      <td>0.811192</td>\n",
       "      <td>123.620500</td>\n",
       "      <td>49.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.533421</td>\n",
       "      <td>0.801672</td>\n",
       "      <td>123.733700</td>\n",
       "      <td>48.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.694392</td>\n",
       "      <td>0.555779</td>\n",
       "      <td>123.666000</td>\n",
       "      <td>49.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>0.698962</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>123.698600</td>\n",
       "      <td>49.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>0.695592</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>123.871900</td>\n",
       "      <td>48.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.695038</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>123.694700</td>\n",
       "      <td>49.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.699781</td>\n",
       "      <td>0.450956</td>\n",
       "      <td>123.587800</td>\n",
       "      <td>49.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.694218</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>123.794500</td>\n",
       "      <td>48.968000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/xlnet-base-cased-sst2-ORIG+SIB\n",
      "{'eval_loss': 2.053861379623413, 'eval_accuracy': 0.9083890126206384, 'eval_f1': 0.9065926264078867, 'eval_precision': 0.9088810045927722, 'eval_recall': 0.9048819924562238, 'eval_runtime': 41.4256, 'eval_samples_per_second': 162.581, 'epoch': 1.94, 'run': 'pretrained/xlnet-base-cased-sst2-ORIG+SIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8a7bc7ad30484ebec97af9633dd2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccce8e31aa94b67bddf2ed80e80f968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-09c1e84e8f5fbf93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 6:35:50 < 27:19:25, 1.18 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.476557</td>\n",
       "      <td>0.810192</td>\n",
       "      <td>243.181700</td>\n",
       "      <td>24.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.464699</td>\n",
       "      <td>0.796834</td>\n",
       "      <td>242.754400</td>\n",
       "      <td>24.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.474495</td>\n",
       "      <td>0.833785</td>\n",
       "      <td>243.022600</td>\n",
       "      <td>24.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.463861</td>\n",
       "      <td>0.850495</td>\n",
       "      <td>242.988800</td>\n",
       "      <td>24.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.464701</td>\n",
       "      <td>0.845448</td>\n",
       "      <td>242.784900</td>\n",
       "      <td>24.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.422936</td>\n",
       "      <td>0.839863</td>\n",
       "      <td>242.712500</td>\n",
       "      <td>24.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.488287</td>\n",
       "      <td>0.829658</td>\n",
       "      <td>242.727200</td>\n",
       "      <td>24.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.495681</td>\n",
       "      <td>0.838411</td>\n",
       "      <td>242.615800</td>\n",
       "      <td>24.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>242.848200</td>\n",
       "      <td>24.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.519300</td>\n",
       "      <td>0.537047</td>\n",
       "      <td>0.819009</td>\n",
       "      <td>242.911800</td>\n",
       "      <td>24.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.505096</td>\n",
       "      <td>0.823896</td>\n",
       "      <td>242.784200</td>\n",
       "      <td>24.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.549867</td>\n",
       "      <td>0.799995</td>\n",
       "      <td>242.976300</td>\n",
       "      <td>24.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.584104</td>\n",
       "      <td>0.763458</td>\n",
       "      <td>242.738500</td>\n",
       "      <td>24.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.696369</td>\n",
       "      <td>0.449951</td>\n",
       "      <td>242.681800</td>\n",
       "      <td>24.979000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/xlnet-base-cased-sst2-ORIG+INVSIB\n",
      "{'eval_loss': 3.0075347423553467, 'eval_accuracy': 0.9146250927988122, 'eval_f1': 0.9136101696900272, 'eval_precision': 0.9125951547670516, 'eval_recall': 0.914940405528093, 'eval_runtime': 41.7222, 'eval_samples_per_second': 161.425, 'epoch': 1.94, 'run': 'pretrained/xlnet-base-cased-sst2-ORIG+INVSIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b232e834684e4faab4d386a7e997ed96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef6ae9342444c7cadfbdf4ef6811ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-09c1e84e8f5fbf93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15444' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15444/143957 1:39:11 < 13:45:28, 2.59 it/s, Epoch 1.07/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.394480</td>\n",
       "      <td>0.850874</td>\n",
       "      <td>68.629600</td>\n",
       "      <td>88.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.364768</td>\n",
       "      <td>0.876773</td>\n",
       "      <td>68.554800</td>\n",
       "      <td>88.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>0.893270</td>\n",
       "      <td>68.569500</td>\n",
       "      <td>88.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.321589</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>68.590300</td>\n",
       "      <td>88.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.324251</td>\n",
       "      <td>0.910920</td>\n",
       "      <td>68.638300</td>\n",
       "      <td>88.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.343049</td>\n",
       "      <td>0.900693</td>\n",
       "      <td>68.680600</td>\n",
       "      <td>88.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.336528</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>56.863000</td>\n",
       "      <td>106.607000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_pretrain = False\n",
    "\n",
    "results = []\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    for t in ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']: \n",
    "        \n",
    "        soft_target = False\n",
    "        eval_only = False\n",
    "        \n",
    "        checkpoint = 'pretrained/' + MODEL_NAME + \"-sst2-ORIG+\" + t \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        if t == 'ORIG':\n",
    "            train_dataset = load_dataset('glue', 'sst2', split='train[:90%]')\n",
    "            train_dataset.rename_column_('sentence', 'text')\n",
    "        else: \n",
    "            \n",
    "            # load custom data    \n",
    "            text = npy_load(\"./assets/SST2/\" + t + \"/text.npy\")\n",
    "            label = npy_load(\"./assets/SST2/\" + t + \"/label.npy\")\n",
    "            if len(label.shape) > 1:\n",
    "                df = pd.DataFrame({'text': text, 'label': label.tolist()})\n",
    "                df.text = df.text.astype(str)\n",
    "                df.label = df.label.map(lambda y: np.array(y))\n",
    "            else:\n",
    "                df = pd.DataFrame({'text': text, 'label': label})\n",
    "                df.text = df.text.astype(str)\n",
    "                df.label = df.label.astype(object)\n",
    "            train_dataset = Dataset.from_pandas(df) \n",
    "            \n",
    "            # load orig data\n",
    "            orig_dataset = load_dataset('glue', 'sst2', split='train[:90%]')\n",
    "            orig_dataset.remove_columns_(['idx'])\n",
    "            orig_dataset.rename_column_('sentence', 'text')\n",
    "            df = orig_dataset.to_pandas()\n",
    "            df = df[df.columns[::-1]]\n",
    "            df.text = df.text.astype(str)\n",
    "            if len(label.shape) > 1:\n",
    "                df.label = df.label.map(one_hot_encode)\n",
    "            else:\n",
    "                df.label = df.label.astype(object)\n",
    "            orig_dataset = Dataset.from_pandas(df)\n",
    "            \n",
    "            # merge orig + custom data\n",
    "            train_dataset = concatenate_datasets([orig_dataset, train_dataset])\n",
    "            train_dataset.shuffle()\n",
    "            \n",
    "        if use_pretrain and os.path.exists(checkpoint):\n",
    "            print('loading {}...'.format(checkpoint))\n",
    "            MODEL_NAME = checkpoint\n",
    "            eval_only = True\n",
    "            \n",
    "        # split to get train\n",
    "        dataset_dict = train_dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[-10%:]')\n",
    "        test_dataset.rename_column_('sentence', 'text')\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "            \n",
    "        train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "        eval_dataset = eval_dataset.map(tokenize, batched=True, batch_size=len(eval_dataset))\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        train_dataset.rename_column_('label', 'labels')\n",
    "        eval_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        if len(np.array(train_dataset['labels']).shape) > 1:\n",
    "            soft_target = True\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 10\n",
    "        gradient_accumulation_steps=1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            seed=1,\n",
    "            # adafactor=True,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            # gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            # run_name=checkpoint\n",
    "        )\n",
    "\n",
    "        if soft_target:\n",
    "            trainer = Trainer_w_soft_target(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                compute_metrics=compute_metrics_w_soft_target,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                data_collator=DefaultCollator(),\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "            )\n",
    "        else: \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "            )\n",
    "\n",
    "        if not eval_only:\n",
    "            trainer.train()\n",
    "        \n",
    "        trainer.compute_metrics = compute_metrics\n",
    "            \n",
    "        # test ORIG\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        out = trainer.evaluate()\n",
    "        out['run'] = checkpoint\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out))   \n",
    "        \n",
    "        results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_r3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_SST2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00554915f7ad47b9b80d65294dbfdd37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0768ccf7a2e64f819c7401d7db258d16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14dc24ef59d341a88d1bdb69f3cacde6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "212a6447739d45b5b57e5815ff538f57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "367cfe385d38427dbdf4325a70c0dc2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "46cc6e7671244f1d9167855a45a36af8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1f7506e1a44d109d918b1f16d6bb75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5f641be531341dbb007330b7e245169",
      "max": 1382015,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_00554915f7ad47b9b80d65294dbfdd37",
      "value": 1382015
     }
    },
    "59b96dd300ae4a56b2d58ca0de0bb7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd27134d896448b6b385ce45da7556ee",
       "IPY_MODEL_c92040617a854d8d96cc8ba678f0b271"
      ],
      "layout": "IPY_MODEL_e3c71b92f2a0484bb70123a86f855f9d"
     }
    },
    "7158ce3f315647aba9299519fcec4be7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81821a97e02445539ccabcc2091cba95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b1f7506e1a44d109d918b1f16d6bb75",
       "IPY_MODEL_f177b91513ec4039bc7c6e61d15db9a2"
      ],
      "layout": "IPY_MODEL_212a6447739d45b5b57e5815ff538f57"
     }
    },
    "c92040617a854d8d96cc8ba678f0b271": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46cc6e7671244f1d9167855a45a36af8",
      "placeholder": "​",
      "style": "IPY_MODEL_14dc24ef59d341a88d1bdb69f3cacde6",
      "value": " 798k/798k [00:00&lt;00:00, 1.99MB/s]"
     }
    },
    "d5f641be531341dbb007330b7e245169": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd27134d896448b6b385ce45da7556ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f86a86b6b9c64cc989e89b27693d8a2f",
      "max": 798011,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_367cfe385d38427dbdf4325a70c0dc2e",
      "value": 798011
     }
    },
    "e3c71b92f2a0484bb70123a86f855f9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f177b91513ec4039bc7c6e61d15db9a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0768ccf7a2e64f819c7401d7db258d16",
      "placeholder": "​",
      "style": "IPY_MODEL_7158ce3f315647aba9299519fcec4be7",
      "value": " 1.38M/1.38M [00:00&lt;00:00, 5.04MB/s]"
     }
    },
    "f86a86b6b9c64cc989e89b27693d8a2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
