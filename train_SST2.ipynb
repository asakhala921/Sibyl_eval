{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "utjMLdmqsUuA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WT_xnGBpTNuZ"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(y, nb_classes=2):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.expand_dims(np.array(y), 0)\n",
    "    res = np.eye(nb_classes)[np.array(y).reshape(-1)]\n",
    "    return res.reshape(list(y.shape)+[nb_classes])[0]\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    if acc.item() > 1:\n",
    "        print(y_true.shape, y_true)\n",
    "        print(y_pred.shape, y_pred)\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class Trainer_w_soft_target(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "# ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nq1i8QiBtY0e"
   },
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "59b96dd300ae4a56b2d58ca0de0bb7f6",
      "e3c71b92f2a0484bb70123a86f855f9d",
      "dd27134d896448b6b385ce45da7556ee",
      "c92040617a854d8d96cc8ba678f0b271",
      "367cfe385d38427dbdf4325a70c0dc2e",
      "f86a86b6b9c64cc989e89b27693d8a2f",
      "14dc24ef59d341a88d1bdb69f3cacde6",
      "46cc6e7671244f1d9167855a45a36af8",
      "81821a97e02445539ccabcc2091cba95",
      "212a6447739d45b5b57e5815ff538f57",
      "4b1f7506e1a44d109d918b1f16d6bb75",
      "f177b91513ec4039bc7c6e61d15db9a2",
      "00554915f7ad47b9b80d65294dbfdd37",
      "d5f641be531341dbb007330b7e245169",
      "7158ce3f315647aba9299519fcec4be7",
      "0768ccf7a2e64f819c7401d7db258d16"
     ]
    },
    "id": "T-krnPy6TDSB",
    "outputId": "7161b883-f15a-449f-86fa-e8bbbfc6e9c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "rename_column_ is deprecated and will be removed in the next major version of datasets. Use the dataset.rename_column method instead.\n",
      "Loading cached split indices for dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-32a6a23548aa6274.arrow and C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-0d3572f739d46d53.arrow\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-6375dba9ab516494.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-d4c4531e1e963225.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='71978' max='71978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71978/71978 2:42:19, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.415100</td>\n",
       "      <td>0.306279</td>\n",
       "      <td>0.906631</td>\n",
       "      <td>0.906223</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.910370</td>\n",
       "      <td>10.262800</td>\n",
       "      <td>295.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.278534</td>\n",
       "      <td>0.923788</td>\n",
       "      <td>0.922177</td>\n",
       "      <td>0.927539</td>\n",
       "      <td>0.919077</td>\n",
       "      <td>10.216000</td>\n",
       "      <td>296.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.265617</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>0.925642</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.928716</td>\n",
       "      <td>10.205200</td>\n",
       "      <td>297.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.932036</td>\n",
       "      <td>0.930813</td>\n",
       "      <td>0.934033</td>\n",
       "      <td>0.928637</td>\n",
       "      <td>10.182300</td>\n",
       "      <td>297.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.283571</td>\n",
       "      <td>0.933685</td>\n",
       "      <td>0.932880</td>\n",
       "      <td>0.932848</td>\n",
       "      <td>0.932912</td>\n",
       "      <td>10.189100</td>\n",
       "      <td>297.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.262476</td>\n",
       "      <td>0.927747</td>\n",
       "      <td>0.926963</td>\n",
       "      <td>0.926440</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>10.207700</td>\n",
       "      <td>296.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>0.935335</td>\n",
       "      <td>0.934308</td>\n",
       "      <td>0.936209</td>\n",
       "      <td>0.932857</td>\n",
       "      <td>10.174300</td>\n",
       "      <td>297.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.288539</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>0.929392</td>\n",
       "      <td>0.933360</td>\n",
       "      <td>0.926861</td>\n",
       "      <td>10.172400</td>\n",
       "      <td>297.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.284688</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>0.936163</td>\n",
       "      <td>0.939669</td>\n",
       "      <td>0.933833</td>\n",
       "      <td>10.164800</td>\n",
       "      <td>298.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.308018</td>\n",
       "      <td>0.938304</td>\n",
       "      <td>0.937298</td>\n",
       "      <td>0.939472</td>\n",
       "      <td>0.935679</td>\n",
       "      <td>10.154000</td>\n",
       "      <td>298.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.305479</td>\n",
       "      <td>0.937314</td>\n",
       "      <td>0.936472</td>\n",
       "      <td>0.937037</td>\n",
       "      <td>0.935962</td>\n",
       "      <td>10.151200</td>\n",
       "      <td>298.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.301189</td>\n",
       "      <td>0.937974</td>\n",
       "      <td>0.937244</td>\n",
       "      <td>0.937059</td>\n",
       "      <td>0.937437</td>\n",
       "      <td>10.165700</td>\n",
       "      <td>298.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.296335</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>0.938948</td>\n",
       "      <td>0.941403</td>\n",
       "      <td>0.937166</td>\n",
       "      <td>10.189500</td>\n",
       "      <td>297.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.302913</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>0.944297</td>\n",
       "      <td>0.943803</td>\n",
       "      <td>0.944853</td>\n",
       "      <td>10.195500</td>\n",
       "      <td>297.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.293415</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>0.939806</td>\n",
       "      <td>0.940454</td>\n",
       "      <td>0.939228</td>\n",
       "      <td>10.205300</td>\n",
       "      <td>297.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.303937</td>\n",
       "      <td>0.943583</td>\n",
       "      <td>0.942898</td>\n",
       "      <td>0.942866</td>\n",
       "      <td>0.942930</td>\n",
       "      <td>10.209500</td>\n",
       "      <td>296.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.273070</td>\n",
       "      <td>0.940944</td>\n",
       "      <td>0.940117</td>\n",
       "      <td>0.940969</td>\n",
       "      <td>0.939378</td>\n",
       "      <td>10.218700</td>\n",
       "      <td>296.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.268895</td>\n",
       "      <td>0.936984</td>\n",
       "      <td>0.936458</td>\n",
       "      <td>0.935288</td>\n",
       "      <td>0.938306</td>\n",
       "      <td>10.210300</td>\n",
       "      <td>296.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.306033</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.943532</td>\n",
       "      <td>0.943770</td>\n",
       "      <td>0.943305</td>\n",
       "      <td>10.197800</td>\n",
       "      <td>297.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.257263</td>\n",
       "      <td>0.943913</td>\n",
       "      <td>0.943177</td>\n",
       "      <td>0.943597</td>\n",
       "      <td>0.942787</td>\n",
       "      <td>10.232800</td>\n",
       "      <td>296.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.252819</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>0.946866</td>\n",
       "      <td>0.947178</td>\n",
       "      <td>0.946571</td>\n",
       "      <td>10.228200</td>\n",
       "      <td>296.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.306126</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>0.941776</td>\n",
       "      <td>0.942759</td>\n",
       "      <td>0.940938</td>\n",
       "      <td>10.237100</td>\n",
       "      <td>296.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.305699</td>\n",
       "      <td>0.948862</td>\n",
       "      <td>0.948321</td>\n",
       "      <td>0.947674</td>\n",
       "      <td>0.949081</td>\n",
       "      <td>10.249000</td>\n",
       "      <td>295.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.320540</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>0.942002</td>\n",
       "      <td>0.944091</td>\n",
       "      <td>0.940428</td>\n",
       "      <td>10.236300</td>\n",
       "      <td>296.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.294898</td>\n",
       "      <td>0.948862</td>\n",
       "      <td>0.948402</td>\n",
       "      <td>0.947311</td>\n",
       "      <td>0.949961</td>\n",
       "      <td>10.256100</td>\n",
       "      <td>295.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.291511</td>\n",
       "      <td>0.948862</td>\n",
       "      <td>0.948194</td>\n",
       "      <td>0.948581</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>10.256900</td>\n",
       "      <td>295.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.302778</td>\n",
       "      <td>0.950511</td>\n",
       "      <td>0.949869</td>\n",
       "      <td>0.950220</td>\n",
       "      <td>0.949540</td>\n",
       "      <td>10.223900</td>\n",
       "      <td>296.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.320609</td>\n",
       "      <td>0.948862</td>\n",
       "      <td>0.948307</td>\n",
       "      <td>0.947757</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>10.240100</td>\n",
       "      <td>295.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.334672</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>0.945683</td>\n",
       "      <td>0.944867</td>\n",
       "      <td>0.946703</td>\n",
       "      <td>10.222500</td>\n",
       "      <td>296.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.314415</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>0.946980</td>\n",
       "      <td>0.946383</td>\n",
       "      <td>0.947672</td>\n",
       "      <td>10.235100</td>\n",
       "      <td>296.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.313390</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>0.946707</td>\n",
       "      <td>0.945759</td>\n",
       "      <td>0.947961</td>\n",
       "      <td>10.240200</td>\n",
       "      <td>295.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.323704</td>\n",
       "      <td>0.948202</td>\n",
       "      <td>0.947640</td>\n",
       "      <td>0.947091</td>\n",
       "      <td>0.948266</td>\n",
       "      <td>10.340500</td>\n",
       "      <td>293.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.333143</td>\n",
       "      <td>0.948202</td>\n",
       "      <td>0.947661</td>\n",
       "      <td>0.946969</td>\n",
       "      <td>0.948486</td>\n",
       "      <td>10.246600</td>\n",
       "      <td>295.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.321860</td>\n",
       "      <td>0.949192</td>\n",
       "      <td>0.948651</td>\n",
       "      <td>0.948027</td>\n",
       "      <td>0.949378</td>\n",
       "      <td>10.281100</td>\n",
       "      <td>294.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.326651</td>\n",
       "      <td>0.949192</td>\n",
       "      <td>0.948651</td>\n",
       "      <td>0.948027</td>\n",
       "      <td>0.949378</td>\n",
       "      <td>10.246100</td>\n",
       "      <td>295.819000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+ORIG\n",
      "{'eval_loss': 0.34084874391555786, 'eval_accuracy': 0.9437268002969562, 'eval_f1': 0.9428203203051284, 'eval_precision': 0.9433627980381265, 'eval_recall': 0.9423196646542293, 'eval_runtime': 22.9149, 'eval_samples_per_second': 293.914, 'epoch': 10.0, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "remove_columns_ is deprecated and will be removed in the next major version of datasets. Use the dataset.remove_columns method instead.\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a06241085c4ee28104fc898a2d1d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799d8d27e45a4ca9b88db957e8f253ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28000/143957 3:15:47 < 13:30:53, 2.38 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.406990</td>\n",
       "      <td>0.827945</td>\n",
       "      <td>0.820940</td>\n",
       "      <td>0.841553</td>\n",
       "      <td>0.816177</td>\n",
       "      <td>91.143600</td>\n",
       "      <td>66.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.321684</td>\n",
       "      <td>0.857638</td>\n",
       "      <td>0.854407</td>\n",
       "      <td>0.860959</td>\n",
       "      <td>0.851381</td>\n",
       "      <td>91.330900</td>\n",
       "      <td>66.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.407728</td>\n",
       "      <td>0.865556</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.865522</td>\n",
       "      <td>0.862047</td>\n",
       "      <td>91.216100</td>\n",
       "      <td>66.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.327549</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>0.868234</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.869261</td>\n",
       "      <td>91.223600</td>\n",
       "      <td>66.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.322734</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>0.865325</td>\n",
       "      <td>0.872318</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>90.993400</td>\n",
       "      <td>66.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.331175</td>\n",
       "      <td>0.858628</td>\n",
       "      <td>0.856278</td>\n",
       "      <td>0.858877</td>\n",
       "      <td>0.854622</td>\n",
       "      <td>91.559900</td>\n",
       "      <td>66.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.463466</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.852733</td>\n",
       "      <td>0.824510</td>\n",
       "      <td>91.106900</td>\n",
       "      <td>66.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.484120</td>\n",
       "      <td>0.847080</td>\n",
       "      <td>0.845307</td>\n",
       "      <td>0.845472</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>90.929500</td>\n",
       "      <td>66.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.399292</td>\n",
       "      <td>0.855823</td>\n",
       "      <td>0.854787</td>\n",
       "      <td>0.853915</td>\n",
       "      <td>0.856388</td>\n",
       "      <td>91.347500</td>\n",
       "      <td>66.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.391869</td>\n",
       "      <td>0.867865</td>\n",
       "      <td>0.865806</td>\n",
       "      <td>0.867879</td>\n",
       "      <td>0.864382</td>\n",
       "      <td>91.057900</td>\n",
       "      <td>66.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.394821</td>\n",
       "      <td>0.863906</td>\n",
       "      <td>0.861865</td>\n",
       "      <td>0.863603</td>\n",
       "      <td>0.860624</td>\n",
       "      <td>91.438400</td>\n",
       "      <td>66.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.401942</td>\n",
       "      <td>0.854668</td>\n",
       "      <td>0.851325</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.848273</td>\n",
       "      <td>91.930400</td>\n",
       "      <td>65.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.460946</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.842051</td>\n",
       "      <td>0.842620</td>\n",
       "      <td>0.846430</td>\n",
       "      <td>90.713800</td>\n",
       "      <td>66.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.427034</td>\n",
       "      <td>0.846255</td>\n",
       "      <td>0.841235</td>\n",
       "      <td>0.855450</td>\n",
       "      <td>0.836809</td>\n",
       "      <td>91.357100</td>\n",
       "      <td>66.355000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+INV\n",
      "{'eval_loss': 0.22934286296367645, 'eval_accuracy': 0.9272457312546399, 'eval_f1': 0.9265553141535987, 'eval_precision': 0.9250828171225598, 'eval_recall': 0.929157122983028, 'eval_runtime': 23.1228, 'eval_samples_per_second': 291.271, 'epoch': 1.94, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+INV'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2161dee474c4593b7ee3b879aeeb2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac93f6a61bfa443ab46821f2b439eed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='140000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140000/143957 13:40:11 < 23:10, 2.84 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.757427</td>\n",
       "      <td>58.260400</td>\n",
       "      <td>104.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.465720</td>\n",
       "      <td>0.810361</td>\n",
       "      <td>58.843000</td>\n",
       "      <td>103.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.458156</td>\n",
       "      <td>0.811933</td>\n",
       "      <td>58.341800</td>\n",
       "      <td>103.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.463236</td>\n",
       "      <td>0.831102</td>\n",
       "      <td>57.586100</td>\n",
       "      <td>105.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.524379</td>\n",
       "      <td>0.838256</td>\n",
       "      <td>56.888100</td>\n",
       "      <td>106.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.457948</td>\n",
       "      <td>0.832105</td>\n",
       "      <td>56.408900</td>\n",
       "      <td>107.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.469572</td>\n",
       "      <td>0.834950</td>\n",
       "      <td>56.052100</td>\n",
       "      <td>108.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.487929</td>\n",
       "      <td>0.817310</td>\n",
       "      <td>56.492400</td>\n",
       "      <td>107.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.437296</td>\n",
       "      <td>0.822175</td>\n",
       "      <td>56.617500</td>\n",
       "      <td>107.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>0.450881</td>\n",
       "      <td>0.852468</td>\n",
       "      <td>57.572800</td>\n",
       "      <td>105.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.452222</td>\n",
       "      <td>0.852745</td>\n",
       "      <td>58.157800</td>\n",
       "      <td>104.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.457462</td>\n",
       "      <td>0.848311</td>\n",
       "      <td>58.279500</td>\n",
       "      <td>104.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.473472</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>58.253500</td>\n",
       "      <td>104.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.497613</td>\n",
       "      <td>0.834541</td>\n",
       "      <td>58.213500</td>\n",
       "      <td>104.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.482476</td>\n",
       "      <td>0.847077</td>\n",
       "      <td>58.261200</td>\n",
       "      <td>104.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.463877</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>58.123800</td>\n",
       "      <td>104.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.859980</td>\n",
       "      <td>58.307600</td>\n",
       "      <td>103.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.849980</td>\n",
       "      <td>58.461100</td>\n",
       "      <td>103.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>0.848031</td>\n",
       "      <td>58.318700</td>\n",
       "      <td>103.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.446638</td>\n",
       "      <td>0.847604</td>\n",
       "      <td>58.152900</td>\n",
       "      <td>104.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.484398</td>\n",
       "      <td>0.852203</td>\n",
       "      <td>59.137500</td>\n",
       "      <td>102.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.485812</td>\n",
       "      <td>0.855335</td>\n",
       "      <td>60.514400</td>\n",
       "      <td>100.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.479075</td>\n",
       "      <td>0.856346</td>\n",
       "      <td>59.943200</td>\n",
       "      <td>101.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.396400</td>\n",
       "      <td>0.458188</td>\n",
       "      <td>0.859016</td>\n",
       "      <td>59.225000</td>\n",
       "      <td>102.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.481552</td>\n",
       "      <td>0.856796</td>\n",
       "      <td>58.187500</td>\n",
       "      <td>104.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.466943</td>\n",
       "      <td>0.860783</td>\n",
       "      <td>58.159900</td>\n",
       "      <td>104.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.488041</td>\n",
       "      <td>0.862608</td>\n",
       "      <td>58.120000</td>\n",
       "      <td>104.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.480285</td>\n",
       "      <td>0.856488</td>\n",
       "      <td>58.267900</td>\n",
       "      <td>104.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.475124</td>\n",
       "      <td>0.863445</td>\n",
       "      <td>58.423800</td>\n",
       "      <td>103.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.464710</td>\n",
       "      <td>0.861171</td>\n",
       "      <td>59.127400</td>\n",
       "      <td>102.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.462993</td>\n",
       "      <td>0.866113</td>\n",
       "      <td>58.953000</td>\n",
       "      <td>102.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.472919</td>\n",
       "      <td>0.862462</td>\n",
       "      <td>59.387300</td>\n",
       "      <td>102.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.455124</td>\n",
       "      <td>0.865126</td>\n",
       "      <td>59.290800</td>\n",
       "      <td>102.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.447674</td>\n",
       "      <td>0.868819</td>\n",
       "      <td>59.461400</td>\n",
       "      <td>101.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.452757</td>\n",
       "      <td>0.867462</td>\n",
       "      <td>60.542000</td>\n",
       "      <td>100.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.448192</td>\n",
       "      <td>0.861497</td>\n",
       "      <td>59.920800</td>\n",
       "      <td>101.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.468447</td>\n",
       "      <td>0.864350</td>\n",
       "      <td>60.372700</td>\n",
       "      <td>100.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.453527</td>\n",
       "      <td>0.871815</td>\n",
       "      <td>59.637600</td>\n",
       "      <td>101.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.367700</td>\n",
       "      <td>0.436565</td>\n",
       "      <td>0.872784</td>\n",
       "      <td>59.973700</td>\n",
       "      <td>101.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.442980</td>\n",
       "      <td>0.870816</td>\n",
       "      <td>60.440300</td>\n",
       "      <td>100.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.871515</td>\n",
       "      <td>60.056000</td>\n",
       "      <td>100.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.451518</td>\n",
       "      <td>0.874201</td>\n",
       "      <td>59.661000</td>\n",
       "      <td>101.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.359600</td>\n",
       "      <td>0.460559</td>\n",
       "      <td>0.873344</td>\n",
       "      <td>59.910600</td>\n",
       "      <td>101.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.439052</td>\n",
       "      <td>0.875269</td>\n",
       "      <td>60.125000</td>\n",
       "      <td>100.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.469187</td>\n",
       "      <td>0.872730</td>\n",
       "      <td>59.663800</td>\n",
       "      <td>101.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.451762</td>\n",
       "      <td>0.867816</td>\n",
       "      <td>59.910100</td>\n",
       "      <td>101.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.462571</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>60.189900</td>\n",
       "      <td>100.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.875569</td>\n",
       "      <td>59.926600</td>\n",
       "      <td>101.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.466540</td>\n",
       "      <td>0.873815</td>\n",
       "      <td>60.113100</td>\n",
       "      <td>100.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.465065</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>60.194700</td>\n",
       "      <td>100.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.439032</td>\n",
       "      <td>0.875411</td>\n",
       "      <td>60.004500</td>\n",
       "      <td>101.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.458853</td>\n",
       "      <td>0.873919</td>\n",
       "      <td>57.387700</td>\n",
       "      <td>105.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.477599</td>\n",
       "      <td>0.877162</td>\n",
       "      <td>58.005200</td>\n",
       "      <td>104.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.469658</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>57.190900</td>\n",
       "      <td>105.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.480182</td>\n",
       "      <td>0.877905</td>\n",
       "      <td>57.740200</td>\n",
       "      <td>104.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.455310</td>\n",
       "      <td>0.878544</td>\n",
       "      <td>58.293900</td>\n",
       "      <td>103.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.457942</td>\n",
       "      <td>0.877476</td>\n",
       "      <td>57.742200</td>\n",
       "      <td>104.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.469931</td>\n",
       "      <td>0.877858</td>\n",
       "      <td>57.348400</td>\n",
       "      <td>105.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.461848</td>\n",
       "      <td>0.879407</td>\n",
       "      <td>58.487200</td>\n",
       "      <td>103.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.458868</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>57.978600</td>\n",
       "      <td>104.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.463811</td>\n",
       "      <td>0.880715</td>\n",
       "      <td>58.415700</td>\n",
       "      <td>103.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.474374</td>\n",
       "      <td>0.879247</td>\n",
       "      <td>57.612100</td>\n",
       "      <td>105.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.468091</td>\n",
       "      <td>0.878813</td>\n",
       "      <td>58.306700</td>\n",
       "      <td>103.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.459646</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>58.821500</td>\n",
       "      <td>103.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.460001</td>\n",
       "      <td>0.880732</td>\n",
       "      <td>57.952300</td>\n",
       "      <td>104.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.470576</td>\n",
       "      <td>0.877083</td>\n",
       "      <td>57.894700</td>\n",
       "      <td>104.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.464005</td>\n",
       "      <td>0.879346</td>\n",
       "      <td>58.014500</td>\n",
       "      <td>104.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.880379</td>\n",
       "      <td>57.799300</td>\n",
       "      <td>104.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.471649</td>\n",
       "      <td>0.878936</td>\n",
       "      <td>57.873100</td>\n",
       "      <td>104.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.473408</td>\n",
       "      <td>0.878001</td>\n",
       "      <td>57.752600</td>\n",
       "      <td>104.965000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+SIB\n",
      "{'eval_loss': 3.8303322792053223, 'eval_accuracy': 0.940014847809948, 'eval_f1': 0.9392432285855458, 'eval_precision': 0.938451741012869, 'eval_recall': 0.9401792273357352, 'eval_runtime': 22.9028, 'eval_samples_per_second': 294.068, 'epoch': 9.72, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+SIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b2072ed42b49d78852de3f8e95ba00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43288e44d4304164a73c3ee3c2e65b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='48000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48000/143957 5:43:38 < 11:27:00, 2.33 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.812479</td>\n",
       "      <td>92.979100</td>\n",
       "      <td>65.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.423294</td>\n",
       "      <td>0.832323</td>\n",
       "      <td>93.730500</td>\n",
       "      <td>64.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.453584</td>\n",
       "      <td>0.842464</td>\n",
       "      <td>92.523300</td>\n",
       "      <td>65.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.846708</td>\n",
       "      <td>92.647000</td>\n",
       "      <td>65.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.440391</td>\n",
       "      <td>0.850654</td>\n",
       "      <td>92.438300</td>\n",
       "      <td>65.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.435332</td>\n",
       "      <td>0.843656</td>\n",
       "      <td>92.744400</td>\n",
       "      <td>65.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.468316</td>\n",
       "      <td>0.851173</td>\n",
       "      <td>92.259700</td>\n",
       "      <td>65.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.427456</td>\n",
       "      <td>0.861764</td>\n",
       "      <td>92.647900</td>\n",
       "      <td>65.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.426851</td>\n",
       "      <td>0.861085</td>\n",
       "      <td>93.078000</td>\n",
       "      <td>65.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.450499</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>93.370200</td>\n",
       "      <td>64.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.863581</td>\n",
       "      <td>93.457500</td>\n",
       "      <td>64.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.442699</td>\n",
       "      <td>0.854382</td>\n",
       "      <td>93.410700</td>\n",
       "      <td>64.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.430385</td>\n",
       "      <td>0.861839</td>\n",
       "      <td>93.127300</td>\n",
       "      <td>65.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.414435</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>93.619700</td>\n",
       "      <td>64.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.422008</td>\n",
       "      <td>0.862346</td>\n",
       "      <td>94.277900</td>\n",
       "      <td>64.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.464207</td>\n",
       "      <td>0.858090</td>\n",
       "      <td>94.286100</td>\n",
       "      <td>64.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.545828</td>\n",
       "      <td>0.832932</td>\n",
       "      <td>93.545800</td>\n",
       "      <td>64.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.433369</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>94.152600</td>\n",
       "      <td>64.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.409994</td>\n",
       "      <td>0.862190</td>\n",
       "      <td>94.827700</td>\n",
       "      <td>63.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>0.861647</td>\n",
       "      <td>94.493000</td>\n",
       "      <td>64.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.412270</td>\n",
       "      <td>0.865184</td>\n",
       "      <td>93.125900</td>\n",
       "      <td>65.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.427260</td>\n",
       "      <td>0.862817</td>\n",
       "      <td>92.836600</td>\n",
       "      <td>65.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.442447</td>\n",
       "      <td>0.864514</td>\n",
       "      <td>91.390700</td>\n",
       "      <td>66.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.449532</td>\n",
       "      <td>0.863822</td>\n",
       "      <td>91.489600</td>\n",
       "      <td>66.259000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+INVSIB\n",
      "{'eval_loss': 2.4658586978912354, 'eval_accuracy': 0.9349665924276169, 'eval_f1': 0.9340570007822142, 'eval_precision': 0.9336723618090452, 'eval_recall': 0.9344706039030594, 'eval_runtime': 22.9495, 'eval_samples_per_second': 293.47, 'epoch': 3.33, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+INVSIB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700259c82a904bfdabf8bb1422433358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416c3567c681428c95f8f23cba8f09e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='78000' max='143957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 78000/143957 4:34:24 < 3:52:03, 4.74 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.383299</td>\n",
       "      <td>0.851534</td>\n",
       "      <td>30.641200</td>\n",
       "      <td>197.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.316063</td>\n",
       "      <td>0.887001</td>\n",
       "      <td>30.701100</td>\n",
       "      <td>197.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.338949</td>\n",
       "      <td>0.902507</td>\n",
       "      <td>30.723200</td>\n",
       "      <td>197.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.301207</td>\n",
       "      <td>0.910261</td>\n",
       "      <td>30.745900</td>\n",
       "      <td>197.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.377753</td>\n",
       "      <td>0.900033</td>\n",
       "      <td>30.833600</td>\n",
       "      <td>196.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.317735</td>\n",
       "      <td>0.910426</td>\n",
       "      <td>30.791900</td>\n",
       "      <td>196.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.293654</td>\n",
       "      <td>0.926097</td>\n",
       "      <td>30.820600</td>\n",
       "      <td>196.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.329496</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>30.789300</td>\n",
       "      <td>196.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.315154</td>\n",
       "      <td>0.922963</td>\n",
       "      <td>30.747300</td>\n",
       "      <td>197.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.922963</td>\n",
       "      <td>30.848300</td>\n",
       "      <td>196.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>0.294206</td>\n",
       "      <td>0.932531</td>\n",
       "      <td>30.847900</td>\n",
       "      <td>196.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.287785</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>30.793400</td>\n",
       "      <td>196.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>0.927252</td>\n",
       "      <td>30.855700</td>\n",
       "      <td>196.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.291584</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>30.851600</td>\n",
       "      <td>196.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.295040</td>\n",
       "      <td>0.936490</td>\n",
       "      <td>30.823600</td>\n",
       "      <td>196.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.291778</td>\n",
       "      <td>0.930221</td>\n",
       "      <td>30.876800</td>\n",
       "      <td>196.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>0.933520</td>\n",
       "      <td>31.019800</td>\n",
       "      <td>195.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.281116</td>\n",
       "      <td>0.937809</td>\n",
       "      <td>30.774100</td>\n",
       "      <td>196.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>0.284303</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>30.694200</td>\n",
       "      <td>197.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.290451</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>30.730400</td>\n",
       "      <td>197.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.284573</td>\n",
       "      <td>0.934180</td>\n",
       "      <td>30.590200</td>\n",
       "      <td>198.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.287662</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>30.606100</td>\n",
       "      <td>198.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.267845</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>30.597300</td>\n",
       "      <td>198.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.275531</td>\n",
       "      <td>0.947212</td>\n",
       "      <td>30.662700</td>\n",
       "      <td>197.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.275815</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>30.712300</td>\n",
       "      <td>197.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.278756</td>\n",
       "      <td>0.944573</td>\n",
       "      <td>30.689500</td>\n",
       "      <td>197.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.278806</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>30.716100</td>\n",
       "      <td>197.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.264328</td>\n",
       "      <td>0.950181</td>\n",
       "      <td>30.779400</td>\n",
       "      <td>196.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>0.954965</td>\n",
       "      <td>30.724500</td>\n",
       "      <td>197.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.270241</td>\n",
       "      <td>0.953151</td>\n",
       "      <td>30.711000</td>\n",
       "      <td>197.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.254355</td>\n",
       "      <td>0.949357</td>\n",
       "      <td>30.953300</td>\n",
       "      <td>195.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.264572</td>\n",
       "      <td>0.954635</td>\n",
       "      <td>30.764900</td>\n",
       "      <td>197.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.248106</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>30.704800</td>\n",
       "      <td>197.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.255386</td>\n",
       "      <td>0.954306</td>\n",
       "      <td>30.703800</td>\n",
       "      <td>197.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.249787</td>\n",
       "      <td>0.951006</td>\n",
       "      <td>30.857700</td>\n",
       "      <td>196.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.267284</td>\n",
       "      <td>0.952656</td>\n",
       "      <td>30.610000</td>\n",
       "      <td>198.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.268296</td>\n",
       "      <td>0.953811</td>\n",
       "      <td>30.577500</td>\n",
       "      <td>198.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.254187</td>\n",
       "      <td>0.950511</td>\n",
       "      <td>30.547700</td>\n",
       "      <td>198.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.258635</td>\n",
       "      <td>0.954141</td>\n",
       "      <td>30.654200</td>\n",
       "      <td>197.754000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for pretrained/bert-base-uncased-sst2-ORIG+TextMix\n",
      "{'eval_loss': 3.459319591522217, 'eval_accuracy': 0.9459539717891611, 'eval_f1': 0.9451746127107483, 'eval_precision': 0.9449550204900994, 'eval_recall': 0.9454027160028868, 'eval_runtime': 22.8649, 'eval_samples_per_second': 294.557, 'epoch': 5.42, 'run': 'pretrained/bert-base-uncased-sst2-ORIG+TextMix'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be069b0625344068d05492b9af78ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a835141893c4d3fa34fbf0edacfa3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-80412efd19620de1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='17957' max='215936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17957/215936 1:08:15 < 12:32:43, 4.38 it/s, Epoch 0.83/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.338679</td>\n",
       "      <td>0.717515</td>\n",
       "      <td>45.778700</td>\n",
       "      <td>198.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.307838</td>\n",
       "      <td>0.746250</td>\n",
       "      <td>45.942300</td>\n",
       "      <td>197.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.270661</td>\n",
       "      <td>0.769150</td>\n",
       "      <td>47.939600</td>\n",
       "      <td>189.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>0.784238</td>\n",
       "      <td>46.077300</td>\n",
       "      <td>197.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.253895</td>\n",
       "      <td>0.786836</td>\n",
       "      <td>45.957400</td>\n",
       "      <td>197.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.248588</td>\n",
       "      <td>0.792271</td>\n",
       "      <td>46.031500</td>\n",
       "      <td>197.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.252973</td>\n",
       "      <td>0.797020</td>\n",
       "      <td>46.085100</td>\n",
       "      <td>197.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.240353</td>\n",
       "      <td>0.791805</td>\n",
       "      <td>46.511700</td>\n",
       "      <td>195.499000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_pretrain = False\n",
    "\n",
    "results = []\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    for t in ['ORIG', 'INV', 'SIB', 'INVSIB', 'TextMix', 'SentMix', 'WordMix']: \n",
    "        \n",
    "        soft_target = False\n",
    "        eval_only = False\n",
    "        \n",
    "        checkpoint = 'pretrained/' + MODEL_NAME + \"-sst2-ORIG+\" + t \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        if t == 'ORIG':\n",
    "            train_dataset = load_dataset('glue', 'sst2', split='train[:90%]')\n",
    "            train_dataset.rename_column_('sentence', 'text')\n",
    "        else: \n",
    "            \n",
    "            # load custom data    \n",
    "            text = npy_load(\"./assets/SST2/\" + t + \"/text.npy\")\n",
    "            label = npy_load(\"./assets/SST2/\" + t + \"/label.npy\")\n",
    "            if len(label.shape) > 1:\n",
    "                df = pd.DataFrame({'text': text, 'label': label.tolist()})\n",
    "                df.text = df.text.astype(str)\n",
    "                df.label = df.label.map(lambda y: np.array(y))\n",
    "            else:\n",
    "                df = pd.DataFrame({'text': text, 'label': label})\n",
    "                df.text = df.text.astype(str)\n",
    "                df.label = df.label.astype(object)\n",
    "            train_dataset = Dataset.from_pandas(df) \n",
    "            \n",
    "            # load orig data\n",
    "            orig_dataset = load_dataset('glue', 'sst2', split='train[:90%]')\n",
    "            orig_dataset.remove_columns_(['idx'])\n",
    "            orig_dataset.rename_column_('sentence', 'text')\n",
    "            df = orig_dataset.to_pandas()\n",
    "            df = df[df.columns[::-1]]\n",
    "            df.text = df.text.astype(str)\n",
    "            if len(label.shape) > 1:\n",
    "                df.label = df.label.map(one_hot_encode)\n",
    "            else:\n",
    "                df.label = df.label.astype(object)\n",
    "            orig_dataset = Dataset.from_pandas(df)\n",
    "            \n",
    "            # merge orig + custom data\n",
    "            train_dataset = concatenate_datasets([orig_dataset, train_dataset])\n",
    "            train_dataset.shuffle()\n",
    "            \n",
    "        if use_pretrain and os.path.exists(checkpoint):\n",
    "            print('loading {}...'.format(checkpoint))\n",
    "            MODEL_NAME = checkpoint\n",
    "            eval_only = True\n",
    "            \n",
    "        # split to get train\n",
    "        dataset_dict = train_dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "        test_dataset = load_dataset('glue', 'sst2', split='train[-10%:]')\n",
    "        test_dataset.rename_column_('sentence', 'text')\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "            \n",
    "        train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "        eval_dataset = eval_dataset.map(tokenize, batched=True, batch_size=len(eval_dataset))\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        train_dataset.rename_column_('label', 'labels')\n",
    "        eval_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        if len(np.array(train_dataset['labels']).shape) > 1:\n",
    "            soft_target = True\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 10\n",
    "        gradient_accumulation_steps=1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            seed=1,\n",
    "            # adafactor=True,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            # gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=2000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            # run_name=checkpoint\n",
    "        )\n",
    "\n",
    "        if soft_target:\n",
    "            trainer = Trainer_w_soft_target(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                compute_metrics=compute_metrics_w_soft_target,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                data_collator=DefaultCollator(),\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "            )\n",
    "        else: \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "            )\n",
    "\n",
    "        if not eval_only:\n",
    "            trainer.train()\n",
    "        \n",
    "        trainer.compute_metrics = compute_metrics\n",
    "            \n",
    "        # test ORIG\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        out = trainer.evaluate()\n",
    "        out['run'] = checkpoint\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out))   \n",
    "        \n",
    "        results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_SST2_r3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_SST2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00554915f7ad47b9b80d65294dbfdd37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0768ccf7a2e64f819c7401d7db258d16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14dc24ef59d341a88d1bdb69f3cacde6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "212a6447739d45b5b57e5815ff538f57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "367cfe385d38427dbdf4325a70c0dc2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "46cc6e7671244f1d9167855a45a36af8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1f7506e1a44d109d918b1f16d6bb75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5f641be531341dbb007330b7e245169",
      "max": 1382015,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_00554915f7ad47b9b80d65294dbfdd37",
      "value": 1382015
     }
    },
    "59b96dd300ae4a56b2d58ca0de0bb7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd27134d896448b6b385ce45da7556ee",
       "IPY_MODEL_c92040617a854d8d96cc8ba678f0b271"
      ],
      "layout": "IPY_MODEL_e3c71b92f2a0484bb70123a86f855f9d"
     }
    },
    "7158ce3f315647aba9299519fcec4be7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81821a97e02445539ccabcc2091cba95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b1f7506e1a44d109d918b1f16d6bb75",
       "IPY_MODEL_f177b91513ec4039bc7c6e61d15db9a2"
      ],
      "layout": "IPY_MODEL_212a6447739d45b5b57e5815ff538f57"
     }
    },
    "c92040617a854d8d96cc8ba678f0b271": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46cc6e7671244f1d9167855a45a36af8",
      "placeholder": "",
      "style": "IPY_MODEL_14dc24ef59d341a88d1bdb69f3cacde6",
      "value": " 798k/798k [00:00&lt;00:00, 1.99MB/s]"
     }
    },
    "d5f641be531341dbb007330b7e245169": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd27134d896448b6b385ce45da7556ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f86a86b6b9c64cc989e89b27693d8a2f",
      "max": 798011,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_367cfe385d38427dbdf4325a70c0dc2e",
      "value": 798011
     }
    },
    "e3c71b92f2a0484bb70123a86f855f9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f177b91513ec4039bc7c6e61d15db9a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0768ccf7a2e64f819c7401d7db258d16",
      "placeholder": "",
      "style": "IPY_MODEL_7158ce3f315647aba9299519fcec4be7",
      "value": " 1.38M/1.38M [00:00&lt;00:00, 5.04MB/s]"
     }
    },
    "f86a86b6b9c64cc989e89b27693d8a2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
