{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_core_web_sm' (2.3.1) requires spaCy >=2.3.0,<2.4.0 and is incompatible with the current version (3.0.3). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }        \n",
    "        \n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, tokenize_fn, transform, target_pairs=[], target_prob=1.0, num_classes=4):\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        batch = self.transform(\n",
    "            batch, \n",
    "            self.target_pairs,   \n",
    "            self.target_prob,\n",
    "            self.num_classes\n",
    "        )\n",
    "        text, labels = batch\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = torch.tensor(labels)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "ts = [TextMix(), SentMix(), WordMix()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "<ipython-input-5-bb65d84970cc>:23: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  test_dataset.rename_column_('label', 'labels')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7312ec28aa49a6944be563825085a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='38000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38000/142500 4:13:42 < 11:37:43, 2.50 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.182700</td>\n",
       "      <td>0.833958</td>\n",
       "      <td>0.652891</td>\n",
       "      <td>69.216200</td>\n",
       "      <td>86.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.814400</td>\n",
       "      <td>0.784703</td>\n",
       "      <td>0.672584</td>\n",
       "      <td>69.486700</td>\n",
       "      <td>86.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>0.782599</td>\n",
       "      <td>0.670510</td>\n",
       "      <td>68.729100</td>\n",
       "      <td>87.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.758900</td>\n",
       "      <td>0.772537</td>\n",
       "      <td>0.701771</td>\n",
       "      <td>69.536800</td>\n",
       "      <td>86.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.752500</td>\n",
       "      <td>0.756096</td>\n",
       "      <td>0.702339</td>\n",
       "      <td>69.648800</td>\n",
       "      <td>86.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.763436</td>\n",
       "      <td>0.746416</td>\n",
       "      <td>69.327000</td>\n",
       "      <td>86.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.759252</td>\n",
       "      <td>0.735518</td>\n",
       "      <td>70.125800</td>\n",
       "      <td>85.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.806503</td>\n",
       "      <td>0.687324</td>\n",
       "      <td>69.312400</td>\n",
       "      <td>86.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.767603</td>\n",
       "      <td>0.762755</td>\n",
       "      <td>69.451300</td>\n",
       "      <td>86.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.749025</td>\n",
       "      <td>0.737470</td>\n",
       "      <td>69.304200</td>\n",
       "      <td>86.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.740663</td>\n",
       "      <td>0.781716</td>\n",
       "      <td>69.887300</td>\n",
       "      <td>85.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.755700</td>\n",
       "      <td>0.761177</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>69.150400</td>\n",
       "      <td>86.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.763500</td>\n",
       "      <td>0.765434</td>\n",
       "      <td>0.748573</td>\n",
       "      <td>69.621300</td>\n",
       "      <td>86.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>0.744455</td>\n",
       "      <td>0.710001</td>\n",
       "      <td>69.354100</td>\n",
       "      <td>86.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>0.824773</td>\n",
       "      <td>0.673190</td>\n",
       "      <td>69.187900</td>\n",
       "      <td>86.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.767447</td>\n",
       "      <td>69.045100</td>\n",
       "      <td>86.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.731700</td>\n",
       "      <td>0.753338</td>\n",
       "      <td>0.802720</td>\n",
       "      <td>69.303400</td>\n",
       "      <td>86.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.734444</td>\n",
       "      <td>0.783761</td>\n",
       "      <td>69.073000</td>\n",
       "      <td>86.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.734600</td>\n",
       "      <td>0.746535</td>\n",
       "      <td>0.794171</td>\n",
       "      <td>68.748900</td>\n",
       "      <td>87.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.719800</td>\n",
       "      <td>0.750302</td>\n",
       "      <td>0.816228</td>\n",
       "      <td>69.267300</td>\n",
       "      <td>86.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.725300</td>\n",
       "      <td>0.750813</td>\n",
       "      <td>0.779301</td>\n",
       "      <td>68.976800</td>\n",
       "      <td>86.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>0.748872</td>\n",
       "      <td>0.753287</td>\n",
       "      <td>65.698100</td>\n",
       "      <td>91.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.721400</td>\n",
       "      <td>0.745315</td>\n",
       "      <td>0.797598</td>\n",
       "      <td>69.581200</td>\n",
       "      <td>86.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.737559</td>\n",
       "      <td>0.786801</td>\n",
       "      <td>68.749900</td>\n",
       "      <td>87.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.713600</td>\n",
       "      <td>0.753267</td>\n",
       "      <td>0.802480</td>\n",
       "      <td>69.386000</td>\n",
       "      <td>86.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.735799</td>\n",
       "      <td>0.785161</td>\n",
       "      <td>68.953000</td>\n",
       "      <td>87.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.810435</td>\n",
       "      <td>0.757786</td>\n",
       "      <td>68.524800</td>\n",
       "      <td>87.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>0.836309</td>\n",
       "      <td>69.589400</td>\n",
       "      <td>86.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>0.766472</td>\n",
       "      <td>0.802283</td>\n",
       "      <td>68.481100</td>\n",
       "      <td>87.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.731103</td>\n",
       "      <td>0.818704</td>\n",
       "      <td>68.095400</td>\n",
       "      <td>88.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.765756</td>\n",
       "      <td>0.784130</td>\n",
       "      <td>69.167900</td>\n",
       "      <td>86.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.757164</td>\n",
       "      <td>0.755421</td>\n",
       "      <td>68.951300</td>\n",
       "      <td>87.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.739513</td>\n",
       "      <td>0.798380</td>\n",
       "      <td>69.063400</td>\n",
       "      <td>86.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.761022</td>\n",
       "      <td>0.815191</td>\n",
       "      <td>68.575400</td>\n",
       "      <td>87.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.738505</td>\n",
       "      <td>0.832542</td>\n",
       "      <td>68.657200</td>\n",
       "      <td>87.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.771789</td>\n",
       "      <td>0.766288</td>\n",
       "      <td>68.852600</td>\n",
       "      <td>87.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.781545</td>\n",
       "      <td>0.807874</td>\n",
       "      <td>69.310400</td>\n",
       "      <td>86.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.780605</td>\n",
       "      <td>0.817240</td>\n",
       "      <td>69.096900</td>\n",
       "      <td>86.835000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
      "{'eval_loss': 24.60407829284668, 'eval_accuracy': 0.9381578947368421, 'eval_f1': 0.9381579117190124, 'eval_precision': 0.9383738209275129, 'eval_recall': 0.9381578947368422, 'eval_runtime': 127.4248, 'eval_samples_per_second': 59.643, 'epoch': 2.67, 'run': './results/bert-base-uncased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edbcf35e0d54981b63dbf2c1cc32cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='22000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22000/142500 2:25:09 < 13:15:10, 2.53 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.174600</td>\n",
       "      <td>0.863394</td>\n",
       "      <td>0.660165</td>\n",
       "      <td>66.879500</td>\n",
       "      <td>89.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.823462</td>\n",
       "      <td>0.623097</td>\n",
       "      <td>66.578800</td>\n",
       "      <td>90.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>0.781701</td>\n",
       "      <td>0.648245</td>\n",
       "      <td>66.959400</td>\n",
       "      <td>89.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>0.829549</td>\n",
       "      <td>0.658423</td>\n",
       "      <td>66.259300</td>\n",
       "      <td>90.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.787813</td>\n",
       "      <td>0.641309</td>\n",
       "      <td>67.245500</td>\n",
       "      <td>89.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.792298</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>65.835400</td>\n",
       "      <td>91.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.823656</td>\n",
       "      <td>0.591991</td>\n",
       "      <td>66.763500</td>\n",
       "      <td>89.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.745800</td>\n",
       "      <td>0.792736</td>\n",
       "      <td>0.702534</td>\n",
       "      <td>66.450500</td>\n",
       "      <td>90.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.743700</td>\n",
       "      <td>0.794693</td>\n",
       "      <td>0.671797</td>\n",
       "      <td>66.199600</td>\n",
       "      <td>90.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.680735</td>\n",
       "      <td>66.513800</td>\n",
       "      <td>90.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.759291</td>\n",
       "      <td>0.637749</td>\n",
       "      <td>66.526000</td>\n",
       "      <td>90.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.732445</td>\n",
       "      <td>66.594200</td>\n",
       "      <td>90.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.756700</td>\n",
       "      <td>0.788694</td>\n",
       "      <td>0.622664</td>\n",
       "      <td>66.274300</td>\n",
       "      <td>90.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.759100</td>\n",
       "      <td>0.750208</td>\n",
       "      <td>0.726524</td>\n",
       "      <td>65.988500</td>\n",
       "      <td>90.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>0.777744</td>\n",
       "      <td>0.691447</td>\n",
       "      <td>66.555600</td>\n",
       "      <td>90.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.780972</td>\n",
       "      <td>0.677755</td>\n",
       "      <td>66.753600</td>\n",
       "      <td>89.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.720400</td>\n",
       "      <td>0.788301</td>\n",
       "      <td>0.693352</td>\n",
       "      <td>66.704500</td>\n",
       "      <td>89.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>0.838608</td>\n",
       "      <td>0.730731</td>\n",
       "      <td>67.045000</td>\n",
       "      <td>89.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.704010</td>\n",
       "      <td>67.054200</td>\n",
       "      <td>89.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.779224</td>\n",
       "      <td>0.714544</td>\n",
       "      <td>65.900000</td>\n",
       "      <td>91.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.729900</td>\n",
       "      <td>0.779633</td>\n",
       "      <td>0.723884</td>\n",
       "      <td>66.490700</td>\n",
       "      <td>90.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.713400</td>\n",
       "      <td>0.761829</td>\n",
       "      <td>0.727065</td>\n",
       "      <td>66.941300</td>\n",
       "      <td>89.631000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-SentMix\n",
      "{'eval_loss': 23.754152297973633, 'eval_accuracy': 0.9301315789473684, 'eval_f1': 0.9300557925165926, 'eval_precision': 0.9300518992798888, 'eval_recall': 0.9301315789473684, 'eval_runtime': 127.3892, 'eval_samples_per_second': 59.66, 'epoch': 1.54, 'run': './results/bert-base-uncased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b65760100b4489a21411b9e31387b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='51000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 51000/142500 5:40:43 < 10:11:18, 2.49 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.151900</td>\n",
       "      <td>0.914932</td>\n",
       "      <td>0.620536</td>\n",
       "      <td>70.965700</td>\n",
       "      <td>84.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.869400</td>\n",
       "      <td>0.872325</td>\n",
       "      <td>0.589514</td>\n",
       "      <td>70.908700</td>\n",
       "      <td>84.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.821688</td>\n",
       "      <td>0.617198</td>\n",
       "      <td>71.544600</td>\n",
       "      <td>83.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.614437</td>\n",
       "      <td>65.033000</td>\n",
       "      <td>92.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.570331</td>\n",
       "      <td>71.503300</td>\n",
       "      <td>83.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.800716</td>\n",
       "      <td>0.621906</td>\n",
       "      <td>71.485200</td>\n",
       "      <td>83.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>0.801144</td>\n",
       "      <td>0.609909</td>\n",
       "      <td>69.333100</td>\n",
       "      <td>86.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>0.837964</td>\n",
       "      <td>0.613703</td>\n",
       "      <td>67.049800</td>\n",
       "      <td>89.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.796257</td>\n",
       "      <td>0.632827</td>\n",
       "      <td>66.344800</td>\n",
       "      <td>90.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.817158</td>\n",
       "      <td>0.590539</td>\n",
       "      <td>64.584900</td>\n",
       "      <td>92.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>0.855845</td>\n",
       "      <td>0.556020</td>\n",
       "      <td>64.481600</td>\n",
       "      <td>93.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.803160</td>\n",
       "      <td>0.608164</td>\n",
       "      <td>68.218300</td>\n",
       "      <td>87.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>0.816031</td>\n",
       "      <td>0.601729</td>\n",
       "      <td>65.590500</td>\n",
       "      <td>91.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.842298</td>\n",
       "      <td>0.605233</td>\n",
       "      <td>66.333600</td>\n",
       "      <td>90.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.780800</td>\n",
       "      <td>0.847776</td>\n",
       "      <td>0.587964</td>\n",
       "      <td>65.254400</td>\n",
       "      <td>91.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.847791</td>\n",
       "      <td>0.592475</td>\n",
       "      <td>65.451900</td>\n",
       "      <td>91.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.769900</td>\n",
       "      <td>0.800477</td>\n",
       "      <td>0.597655</td>\n",
       "      <td>65.344900</td>\n",
       "      <td>91.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.775900</td>\n",
       "      <td>0.796210</td>\n",
       "      <td>0.630626</td>\n",
       "      <td>65.905200</td>\n",
       "      <td>91.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>0.801142</td>\n",
       "      <td>0.650801</td>\n",
       "      <td>65.945000</td>\n",
       "      <td>90.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.775600</td>\n",
       "      <td>0.799259</td>\n",
       "      <td>0.622138</td>\n",
       "      <td>71.866100</td>\n",
       "      <td>83.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.807252</td>\n",
       "      <td>0.602635</td>\n",
       "      <td>72.334800</td>\n",
       "      <td>82.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.769300</td>\n",
       "      <td>0.807796</td>\n",
       "      <td>0.572095</td>\n",
       "      <td>72.061100</td>\n",
       "      <td>83.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.779759</td>\n",
       "      <td>0.615726</td>\n",
       "      <td>72.618600</td>\n",
       "      <td>82.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.780456</td>\n",
       "      <td>0.605745</td>\n",
       "      <td>71.460500</td>\n",
       "      <td>83.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.780400</td>\n",
       "      <td>0.824933</td>\n",
       "      <td>0.642847</td>\n",
       "      <td>71.952200</td>\n",
       "      <td>83.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.797687</td>\n",
       "      <td>0.631703</td>\n",
       "      <td>72.265200</td>\n",
       "      <td>83.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.778155</td>\n",
       "      <td>0.653478</td>\n",
       "      <td>71.998000</td>\n",
       "      <td>83.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.757800</td>\n",
       "      <td>0.793692</td>\n",
       "      <td>0.630033</td>\n",
       "      <td>71.808000</td>\n",
       "      <td>83.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>0.765489</td>\n",
       "      <td>0.636151</td>\n",
       "      <td>72.415300</td>\n",
       "      <td>82.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.797396</td>\n",
       "      <td>0.652371</td>\n",
       "      <td>71.228700</td>\n",
       "      <td>84.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>0.796098</td>\n",
       "      <td>0.604795</td>\n",
       "      <td>72.587400</td>\n",
       "      <td>82.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.743600</td>\n",
       "      <td>0.814822</td>\n",
       "      <td>0.628280</td>\n",
       "      <td>71.482800</td>\n",
       "      <td>83.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>0.811289</td>\n",
       "      <td>0.650557</td>\n",
       "      <td>71.779700</td>\n",
       "      <td>83.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.732600</td>\n",
       "      <td>0.807186</td>\n",
       "      <td>0.648312</td>\n",
       "      <td>72.152900</td>\n",
       "      <td>83.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>0.784259</td>\n",
       "      <td>0.663678</td>\n",
       "      <td>72.635800</td>\n",
       "      <td>82.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>0.797081</td>\n",
       "      <td>0.666261</td>\n",
       "      <td>72.584600</td>\n",
       "      <td>82.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.732700</td>\n",
       "      <td>0.787113</td>\n",
       "      <td>0.643609</td>\n",
       "      <td>71.998500</td>\n",
       "      <td>83.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.773456</td>\n",
       "      <td>0.605751</td>\n",
       "      <td>72.236300</td>\n",
       "      <td>83.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.729300</td>\n",
       "      <td>0.765270</td>\n",
       "      <td>0.659487</td>\n",
       "      <td>72.678800</td>\n",
       "      <td>82.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.718800</td>\n",
       "      <td>0.785784</td>\n",
       "      <td>0.659433</td>\n",
       "      <td>72.677500</td>\n",
       "      <td>82.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.740100</td>\n",
       "      <td>0.773786</td>\n",
       "      <td>0.675072</td>\n",
       "      <td>72.388400</td>\n",
       "      <td>82.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.757590</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>72.021800</td>\n",
       "      <td>83.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.725100</td>\n",
       "      <td>0.803340</td>\n",
       "      <td>0.652935</td>\n",
       "      <td>72.508100</td>\n",
       "      <td>82.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.787768</td>\n",
       "      <td>0.633542</td>\n",
       "      <td>71.714500</td>\n",
       "      <td>83.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.786597</td>\n",
       "      <td>0.654607</td>\n",
       "      <td>72.475900</td>\n",
       "      <td>82.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.715500</td>\n",
       "      <td>0.789780</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>72.261800</td>\n",
       "      <td>83.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.791844</td>\n",
       "      <td>0.615170</td>\n",
       "      <td>72.117400</td>\n",
       "      <td>83.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>0.658797</td>\n",
       "      <td>71.990700</td>\n",
       "      <td>83.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.769744</td>\n",
       "      <td>0.602639</td>\n",
       "      <td>71.320400</td>\n",
       "      <td>84.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.756947</td>\n",
       "      <td>0.671089</td>\n",
       "      <td>72.197300</td>\n",
       "      <td>83.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.785060</td>\n",
       "      <td>0.672486</td>\n",
       "      <td>71.265100</td>\n",
       "      <td>84.193000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 1], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 0]]\n",
      "New targets: [[0, 1], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-WordMix\n",
      "{'eval_loss': 21.714515686035156, 'eval_accuracy': 0.9153947368421053, 'eval_f1': 0.9153811071568305, 'eval_precision': 0.9160972740976473, 'eval_recall': 0.9153947368421053, 'eval_runtime': 128.1311, 'eval_samples_per_second': 59.314, 'epoch': 3.58, 'run': './results/bert-base-uncased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cd823b3fe446edb1539b5c210ce1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='46000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 46000/142500 5:10:56 < 10:52:20, 2.47 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.134300</td>\n",
       "      <td>0.765374</td>\n",
       "      <td>0.716245</td>\n",
       "      <td>68.915300</td>\n",
       "      <td>87.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.776800</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.680535</td>\n",
       "      <td>68.401500</td>\n",
       "      <td>87.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.778200</td>\n",
       "      <td>0.805896</td>\n",
       "      <td>0.670010</td>\n",
       "      <td>68.864400</td>\n",
       "      <td>87.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.776700</td>\n",
       "      <td>0.800256</td>\n",
       "      <td>0.706893</td>\n",
       "      <td>68.109700</td>\n",
       "      <td>88.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>0.770295</td>\n",
       "      <td>0.704319</td>\n",
       "      <td>68.368900</td>\n",
       "      <td>87.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.814719</td>\n",
       "      <td>0.677127</td>\n",
       "      <td>69.038100</td>\n",
       "      <td>86.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.748191</td>\n",
       "      <td>0.758278</td>\n",
       "      <td>68.718100</td>\n",
       "      <td>87.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.753700</td>\n",
       "      <td>0.774299</td>\n",
       "      <td>0.716635</td>\n",
       "      <td>68.858000</td>\n",
       "      <td>87.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.771200</td>\n",
       "      <td>0.765151</td>\n",
       "      <td>0.771367</td>\n",
       "      <td>67.836400</td>\n",
       "      <td>88.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.768043</td>\n",
       "      <td>0.678310</td>\n",
       "      <td>68.126000</td>\n",
       "      <td>88.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.753800</td>\n",
       "      <td>0.781148</td>\n",
       "      <td>0.734343</td>\n",
       "      <td>68.534100</td>\n",
       "      <td>87.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.862173</td>\n",
       "      <td>0.700133</td>\n",
       "      <td>68.638900</td>\n",
       "      <td>87.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.768200</td>\n",
       "      <td>0.787472</td>\n",
       "      <td>0.735416</td>\n",
       "      <td>68.425600</td>\n",
       "      <td>87.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.763600</td>\n",
       "      <td>0.773286</td>\n",
       "      <td>0.714473</td>\n",
       "      <td>68.361200</td>\n",
       "      <td>87.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>0.843030</td>\n",
       "      <td>0.753068</td>\n",
       "      <td>67.939500</td>\n",
       "      <td>88.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.769900</td>\n",
       "      <td>0.802608</td>\n",
       "      <td>0.666911</td>\n",
       "      <td>67.797400</td>\n",
       "      <td>88.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.734207</td>\n",
       "      <td>0.762708</td>\n",
       "      <td>67.884900</td>\n",
       "      <td>88.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.770196</td>\n",
       "      <td>0.772297</td>\n",
       "      <td>67.379900</td>\n",
       "      <td>89.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.745462</td>\n",
       "      <td>0.776070</td>\n",
       "      <td>68.810100</td>\n",
       "      <td>87.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.725600</td>\n",
       "      <td>0.786454</td>\n",
       "      <td>0.744311</td>\n",
       "      <td>68.298100</td>\n",
       "      <td>87.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.757193</td>\n",
       "      <td>0.704947</td>\n",
       "      <td>68.108100</td>\n",
       "      <td>88.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.730500</td>\n",
       "      <td>0.766693</td>\n",
       "      <td>0.767671</td>\n",
       "      <td>67.882500</td>\n",
       "      <td>88.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.759175</td>\n",
       "      <td>0.693215</td>\n",
       "      <td>67.949400</td>\n",
       "      <td>88.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.759801</td>\n",
       "      <td>0.784014</td>\n",
       "      <td>68.460000</td>\n",
       "      <td>87.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.740100</td>\n",
       "      <td>0.737081</td>\n",
       "      <td>0.763012</td>\n",
       "      <td>68.324500</td>\n",
       "      <td>87.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.733600</td>\n",
       "      <td>0.749286</td>\n",
       "      <td>0.762624</td>\n",
       "      <td>67.369000</td>\n",
       "      <td>89.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.747760</td>\n",
       "      <td>0.771138</td>\n",
       "      <td>68.648800</td>\n",
       "      <td>87.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.718900</td>\n",
       "      <td>0.751744</td>\n",
       "      <td>0.756749</td>\n",
       "      <td>68.689100</td>\n",
       "      <td>87.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.775712</td>\n",
       "      <td>0.813136</td>\n",
       "      <td>68.790100</td>\n",
       "      <td>87.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>0.762410</td>\n",
       "      <td>0.797005</td>\n",
       "      <td>68.296400</td>\n",
       "      <td>87.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.715700</td>\n",
       "      <td>0.753982</td>\n",
       "      <td>0.805728</td>\n",
       "      <td>69.185900</td>\n",
       "      <td>86.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.729396</td>\n",
       "      <td>0.797397</td>\n",
       "      <td>68.283300</td>\n",
       "      <td>87.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.736920</td>\n",
       "      <td>0.827299</td>\n",
       "      <td>68.109100</td>\n",
       "      <td>88.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.749644</td>\n",
       "      <td>0.820988</td>\n",
       "      <td>68.493500</td>\n",
       "      <td>87.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.740176</td>\n",
       "      <td>0.811171</td>\n",
       "      <td>68.187300</td>\n",
       "      <td>87.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.748025</td>\n",
       "      <td>0.844907</td>\n",
       "      <td>68.508500</td>\n",
       "      <td>87.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>0.729401</td>\n",
       "      <td>0.770451</td>\n",
       "      <td>68.490500</td>\n",
       "      <td>87.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.699800</td>\n",
       "      <td>0.732752</td>\n",
       "      <td>0.766973</td>\n",
       "      <td>69.163100</td>\n",
       "      <td>86.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.798847</td>\n",
       "      <td>68.207100</td>\n",
       "      <td>87.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.737281</td>\n",
       "      <td>0.802331</td>\n",
       "      <td>68.764000</td>\n",
       "      <td>87.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.756005</td>\n",
       "      <td>0.827321</td>\n",
       "      <td>68.175900</td>\n",
       "      <td>88.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.709300</td>\n",
       "      <td>0.759010</td>\n",
       "      <td>0.795028</td>\n",
       "      <td>68.241800</td>\n",
       "      <td>87.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.758323</td>\n",
       "      <td>0.812829</td>\n",
       "      <td>68.279600</td>\n",
       "      <td>87.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.768826</td>\n",
       "      <td>0.826623</td>\n",
       "      <td>67.741600</td>\n",
       "      <td>88.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.753860</td>\n",
       "      <td>0.831530</td>\n",
       "      <td>68.591700</td>\n",
       "      <td>87.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.771229</td>\n",
       "      <td>0.825232</td>\n",
       "      <td>69.276900</td>\n",
       "      <td>86.609000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 18:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-TextMix\n",
      "{'eval_loss': 24.987064361572266, 'eval_accuracy': 0.9331578947368421, 'eval_f1': 0.9329715926506041, 'eval_precision': 0.9334148742540789, 'eval_recall': 0.9331578947368422, 'eval_runtime': 1084.046, 'eval_samples_per_second': 7.011, 'epoch': 3.23, 'run': './results/roberta-base-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51be61d4fd784679ba051a27ff1a67ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='13000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13000/142500 1:27:32 < 14:32:07, 2.47 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.100700</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>0.696285</td>\n",
       "      <td>67.107700</td>\n",
       "      <td>89.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.805880</td>\n",
       "      <td>0.705148</td>\n",
       "      <td>67.256900</td>\n",
       "      <td>89.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.787899</td>\n",
       "      <td>0.729695</td>\n",
       "      <td>67.497800</td>\n",
       "      <td>88.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.766300</td>\n",
       "      <td>0.783725</td>\n",
       "      <td>0.674652</td>\n",
       "      <td>67.754500</td>\n",
       "      <td>88.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.768400</td>\n",
       "      <td>0.763079</td>\n",
       "      <td>0.647183</td>\n",
       "      <td>67.325900</td>\n",
       "      <td>89.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>0.797541</td>\n",
       "      <td>0.648583</td>\n",
       "      <td>67.211600</td>\n",
       "      <td>89.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.794311</td>\n",
       "      <td>0.716171</td>\n",
       "      <td>67.966600</td>\n",
       "      <td>88.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>0.772531</td>\n",
       "      <td>0.671297</td>\n",
       "      <td>67.317200</td>\n",
       "      <td>89.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.819323</td>\n",
       "      <td>0.662982</td>\n",
       "      <td>67.049900</td>\n",
       "      <td>89.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.743800</td>\n",
       "      <td>0.821880</td>\n",
       "      <td>0.599392</td>\n",
       "      <td>67.413600</td>\n",
       "      <td>89.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.766977</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>67.907300</td>\n",
       "      <td>88.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.776900</td>\n",
       "      <td>0.767915</td>\n",
       "      <td>0.664021</td>\n",
       "      <td>67.898200</td>\n",
       "      <td>88.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.762900</td>\n",
       "      <td>0.808395</td>\n",
       "      <td>0.651666</td>\n",
       "      <td>67.811900</td>\n",
       "      <td>88.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-SentMix\n",
      "{'eval_loss': 22.001941680908203, 'eval_accuracy': 0.9157894736842105, 'eval_f1': 0.9155353226859555, 'eval_precision': 0.9155848007271494, 'eval_recall': 0.9157894736842105, 'eval_runtime': 128.1835, 'eval_samples_per_second': 59.29, 'epoch': 0.91, 'run': './results/roberta-base-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4f30086c6a4a5d9f6d847998e10c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='30000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30000/142500 3:24:02 < 12:45:11, 2.45 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.847293</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>69.724200</td>\n",
       "      <td>86.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>0.848029</td>\n",
       "      <td>0.575985</td>\n",
       "      <td>69.519700</td>\n",
       "      <td>86.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.846949</td>\n",
       "      <td>0.584822</td>\n",
       "      <td>68.824800</td>\n",
       "      <td>87.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.853230</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>68.724400</td>\n",
       "      <td>87.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.837976</td>\n",
       "      <td>0.573284</td>\n",
       "      <td>69.228500</td>\n",
       "      <td>86.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.816400</td>\n",
       "      <td>0.834973</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>69.065600</td>\n",
       "      <td>86.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.845411</td>\n",
       "      <td>0.605646</td>\n",
       "      <td>69.186400</td>\n",
       "      <td>86.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.802400</td>\n",
       "      <td>0.819453</td>\n",
       "      <td>0.601753</td>\n",
       "      <td>69.511700</td>\n",
       "      <td>86.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.872504</td>\n",
       "      <td>0.560564</td>\n",
       "      <td>68.770800</td>\n",
       "      <td>87.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.846931</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>69.308700</td>\n",
       "      <td>86.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.802813</td>\n",
       "      <td>0.609210</td>\n",
       "      <td>68.958600</td>\n",
       "      <td>87.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.849272</td>\n",
       "      <td>0.542674</td>\n",
       "      <td>69.191300</td>\n",
       "      <td>86.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.603862</td>\n",
       "      <td>69.075400</td>\n",
       "      <td>86.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.859361</td>\n",
       "      <td>0.543694</td>\n",
       "      <td>69.367000</td>\n",
       "      <td>86.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>0.845443</td>\n",
       "      <td>0.582735</td>\n",
       "      <td>69.198900</td>\n",
       "      <td>86.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.795300</td>\n",
       "      <td>0.828095</td>\n",
       "      <td>0.560836</td>\n",
       "      <td>69.618600</td>\n",
       "      <td>86.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.816691</td>\n",
       "      <td>0.632878</td>\n",
       "      <td>68.705000</td>\n",
       "      <td>87.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.816227</td>\n",
       "      <td>0.615825</td>\n",
       "      <td>69.209200</td>\n",
       "      <td>86.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.821641</td>\n",
       "      <td>0.607703</td>\n",
       "      <td>69.378700</td>\n",
       "      <td>86.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.795754</td>\n",
       "      <td>0.645693</td>\n",
       "      <td>69.159800</td>\n",
       "      <td>86.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.796120</td>\n",
       "      <td>0.608726</td>\n",
       "      <td>68.891500</td>\n",
       "      <td>87.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.813858</td>\n",
       "      <td>0.630459</td>\n",
       "      <td>69.360200</td>\n",
       "      <td>86.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.822095</td>\n",
       "      <td>0.613693</td>\n",
       "      <td>69.189400</td>\n",
       "      <td>86.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.826952</td>\n",
       "      <td>0.575422</td>\n",
       "      <td>69.132400</td>\n",
       "      <td>86.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.826095</td>\n",
       "      <td>0.559502</td>\n",
       "      <td>69.133200</td>\n",
       "      <td>86.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.786600</td>\n",
       "      <td>0.845342</td>\n",
       "      <td>0.608999</td>\n",
       "      <td>68.972800</td>\n",
       "      <td>86.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.786600</td>\n",
       "      <td>0.787284</td>\n",
       "      <td>0.642230</td>\n",
       "      <td>68.694900</td>\n",
       "      <td>87.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.789800</td>\n",
       "      <td>0.815498</td>\n",
       "      <td>0.625763</td>\n",
       "      <td>69.078300</td>\n",
       "      <td>86.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.777700</td>\n",
       "      <td>0.828772</td>\n",
       "      <td>0.600986</td>\n",
       "      <td>68.737500</td>\n",
       "      <td>87.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>0.845209</td>\n",
       "      <td>0.627416</td>\n",
       "      <td>69.177400</td>\n",
       "      <td>86.734000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 0]]\n",
      "New targets: [[0, 1], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 02:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-WordMix\n",
      "{'eval_loss': 18.879497528076172, 'eval_accuracy': 0.8980263157894737, 'eval_f1': 0.8980891878968906, 'eval_precision': 0.8988408262938834, 'eval_recall': 0.8980263157894737, 'eval_runtime': 128.3595, 'eval_samples_per_second': 59.209, 'epoch': 2.11, 'run': './results/roberta-base-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26eda7e40544a699d42000516e4f0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='23000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23000/142500 3:54:45 < 20:19:47, 1.63 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.099900</td>\n",
       "      <td>0.787001</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>156.414800</td>\n",
       "      <td>38.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.792994</td>\n",
       "      <td>0.616497</td>\n",
       "      <td>157.578300</td>\n",
       "      <td>38.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.772943</td>\n",
       "      <td>0.696355</td>\n",
       "      <td>140.272200</td>\n",
       "      <td>42.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.777160</td>\n",
       "      <td>0.674428</td>\n",
       "      <td>144.282500</td>\n",
       "      <td>41.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>0.775821</td>\n",
       "      <td>0.632179</td>\n",
       "      <td>142.578500</td>\n",
       "      <td>42.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.780183</td>\n",
       "      <td>0.685985</td>\n",
       "      <td>142.348000</td>\n",
       "      <td>42.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.750400</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>0.712454</td>\n",
       "      <td>142.695200</td>\n",
       "      <td>42.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.748100</td>\n",
       "      <td>0.771263</td>\n",
       "      <td>0.670011</td>\n",
       "      <td>141.149300</td>\n",
       "      <td>42.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.740800</td>\n",
       "      <td>0.797307</td>\n",
       "      <td>0.725105</td>\n",
       "      <td>142.259000</td>\n",
       "      <td>42.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.781440</td>\n",
       "      <td>0.734887</td>\n",
       "      <td>142.227800</td>\n",
       "      <td>42.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.748500</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.686291</td>\n",
       "      <td>142.061000</td>\n",
       "      <td>42.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.795511</td>\n",
       "      <td>0.659866</td>\n",
       "      <td>143.808500</td>\n",
       "      <td>41.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>0.796717</td>\n",
       "      <td>0.742553</td>\n",
       "      <td>143.318200</td>\n",
       "      <td>41.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>0.751871</td>\n",
       "      <td>0.740697</td>\n",
       "      <td>140.893800</td>\n",
       "      <td>42.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.840607</td>\n",
       "      <td>0.707985</td>\n",
       "      <td>142.234400</td>\n",
       "      <td>42.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>0.744784</td>\n",
       "      <td>0.736961</td>\n",
       "      <td>140.504100</td>\n",
       "      <td>42.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.735700</td>\n",
       "      <td>0.760210</td>\n",
       "      <td>0.677291</td>\n",
       "      <td>140.837100</td>\n",
       "      <td>42.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.808692</td>\n",
       "      <td>0.719611</td>\n",
       "      <td>143.389400</td>\n",
       "      <td>41.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.729800</td>\n",
       "      <td>0.799997</td>\n",
       "      <td>0.692945</td>\n",
       "      <td>141.340100</td>\n",
       "      <td>42.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.779946</td>\n",
       "      <td>0.664617</td>\n",
       "      <td>141.423500</td>\n",
       "      <td>42.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.757679</td>\n",
       "      <td>0.713965</td>\n",
       "      <td>142.891900</td>\n",
       "      <td>41.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.805676</td>\n",
       "      <td>0.730077</td>\n",
       "      <td>138.891800</td>\n",
       "      <td>43.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.789075</td>\n",
       "      <td>0.727252</td>\n",
       "      <td>142.735000</td>\n",
       "      <td>42.036000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 04:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/xlnet-base-cased-targeted-TextMix\n",
      "{'eval_loss': 26.102365493774414, 'eval_accuracy': 0.9273684210526316, 'eval_f1': 0.9271413058476453, 'eval_precision': 0.9272047321159348, 'eval_recall': 0.9273684210526316, 'eval_runtime': 265.1047, 'eval_samples_per_second': 28.668, 'epoch': 1.61, 'run': './results/xlnet-base-cased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402840c172a94d4baab51d08f9e06228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='35' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    35/142500 00:13 < 16:10:45, 2.45 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 6.00 GiB total capacity; 4.18 GiB already allocated; 18.44 MiB free; 4.56 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bb65d84970cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# test with ORIG data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1248\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1c4b6ee207a1>\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCEwST_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1517\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m   1518\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutput_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m             outputs = layer_module(\n\u001b[0m\u001b[0;32m   1245\u001b[0m                 \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m                 \u001b[0moutput_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             )\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0moutput_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Add again attentions if there are there\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1785\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mff_chunk\u001b[1;34m(self, output_x)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mgelu\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 6.00 GiB total capacity; 4.18 GiB already allocated; 18.44 MiB free; 4.56 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "        \n",
    "    for t in ts: \n",
    "        \n",
    "        t_str = t.__class__.__name__\n",
    "        checkpoint = './results/' + MODEL_NAME + '-targeted-' + t_str\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4).to(device)\n",
    "\n",
    "        dataset = load_dataset('ag_news', split='train') \n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('ag_news', split='test') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 10\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "        tmcb = TargetedMixturesCallback(\n",
    "            dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "            device=device\n",
    "        )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "        tmc = TargetedMixturesCollator(\n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=t,\n",
    "            target_prob=0.5\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            seed=1,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=1000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics_w_soft_target,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=tmc,\n",
    "            callbacks=[tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.compute_metrics = compute_metrics\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_AG_NEWS_targeted_r1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
    "# {'eval_loss': 31.2364559173584, 'eval_accuracy': 0.9381578947368421, 'eval_f1': 0.9381945850526017, 'eval_precision': 0.938240633851668, 'eval_recall': 0.9381578947368421, 'eval_runtime': 117.2622, 'eval_samples_per_second': 64.812, 'epoch': 5.0, 'run': 'TextMix', 'test': 'ORIG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
