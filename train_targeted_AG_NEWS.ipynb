{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted SIB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import TextMix, SentMix, WordMix\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=250)\n",
    "\n",
    "def acc_at_k(y_true, y_pred, k=2):\n",
    "    y_true = torch.tensor(y_true) if type(y_true) != torch.Tensor else y_true\n",
    "    y_pred = torch.tensor(y_pred) if type(y_pred) != torch.Tensor else y_pred\n",
    "    total = len(y_true)\n",
    "    y_weights, y_idx = torch.topk(y_true, k=k, dim=-1)\n",
    "    out_weights, out_idx = torch.topk(y_pred, k=k, dim=-1)\n",
    "    correct = torch.sum(torch.eq(y_idx, out_idx) * y_weights)\n",
    "    acc = correct / total\n",
    "    return acc.item()\n",
    "\n",
    "def CEwST_loss(logits, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Cross Entropy with Soft Target (CEwST) Loss\n",
    "    :param logits: (batch, *)\n",
    "    :param target: (batch, *) same shape as logits, each item must be a valid distribution: target[i, :].sum() == 1.\n",
    "    \"\"\"\n",
    "    logprobs = torch.nn.functional.log_softmax(logits.view(logits.shape[0], -1), dim=1)\n",
    "    batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\n",
    "    if reduction == 'none':\n",
    "        return batchloss\n",
    "    elif reduction == 'mean':\n",
    "        return torch.mean(batchloss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(batchloss)\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported reduction mode.')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1.mean(),\n",
    "        'precision': precision.mean(),\n",
    "        'recall': recall.mean()\n",
    "    }        \n",
    "        \n",
    "def compute_metrics_w_soft_target(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    acc = acc_at_k(labels, preds, k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "class TargetedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = CEwST_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "class TargetedMixturesCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that calculates a confusion matrix on the validation\n",
    "    data and returns the most confused class pairings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        cnf_mat = self.get_confusion_matrix(model, tokenizer, self.dataloader)\n",
    "        new_targets = self.get_most_confused_per_class(cnf_mat)\n",
    "        print(\"New targets:\", new_targets)\n",
    "        control = TrainerControl\n",
    "        control.new_targets = new_targets\n",
    "        if state.global_step < state.max_steps:\n",
    "            control.should_training_stop = False\n",
    "        else:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "        \n",
    "    def get_confusion_matrix(self, model, tokenizer, dataloader, normalize=True):\n",
    "        n_classes = max(dataloader.dataset['label']) + 1\n",
    "        confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dataloader):\n",
    "                data, targets = batch['text'], batch['label']\n",
    "                data = tokenizer(data, padding=True, truncation=True, max_length=250, return_tensors='pt')\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                preds = torch.argmax(outputs, dim=1).cpu()\n",
    "                for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1    \n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=0)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def get_most_confused_per_class(self, confusion_matrix):\n",
    "        idx = torch.arange(len(confusion_matrix))\n",
    "        cnf = confusion_matrix.fill_diagonal_(0).max(dim=1)[1]\n",
    "        return torch.stack((idx, cnf)).T.tolist()\n",
    "\n",
    "class TargetedMixturesCollator:\n",
    "    def __init__(self, tokenize_fn, transform, target_pairs=[], target_prob=1.0, num_classes=4):\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.transform = transform\n",
    "        self.target_pairs = target_pairs\n",
    "        self.target_prob = target_prob\n",
    "        self.num_classes = num_classes\n",
    "        print(\"TargetedMixturesCollator initialized with {}\".format(transform.__class__.__name__))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        text = [x['text'] for x in batch]\n",
    "        labels = [x['label'] for x in batch]\n",
    "        batch = (text, labels)\n",
    "        batch = self.transform(\n",
    "            batch, \n",
    "            self.target_pairs,   \n",
    "            self.target_prob,\n",
    "            self.num_classes\n",
    "        )\n",
    "        text, labels = batch\n",
    "        batch = self.tokenize_fn(text)\n",
    "        batch['labels'] = torch.tensor(labels)\n",
    "        return batch\n",
    "    \n",
    "class DefaultCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "ts = [TextMix(), SentMix(), WordMix()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "<ipython-input-5-bb65d84970cc>:23: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  test_dataset.rename_column_('label', 'labels')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9a73ec2e314a3da622044bb750819f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='39000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 39000/142500 2:53:39 < 7:40:53, 3.74 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.163200</td>\n",
       "      <td>0.841849</td>\n",
       "      <td>0.658472</td>\n",
       "      <td>46.205900</td>\n",
       "      <td>129.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.809177</td>\n",
       "      <td>0.664699</td>\n",
       "      <td>46.207000</td>\n",
       "      <td>129.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.780800</td>\n",
       "      <td>0.771794</td>\n",
       "      <td>0.658102</td>\n",
       "      <td>45.557500</td>\n",
       "      <td>131.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.780057</td>\n",
       "      <td>0.680474</td>\n",
       "      <td>45.935700</td>\n",
       "      <td>130.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.768400</td>\n",
       "      <td>0.758096</td>\n",
       "      <td>0.720509</td>\n",
       "      <td>45.969900</td>\n",
       "      <td>130.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.744300</td>\n",
       "      <td>0.766487</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>45.628800</td>\n",
       "      <td>131.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.745382</td>\n",
       "      <td>0.689125</td>\n",
       "      <td>46.387600</td>\n",
       "      <td>129.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.758748</td>\n",
       "      <td>0.729534</td>\n",
       "      <td>45.939500</td>\n",
       "      <td>130.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.741300</td>\n",
       "      <td>0.771517</td>\n",
       "      <td>0.650278</td>\n",
       "      <td>45.742700</td>\n",
       "      <td>131.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.769781</td>\n",
       "      <td>0.710425</td>\n",
       "      <td>46.191000</td>\n",
       "      <td>129.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.771925</td>\n",
       "      <td>0.706296</td>\n",
       "      <td>45.778800</td>\n",
       "      <td>131.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.756708</td>\n",
       "      <td>0.755749</td>\n",
       "      <td>46.368100</td>\n",
       "      <td>129.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.749410</td>\n",
       "      <td>0.720301</td>\n",
       "      <td>46.645800</td>\n",
       "      <td>128.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.742700</td>\n",
       "      <td>0.743963</td>\n",
       "      <td>0.750738</td>\n",
       "      <td>45.738000</td>\n",
       "      <td>131.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.800260</td>\n",
       "      <td>0.738132</td>\n",
       "      <td>45.558500</td>\n",
       "      <td>131.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>0.768767</td>\n",
       "      <td>0.714978</td>\n",
       "      <td>46.070600</td>\n",
       "      <td>130.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>0.742895</td>\n",
       "      <td>0.720104</td>\n",
       "      <td>46.053900</td>\n",
       "      <td>130.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>0.780979</td>\n",
       "      <td>0.721035</td>\n",
       "      <td>45.628500</td>\n",
       "      <td>131.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.772410</td>\n",
       "      <td>0.778922</td>\n",
       "      <td>45.112200</td>\n",
       "      <td>133.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.762417</td>\n",
       "      <td>0.774497</td>\n",
       "      <td>45.679500</td>\n",
       "      <td>131.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.786194</td>\n",
       "      <td>0.772272</td>\n",
       "      <td>46.031200</td>\n",
       "      <td>130.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.725100</td>\n",
       "      <td>0.769229</td>\n",
       "      <td>0.780721</td>\n",
       "      <td>45.809000</td>\n",
       "      <td>130.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.718700</td>\n",
       "      <td>0.765573</td>\n",
       "      <td>0.756748</td>\n",
       "      <td>46.217300</td>\n",
       "      <td>129.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.717700</td>\n",
       "      <td>0.724697</td>\n",
       "      <td>0.731154</td>\n",
       "      <td>46.386400</td>\n",
       "      <td>129.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.744890</td>\n",
       "      <td>0.746871</td>\n",
       "      <td>45.601400</td>\n",
       "      <td>131.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.728900</td>\n",
       "      <td>0.724777</td>\n",
       "      <td>0.817296</td>\n",
       "      <td>45.541900</td>\n",
       "      <td>131.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.753277</td>\n",
       "      <td>0.801097</td>\n",
       "      <td>45.565800</td>\n",
       "      <td>131.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.723100</td>\n",
       "      <td>0.729633</td>\n",
       "      <td>0.815987</td>\n",
       "      <td>45.877900</td>\n",
       "      <td>130.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.730529</td>\n",
       "      <td>0.830434</td>\n",
       "      <td>45.630000</td>\n",
       "      <td>131.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.726486</td>\n",
       "      <td>0.785547</td>\n",
       "      <td>45.736100</td>\n",
       "      <td>131.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>0.791194</td>\n",
       "      <td>46.612200</td>\n",
       "      <td>128.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.788852</td>\n",
       "      <td>46.255700</td>\n",
       "      <td>129.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.719590</td>\n",
       "      <td>0.828002</td>\n",
       "      <td>45.861500</td>\n",
       "      <td>130.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.739241</td>\n",
       "      <td>0.826515</td>\n",
       "      <td>46.142500</td>\n",
       "      <td>130.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.748801</td>\n",
       "      <td>0.806808</td>\n",
       "      <td>44.990600</td>\n",
       "      <td>133.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.796477</td>\n",
       "      <td>0.747272</td>\n",
       "      <td>45.542900</td>\n",
       "      <td>131.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.757712</td>\n",
       "      <td>0.824425</td>\n",
       "      <td>46.309400</td>\n",
       "      <td>129.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.766515</td>\n",
       "      <td>0.794478</td>\n",
       "      <td>45.887300</td>\n",
       "      <td>130.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.750358</td>\n",
       "      <td>0.785396</td>\n",
       "      <td>45.535700</td>\n",
       "      <td>131.765000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 2], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
      "{'eval_loss': 25.431032180786133, 'eval_accuracy': 0.9367105263157894, 'eval_f1': 0.9365973002462261, 'eval_precision': 0.9366754242217419, 'eval_recall': 0.9367105263157895, 'eval_runtime': 100.5442, 'eval_samples_per_second': 75.589, 'epoch': 2.74, 'run': './results/bert-base-uncased-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61483097ec648da857eee2f9f48129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='110000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110000/142500 8:07:54 < 2:24:09, 3.76 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>0.849899</td>\n",
       "      <td>0.661589</td>\n",
       "      <td>45.518000</td>\n",
       "      <td>131.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.823100</td>\n",
       "      <td>0.817805</td>\n",
       "      <td>0.639086</td>\n",
       "      <td>45.891000</td>\n",
       "      <td>130.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.785314</td>\n",
       "      <td>0.598100</td>\n",
       "      <td>46.993200</td>\n",
       "      <td>127.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.779900</td>\n",
       "      <td>0.786050</td>\n",
       "      <td>0.596946</td>\n",
       "      <td>46.291700</td>\n",
       "      <td>129.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.762700</td>\n",
       "      <td>0.763109</td>\n",
       "      <td>0.693854</td>\n",
       "      <td>46.057200</td>\n",
       "      <td>130.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.820140</td>\n",
       "      <td>0.660154</td>\n",
       "      <td>46.198000</td>\n",
       "      <td>129.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.775092</td>\n",
       "      <td>0.645828</td>\n",
       "      <td>46.024600</td>\n",
       "      <td>130.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.741300</td>\n",
       "      <td>0.769967</td>\n",
       "      <td>0.658169</td>\n",
       "      <td>46.084800</td>\n",
       "      <td>130.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0.778055</td>\n",
       "      <td>0.677989</td>\n",
       "      <td>46.001900</td>\n",
       "      <td>130.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.767200</td>\n",
       "      <td>0.760361</td>\n",
       "      <td>0.667743</td>\n",
       "      <td>45.244300</td>\n",
       "      <td>132.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.757800</td>\n",
       "      <td>0.799724</td>\n",
       "      <td>0.682298</td>\n",
       "      <td>46.083300</td>\n",
       "      <td>130.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.771914</td>\n",
       "      <td>0.670625</td>\n",
       "      <td>45.955500</td>\n",
       "      <td>130.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.757252</td>\n",
       "      <td>0.721347</td>\n",
       "      <td>46.013100</td>\n",
       "      <td>130.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>0.774765</td>\n",
       "      <td>0.678350</td>\n",
       "      <td>45.420000</td>\n",
       "      <td>132.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.740400</td>\n",
       "      <td>0.768881</td>\n",
       "      <td>0.692052</td>\n",
       "      <td>46.333800</td>\n",
       "      <td>129.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.751400</td>\n",
       "      <td>0.772906</td>\n",
       "      <td>0.570239</td>\n",
       "      <td>46.131100</td>\n",
       "      <td>130.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.824844</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>46.118600</td>\n",
       "      <td>130.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>0.753082</td>\n",
       "      <td>0.672137</td>\n",
       "      <td>45.691100</td>\n",
       "      <td>131.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.785813</td>\n",
       "      <td>0.718382</td>\n",
       "      <td>45.762400</td>\n",
       "      <td>131.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>0.789786</td>\n",
       "      <td>0.729532</td>\n",
       "      <td>45.853200</td>\n",
       "      <td>130.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>0.732929</td>\n",
       "      <td>45.766800</td>\n",
       "      <td>131.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.778361</td>\n",
       "      <td>0.722819</td>\n",
       "      <td>46.196000</td>\n",
       "      <td>129.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.742031</td>\n",
       "      <td>0.744218</td>\n",
       "      <td>45.409900</td>\n",
       "      <td>132.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>0.765180</td>\n",
       "      <td>0.756056</td>\n",
       "      <td>45.568700</td>\n",
       "      <td>131.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.730500</td>\n",
       "      <td>0.788505</td>\n",
       "      <td>0.748751</td>\n",
       "      <td>46.075100</td>\n",
       "      <td>130.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.716100</td>\n",
       "      <td>0.776533</td>\n",
       "      <td>0.709821</td>\n",
       "      <td>45.850000</td>\n",
       "      <td>130.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.756358</td>\n",
       "      <td>0.754828</td>\n",
       "      <td>45.966300</td>\n",
       "      <td>130.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.744179</td>\n",
       "      <td>0.744218</td>\n",
       "      <td>46.145400</td>\n",
       "      <td>130.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.718900</td>\n",
       "      <td>0.813593</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>45.614500</td>\n",
       "      <td>131.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.794023</td>\n",
       "      <td>0.751135</td>\n",
       "      <td>46.293000</td>\n",
       "      <td>129.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.738173</td>\n",
       "      <td>46.198200</td>\n",
       "      <td>129.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.762034</td>\n",
       "      <td>0.757990</td>\n",
       "      <td>46.394300</td>\n",
       "      <td>129.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.759889</td>\n",
       "      <td>0.770895</td>\n",
       "      <td>45.570000</td>\n",
       "      <td>131.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.778549</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>46.449300</td>\n",
       "      <td>129.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.744350</td>\n",
       "      <td>0.792949</td>\n",
       "      <td>45.956500</td>\n",
       "      <td>130.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.758528</td>\n",
       "      <td>0.767489</td>\n",
       "      <td>45.904800</td>\n",
       "      <td>130.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>0.765005</td>\n",
       "      <td>0.763332</td>\n",
       "      <td>45.945200</td>\n",
       "      <td>130.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.755703</td>\n",
       "      <td>0.798114</td>\n",
       "      <td>45.528400</td>\n",
       "      <td>131.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0.793801</td>\n",
       "      <td>0.788100</td>\n",
       "      <td>46.095200</td>\n",
       "      <td>130.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.792527</td>\n",
       "      <td>0.787889</td>\n",
       "      <td>45.615200</td>\n",
       "      <td>131.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.683800</td>\n",
       "      <td>0.745380</td>\n",
       "      <td>0.800191</td>\n",
       "      <td>46.375700</td>\n",
       "      <td>129.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.752848</td>\n",
       "      <td>0.767886</td>\n",
       "      <td>46.164600</td>\n",
       "      <td>129.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.765905</td>\n",
       "      <td>0.815547</td>\n",
       "      <td>45.955000</td>\n",
       "      <td>130.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.757990</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>46.117500</td>\n",
       "      <td>130.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.785249</td>\n",
       "      <td>0.802389</td>\n",
       "      <td>45.951100</td>\n",
       "      <td>130.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.770713</td>\n",
       "      <td>0.762965</td>\n",
       "      <td>45.960900</td>\n",
       "      <td>130.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.800218</td>\n",
       "      <td>0.786961</td>\n",
       "      <td>45.896000</td>\n",
       "      <td>130.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.800475</td>\n",
       "      <td>0.802610</td>\n",
       "      <td>46.739800</td>\n",
       "      <td>128.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.675300</td>\n",
       "      <td>0.782922</td>\n",
       "      <td>0.812077</td>\n",
       "      <td>45.852000</td>\n",
       "      <td>130.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.758289</td>\n",
       "      <td>0.807693</td>\n",
       "      <td>45.799000</td>\n",
       "      <td>131.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>0.750007</td>\n",
       "      <td>0.801413</td>\n",
       "      <td>45.814000</td>\n",
       "      <td>130.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.765546</td>\n",
       "      <td>0.776719</td>\n",
       "      <td>46.172700</td>\n",
       "      <td>129.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.756009</td>\n",
       "      <td>0.820605</td>\n",
       "      <td>45.410400</td>\n",
       "      <td>132.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.820128</td>\n",
       "      <td>45.747100</td>\n",
       "      <td>131.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.740950</td>\n",
       "      <td>0.832220</td>\n",
       "      <td>45.771100</td>\n",
       "      <td>131.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.802330</td>\n",
       "      <td>0.799309</td>\n",
       "      <td>46.866400</td>\n",
       "      <td>128.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.786021</td>\n",
       "      <td>0.815473</td>\n",
       "      <td>45.574100</td>\n",
       "      <td>131.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.771755</td>\n",
       "      <td>0.807653</td>\n",
       "      <td>45.981700</td>\n",
       "      <td>130.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.762879</td>\n",
       "      <td>0.840712</td>\n",
       "      <td>46.883000</td>\n",
       "      <td>127.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.798713</td>\n",
       "      <td>0.819184</td>\n",
       "      <td>46.023700</td>\n",
       "      <td>130.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.778894</td>\n",
       "      <td>0.830887</td>\n",
       "      <td>46.184600</td>\n",
       "      <td>129.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>0.808459</td>\n",
       "      <td>0.832231</td>\n",
       "      <td>45.044100</td>\n",
       "      <td>133.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>0.764470</td>\n",
       "      <td>0.812083</td>\n",
       "      <td>46.332000</td>\n",
       "      <td>129.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>0.784163</td>\n",
       "      <td>0.810939</td>\n",
       "      <td>46.208200</td>\n",
       "      <td>129.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.636400</td>\n",
       "      <td>0.781516</td>\n",
       "      <td>0.829296</td>\n",
       "      <td>46.007900</td>\n",
       "      <td>130.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.757131</td>\n",
       "      <td>0.799877</td>\n",
       "      <td>45.773400</td>\n",
       "      <td>131.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.782543</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>46.009400</td>\n",
       "      <td>130.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.655100</td>\n",
       "      <td>0.782402</td>\n",
       "      <td>0.827048</td>\n",
       "      <td>45.840400</td>\n",
       "      <td>130.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.647800</td>\n",
       "      <td>0.782901</td>\n",
       "      <td>0.846747</td>\n",
       "      <td>46.715000</td>\n",
       "      <td>128.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.648300</td>\n",
       "      <td>0.758355</td>\n",
       "      <td>0.824525</td>\n",
       "      <td>45.031400</td>\n",
       "      <td>133.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.639900</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>0.837368</td>\n",
       "      <td>45.364400</td>\n",
       "      <td>132.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>0.794284</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>46.026100</td>\n",
       "      <td>130.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.822371</td>\n",
       "      <td>46.060700</td>\n",
       "      <td>130.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.624500</td>\n",
       "      <td>0.791369</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>46.229500</td>\n",
       "      <td>129.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.757590</td>\n",
       "      <td>0.838805</td>\n",
       "      <td>45.935200</td>\n",
       "      <td>130.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.786463</td>\n",
       "      <td>0.854173</td>\n",
       "      <td>45.412400</td>\n",
       "      <td>132.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.817985</td>\n",
       "      <td>0.842656</td>\n",
       "      <td>46.330600</td>\n",
       "      <td>129.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>0.774797</td>\n",
       "      <td>0.845527</td>\n",
       "      <td>45.843000</td>\n",
       "      <td>130.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>0.796597</td>\n",
       "      <td>0.809826</td>\n",
       "      <td>45.625700</td>\n",
       "      <td>131.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>0.838131</td>\n",
       "      <td>46.062500</td>\n",
       "      <td>130.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.835757</td>\n",
       "      <td>45.872200</td>\n",
       "      <td>130.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.775601</td>\n",
       "      <td>0.855792</td>\n",
       "      <td>47.142100</td>\n",
       "      <td>127.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.790759</td>\n",
       "      <td>0.839013</td>\n",
       "      <td>45.973500</td>\n",
       "      <td>130.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.771211</td>\n",
       "      <td>0.849052</td>\n",
       "      <td>46.342100</td>\n",
       "      <td>129.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.790288</td>\n",
       "      <td>0.816111</td>\n",
       "      <td>46.467600</td>\n",
       "      <td>129.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>0.802137</td>\n",
       "      <td>0.851763</td>\n",
       "      <td>44.985300</td>\n",
       "      <td>133.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.618300</td>\n",
       "      <td>0.802871</td>\n",
       "      <td>0.854509</td>\n",
       "      <td>46.208000</td>\n",
       "      <td>129.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.619800</td>\n",
       "      <td>0.809180</td>\n",
       "      <td>0.858105</td>\n",
       "      <td>45.759700</td>\n",
       "      <td>131.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.610700</td>\n",
       "      <td>0.822401</td>\n",
       "      <td>0.855348</td>\n",
       "      <td>45.518600</td>\n",
       "      <td>131.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.802874</td>\n",
       "      <td>0.860479</td>\n",
       "      <td>46.024800</td>\n",
       "      <td>130.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.622800</td>\n",
       "      <td>0.795902</td>\n",
       "      <td>0.848951</td>\n",
       "      <td>46.361100</td>\n",
       "      <td>129.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.796180</td>\n",
       "      <td>0.842434</td>\n",
       "      <td>46.036300</td>\n",
       "      <td>130.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.831107</td>\n",
       "      <td>0.858853</td>\n",
       "      <td>46.317200</td>\n",
       "      <td>129.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.845277</td>\n",
       "      <td>0.842335</td>\n",
       "      <td>45.931000</td>\n",
       "      <td>130.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>0.816037</td>\n",
       "      <td>0.860008</td>\n",
       "      <td>46.007200</td>\n",
       "      <td>130.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.787132</td>\n",
       "      <td>0.870604</td>\n",
       "      <td>46.119000</td>\n",
       "      <td>130.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.801107</td>\n",
       "      <td>0.864803</td>\n",
       "      <td>45.933300</td>\n",
       "      <td>130.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.812823</td>\n",
       "      <td>0.857423</td>\n",
       "      <td>46.144100</td>\n",
       "      <td>130.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.795599</td>\n",
       "      <td>0.864641</td>\n",
       "      <td>45.773600</td>\n",
       "      <td>131.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.814544</td>\n",
       "      <td>0.874736</td>\n",
       "      <td>46.123400</td>\n",
       "      <td>130.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.812360</td>\n",
       "      <td>0.863317</td>\n",
       "      <td>45.558600</td>\n",
       "      <td>131.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.587600</td>\n",
       "      <td>0.826033</td>\n",
       "      <td>0.864337</td>\n",
       "      <td>46.367800</td>\n",
       "      <td>129.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.846848</td>\n",
       "      <td>0.865159</td>\n",
       "      <td>45.580700</td>\n",
       "      <td>131.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.800348</td>\n",
       "      <td>0.863012</td>\n",
       "      <td>46.537400</td>\n",
       "      <td>128.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.787576</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>46.182100</td>\n",
       "      <td>129.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>0.822490</td>\n",
       "      <td>0.865877</td>\n",
       "      <td>46.394900</td>\n",
       "      <td>129.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>0.809542</td>\n",
       "      <td>0.852442</td>\n",
       "      <td>44.903300</td>\n",
       "      <td>133.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.832391</td>\n",
       "      <td>0.852090</td>\n",
       "      <td>45.685200</td>\n",
       "      <td>131.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.806128</td>\n",
       "      <td>0.861389</td>\n",
       "      <td>46.370300</td>\n",
       "      <td>129.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.822704</td>\n",
       "      <td>0.872688</td>\n",
       "      <td>45.755100</td>\n",
       "      <td>131.133000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-SentMix\n",
      "{'eval_loss': 31.42345428466797, 'eval_accuracy': 0.9355263157894737, 'eval_f1': 0.93549770002292, 'eval_precision': 0.9355946206596242, 'eval_recall': 0.9355263157894738, 'eval_runtime': 100.4882, 'eval_samples_per_second': 75.631, 'epoch': 7.72, 'run': './results/bert-base-uncased-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30089a9d6103461a901239817342e6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='14000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14000/142500 1:03:04 < 9:39:00, 3.70 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.156400</td>\n",
       "      <td>0.908590</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>46.353900</td>\n",
       "      <td>129.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.965018</td>\n",
       "      <td>0.565409</td>\n",
       "      <td>46.313100</td>\n",
       "      <td>129.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.825100</td>\n",
       "      <td>0.853418</td>\n",
       "      <td>0.585338</td>\n",
       "      <td>46.890100</td>\n",
       "      <td>127.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.804300</td>\n",
       "      <td>0.809024</td>\n",
       "      <td>0.623672</td>\n",
       "      <td>46.997900</td>\n",
       "      <td>127.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.801500</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.601925</td>\n",
       "      <td>46.724200</td>\n",
       "      <td>128.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.803579</td>\n",
       "      <td>0.617895</td>\n",
       "      <td>46.649300</td>\n",
       "      <td>128.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.795400</td>\n",
       "      <td>0.805269</td>\n",
       "      <td>0.599631</td>\n",
       "      <td>46.661000</td>\n",
       "      <td>128.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.816359</td>\n",
       "      <td>0.595833</td>\n",
       "      <td>46.805900</td>\n",
       "      <td>128.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.830895</td>\n",
       "      <td>0.612883</td>\n",
       "      <td>46.444800</td>\n",
       "      <td>129.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.807267</td>\n",
       "      <td>0.542680</td>\n",
       "      <td>46.371700</td>\n",
       "      <td>129.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.821096</td>\n",
       "      <td>0.583571</td>\n",
       "      <td>46.963300</td>\n",
       "      <td>127.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.831269</td>\n",
       "      <td>0.594234</td>\n",
       "      <td>46.408700</td>\n",
       "      <td>129.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.803100</td>\n",
       "      <td>0.844698</td>\n",
       "      <td>0.595383</td>\n",
       "      <td>47.176900</td>\n",
       "      <td>127.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.822345</td>\n",
       "      <td>0.557191</td>\n",
       "      <td>47.524000</td>\n",
       "      <td>126.252000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 0], [3, 2]]\n",
      "New targets: [[0, 1], [1, 3], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/bert-base-uncased-targeted-WordMix\n",
      "{'eval_loss': 17.457401275634766, 'eval_accuracy': 0.9039473684210526, 'eval_f1': 0.9036912981525473, 'eval_precision': 0.9036114688366205, 'eval_recall': 0.9039473684210526, 'eval_runtime': 100.2085, 'eval_samples_per_second': 75.842, 'epoch': 0.98, 'run': './results/bert-base-uncased-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f118c6b75ba44ee9dc474128b357f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='19000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 19000/142500 1:25:59 < 9:18:59, 3.68 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.112700</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>45.440100</td>\n",
       "      <td>132.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.783200</td>\n",
       "      <td>0.803310</td>\n",
       "      <td>0.695313</td>\n",
       "      <td>45.397700</td>\n",
       "      <td>132.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.781338</td>\n",
       "      <td>0.743023</td>\n",
       "      <td>45.671000</td>\n",
       "      <td>131.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.765200</td>\n",
       "      <td>0.820864</td>\n",
       "      <td>0.646053</td>\n",
       "      <td>45.103500</td>\n",
       "      <td>133.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.749900</td>\n",
       "      <td>0.776440</td>\n",
       "      <td>0.648404</td>\n",
       "      <td>45.398100</td>\n",
       "      <td>132.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.754900</td>\n",
       "      <td>0.798856</td>\n",
       "      <td>0.709035</td>\n",
       "      <td>45.336000</td>\n",
       "      <td>132.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.764900</td>\n",
       "      <td>0.792176</td>\n",
       "      <td>0.730105</td>\n",
       "      <td>45.732100</td>\n",
       "      <td>131.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.773055</td>\n",
       "      <td>0.731464</td>\n",
       "      <td>45.049500</td>\n",
       "      <td>133.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.795473</td>\n",
       "      <td>0.761945</td>\n",
       "      <td>44.587800</td>\n",
       "      <td>134.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.794539</td>\n",
       "      <td>0.682491</td>\n",
       "      <td>44.972200</td>\n",
       "      <td>133.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>0.770127</td>\n",
       "      <td>0.709478</td>\n",
       "      <td>45.172200</td>\n",
       "      <td>132.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.761400</td>\n",
       "      <td>0.811858</td>\n",
       "      <td>0.660275</td>\n",
       "      <td>44.780400</td>\n",
       "      <td>133.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>0.791824</td>\n",
       "      <td>0.686806</td>\n",
       "      <td>44.626500</td>\n",
       "      <td>134.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.755400</td>\n",
       "      <td>0.795980</td>\n",
       "      <td>0.732947</td>\n",
       "      <td>44.978500</td>\n",
       "      <td>133.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>0.824114</td>\n",
       "      <td>0.619699</td>\n",
       "      <td>45.572200</td>\n",
       "      <td>131.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.793571</td>\n",
       "      <td>0.693769</td>\n",
       "      <td>45.323100</td>\n",
       "      <td>132.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.817665</td>\n",
       "      <td>0.744461</td>\n",
       "      <td>44.949400</td>\n",
       "      <td>133.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>0.785947</td>\n",
       "      <td>0.756013</td>\n",
       "      <td>45.488400</td>\n",
       "      <td>131.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.729400</td>\n",
       "      <td>0.774472</td>\n",
       "      <td>0.736833</td>\n",
       "      <td>44.597400</td>\n",
       "      <td>134.537000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 3], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-TextMix\n",
      "{'eval_loss': 24.688467025756836, 'eval_accuracy': 0.9221052631578948, 'eval_f1': 0.9224425641357582, 'eval_precision': 0.9234926254741409, 'eval_recall': 0.9221052631578948, 'eval_runtime': 99.0346, 'eval_samples_per_second': 76.741, 'epoch': 1.33, 'run': './results/roberta-base-targeted-TextMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd7828a2d24f4998dd1890c62d71ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with SentMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15000/142500 1:08:16 < 9:40:28, 3.66 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.101500</td>\n",
       "      <td>0.759651</td>\n",
       "      <td>0.720774</td>\n",
       "      <td>45.782400</td>\n",
       "      <td>131.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>0.776611</td>\n",
       "      <td>0.684789</td>\n",
       "      <td>45.939100</td>\n",
       "      <td>130.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.782814</td>\n",
       "      <td>0.637705</td>\n",
       "      <td>45.980200</td>\n",
       "      <td>130.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.796187</td>\n",
       "      <td>0.680435</td>\n",
       "      <td>45.969300</td>\n",
       "      <td>130.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>0.794483</td>\n",
       "      <td>0.744525</td>\n",
       "      <td>45.576100</td>\n",
       "      <td>131.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.772300</td>\n",
       "      <td>0.786910</td>\n",
       "      <td>0.671541</td>\n",
       "      <td>45.837800</td>\n",
       "      <td>130.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>0.811842</td>\n",
       "      <td>0.736622</td>\n",
       "      <td>46.133700</td>\n",
       "      <td>130.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.746900</td>\n",
       "      <td>0.768118</td>\n",
       "      <td>0.687954</td>\n",
       "      <td>45.621200</td>\n",
       "      <td>131.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.765200</td>\n",
       "      <td>0.775836</td>\n",
       "      <td>0.705598</td>\n",
       "      <td>46.030200</td>\n",
       "      <td>130.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.786508</td>\n",
       "      <td>0.637667</td>\n",
       "      <td>45.813700</td>\n",
       "      <td>130.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.786784</td>\n",
       "      <td>0.708724</td>\n",
       "      <td>46.020200</td>\n",
       "      <td>130.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.789656</td>\n",
       "      <td>0.662690</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>131.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.769500</td>\n",
       "      <td>0.786302</td>\n",
       "      <td>0.616997</td>\n",
       "      <td>45.962700</td>\n",
       "      <td>130.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.761200</td>\n",
       "      <td>0.764610</td>\n",
       "      <td>0.669205</td>\n",
       "      <td>45.589000</td>\n",
       "      <td>131.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.806065</td>\n",
       "      <td>0.606583</td>\n",
       "      <td>45.721800</td>\n",
       "      <td>131.228000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-SentMix\n",
      "{'eval_loss': 23.09513282775879, 'eval_accuracy': 0.9173684210526316, 'eval_f1': 0.9173812746159944, 'eval_precision': 0.9185799634956395, 'eval_recall': 0.9173684210526316, 'eval_runtime': 98.8003, 'eval_samples_per_second': 76.923, 'epoch': 1.05, 'run': './results/roberta-base-targeted-SentMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d64ed5cd48470cb83799d8b972f273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with WordMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='31000' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 31000/142500 2:20:10 < 8:24:13, 3.69 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.097600</td>\n",
       "      <td>0.832878</td>\n",
       "      <td>0.643672</td>\n",
       "      <td>44.252600</td>\n",
       "      <td>135.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.842358</td>\n",
       "      <td>0.570324</td>\n",
       "      <td>43.867000</td>\n",
       "      <td>136.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.829623</td>\n",
       "      <td>0.576125</td>\n",
       "      <td>44.419000</td>\n",
       "      <td>135.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.826187</td>\n",
       "      <td>0.570909</td>\n",
       "      <td>43.500200</td>\n",
       "      <td>137.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.550424</td>\n",
       "      <td>44.493500</td>\n",
       "      <td>134.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.821086</td>\n",
       "      <td>0.534845</td>\n",
       "      <td>44.936700</td>\n",
       "      <td>133.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.806100</td>\n",
       "      <td>0.813902</td>\n",
       "      <td>0.587292</td>\n",
       "      <td>44.648800</td>\n",
       "      <td>134.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.802383</td>\n",
       "      <td>0.622662</td>\n",
       "      <td>44.829800</td>\n",
       "      <td>133.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.803633</td>\n",
       "      <td>0.608604</td>\n",
       "      <td>44.066400</td>\n",
       "      <td>136.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.789900</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>0.605293</td>\n",
       "      <td>44.171800</td>\n",
       "      <td>135.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>0.812333</td>\n",
       "      <td>0.593331</td>\n",
       "      <td>44.681300</td>\n",
       "      <td>134.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.819338</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>43.417900</td>\n",
       "      <td>138.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.854306</td>\n",
       "      <td>0.586639</td>\n",
       "      <td>43.719600</td>\n",
       "      <td>137.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.836542</td>\n",
       "      <td>0.544078</td>\n",
       "      <td>43.965000</td>\n",
       "      <td>136.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.809700</td>\n",
       "      <td>0.806516</td>\n",
       "      <td>0.623244</td>\n",
       "      <td>44.204400</td>\n",
       "      <td>135.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.859239</td>\n",
       "      <td>0.542386</td>\n",
       "      <td>44.477500</td>\n",
       "      <td>134.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.826738</td>\n",
       "      <td>0.601607</td>\n",
       "      <td>44.323800</td>\n",
       "      <td>135.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>0.836822</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>43.921800</td>\n",
       "      <td>136.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.612821</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>133.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.792200</td>\n",
       "      <td>0.834082</td>\n",
       "      <td>0.600543</td>\n",
       "      <td>44.715100</td>\n",
       "      <td>134.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.824451</td>\n",
       "      <td>0.634927</td>\n",
       "      <td>44.412600</td>\n",
       "      <td>135.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.809600</td>\n",
       "      <td>0.861144</td>\n",
       "      <td>0.587536</td>\n",
       "      <td>44.088900</td>\n",
       "      <td>136.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.849023</td>\n",
       "      <td>0.581778</td>\n",
       "      <td>44.390000</td>\n",
       "      <td>135.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.811400</td>\n",
       "      <td>0.874432</td>\n",
       "      <td>0.604483</td>\n",
       "      <td>44.728600</td>\n",
       "      <td>134.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.825222</td>\n",
       "      <td>0.617124</td>\n",
       "      <td>43.758200</td>\n",
       "      <td>137.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.851962</td>\n",
       "      <td>0.609938</td>\n",
       "      <td>44.180800</td>\n",
       "      <td>135.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>0.842256</td>\n",
       "      <td>0.607055</td>\n",
       "      <td>44.492300</td>\n",
       "      <td>134.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>0.817139</td>\n",
       "      <td>0.590739</td>\n",
       "      <td>44.778400</td>\n",
       "      <td>133.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.894816</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>43.535100</td>\n",
       "      <td>137.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.877905</td>\n",
       "      <td>0.607922</td>\n",
       "      <td>44.175900</td>\n",
       "      <td>135.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.904637</td>\n",
       "      <td>0.609033</td>\n",
       "      <td>43.735500</td>\n",
       "      <td>137.188000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 1], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 0]]\n",
      "New targets: [[0, 1], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "New targets: [[0, 1], [1, 2], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 3], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 0]]\n",
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n",
      "New targets: [[0, 3], [1, 0], [2, 0], [3, 2]]\n",
      "New targets: [[0, 2], [1, 2], [2, 3], [3, 2]]\n",
      "early_stopping_patience_counter\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 01:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG for ./results/roberta-base-targeted-WordMix\n",
      "{'eval_loss': 17.9605770111084, 'eval_accuracy': 0.8863157894736842, 'eval_f1': 0.8870375967397961, 'eval_precision': 0.8898028670023486, 'eval_recall': 0.8863157894736843, 'eval_runtime': 98.2325, 'eval_samples_per_second': 77.367, 'epoch': 2.18, 'run': './results/roberta-base-targeted-WordMix', 'test': 'ORIG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\sleev\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce96c3e867544068ecdadc7be123df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TargetedMixturesCollator initialized with TextMix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1037' max='142500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1037/142500 07:28 < 17:00:48, 2.31 it/s, Epoch 0.07/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.068800</td>\n",
       "      <td>0.823994</td>\n",
       "      <td>0.652225</td>\n",
       "      <td>108.478600</td>\n",
       "      <td>55.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New targets: [[0, 2], [1, 0], [2, 3], [3, 2]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 6.00 GiB total capacity; 4.20 GiB already allocated; 5.62 MiB free; 4.57 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bb65d84970cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# test with ORIG data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1248\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1c4b6ee207a1>\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCEwST_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1517\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m   1518\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutput_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m             outputs = layer_module(\n\u001b[0m\u001b[0;32m   1245\u001b[0m                 \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m                 \u001b[0moutput_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             )\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0moutput_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Add again attentions if there are there\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1785\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mff_chunk\u001b[1;34m(self, output_x)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mgelu\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 6.00 GiB total capacity; 4.20 GiB already allocated; 5.62 MiB free; 4.57 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "        \n",
    "    for t in ts: \n",
    "        \n",
    "        t_str = t.__class__.__name__\n",
    "        checkpoint = './results/' + MODEL_NAME + '-targeted-' + t_str\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4).to(device)\n",
    "\n",
    "        dataset = load_dataset('ag_news', split='train') \n",
    "        dataset_dict = dataset.train_test_split(\n",
    "            test_size = 0.05,\n",
    "            train_size = 0.95,\n",
    "            shuffle = True\n",
    "        )\n",
    "        train_dataset = dataset_dict['train']\n",
    "        eval_dataset = dataset_dict['test']\n",
    "\n",
    "        test_dataset = load_dataset('ag_news', split='test') \n",
    "        test_dataset.rename_column_('label', 'labels')\n",
    "        test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "        test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        train_batch_size = 8\n",
    "        eval_batch_size = 32\n",
    "        num_epoch = 10\n",
    "        gradient_accumulation_steps = 1\n",
    "        max_steps = int((len(train_dataset) * num_epoch / gradient_accumulation_steps) / train_batch_size)\n",
    "\n",
    "        tmcb = TargetedMixturesCallback(\n",
    "            dataloader=DataLoader(eval_dataset, batch_size=32),\n",
    "            device=device\n",
    "        )\n",
    "        escb = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10\n",
    "        )\n",
    "        tmc = TargetedMixturesCollator(\n",
    "            tokenize_fn=tokenize_fn, \n",
    "            transform=t,\n",
    "            target_prob=0.5\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            seed=1,\n",
    "            output_dir=checkpoint,\n",
    "            overwrite_output_dir=True,\n",
    "            max_steps=max_steps,\n",
    "            save_steps=int(max_steps / 10),\n",
    "            save_total_limit=1,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "            warmup_steps=int(max_steps / 10),\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=1000,\n",
    "            logging_first_step=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = TargetedTrainer(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics_w_soft_target,                  \n",
    "            train_dataset=train_dataset,         \n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=tmc,\n",
    "            callbacks=[tmcb, escb]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # test with ORIG data\n",
    "        trainer.eval_dataset = test_dataset\n",
    "        trainer.compute_metrics = compute_metrics\n",
    "        trainer.data_collator = DefaultCollator()\n",
    "        trainer.remove_callback(tmcb)\n",
    "\n",
    "        out_orig = trainer.evaluate()\n",
    "        out_orig['run'] = checkpoint\n",
    "        out_orig['test'] = \"ORIG\"\n",
    "        print('ORIG for {}\\n{}'.format(checkpoint, out_orig))\n",
    "\n",
    "        results.append(out_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_AG_NEWS_targeted_r1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIG for ./results/bert-base-uncased-targeted-TextMix\n",
    "# {'eval_loss': 31.2364559173584, 'eval_accuracy': 0.9381578947368421, 'eval_f1': 0.9381945850526017, 'eval_precision': 0.938240633851668, 'eval_recall': 0.9381578947368421, 'eval_runtime': 117.2622, 'eval_samples_per_second': 64.812, 'epoch': 5.0, 'run': 'TextMix', 'test': 'ORIG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
